{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vote_classifier_sklearn_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yq2y7jzAe0I"
      },
      "source": [
        "# **Stock market news feed semantic analysis** *(Baseline Classifiers)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpzSBujdAT3b"
      },
      "source": [
        "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
        "https://colab.research.google.com/drive/1RkME2dS0imkikmKsO_lgnuozuriA65V9#scrollTo=Qk-qyHFInIOl&uniqifier=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW0rSa4YBDBj"
      },
      "source": [
        "* https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_processGaussianProcessClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
        "\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yonhN0o-Ecoo",
        "outputId": "b81a27a2-a166-494f-cb9b-e68614b6421d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDPvwHfUAM9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd42453-b1bc-4637-bce4-ee7519a11f8c"
      },
      "source": [
        "# classfiers import\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# others\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtrwQCr5ERUM"
      },
      "source": [
        "# Shuffle cycle number for the dataframe\n",
        "SHUFFLE_CYCLE = 500\n",
        "\n",
        "# Random seed\n",
        "RANDOM_SEED = 1234\n",
        "\n",
        "# Numpy random seed\n",
        "NP_SEED = 1234\n",
        "\n",
        "# Max iteration for training\n",
        "MAX_ITER = 1000\n",
        "\n",
        "# Train size\n",
        "TRAIN_SPLIT = 0.75\n",
        "\n",
        "# Test size\n",
        "TEST_SPLIT = 0.25\n",
        "\n",
        "np.random.seed(NP_SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXSKMWVYF_na"
      },
      "source": [
        "## **Preproces**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0lNxxQbCaL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ceaaff-dd5f-4168-d4f7-367e535cb611"
      },
      "source": [
        "# Load and prerpocess just like in logistic regression baseline\n",
        "\n",
        "# Copy the dataset to the local environment\n",
        "!cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\n",
        "\n",
        "# Number of merged news into one string\n",
        "ROWS = 8\n",
        "\n",
        "# Load the dataset \n",
        "df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\n",
        "\n",
        "# Load the stock data\n",
        "df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \n",
        "                          end=\"2016-07-01\")\n",
        "\n",
        "temp_day = []\n",
        "\n",
        "for day in range(len(df_stock)):\n",
        "    temp_day.append(df_stock.index[day].date())\n",
        "\n",
        "df_stock.index = temp_day\n",
        "\n",
        "difference = []\n",
        "\n",
        "if len(df_combined) == len(df_stock):\n",
        "    print(\"The lengths are the same!\")\n",
        "\n",
        "for day in range(max(len(df_combined), len(df_stock))):\n",
        "    if str(df_combined.index[day]) != str(df_stock.index[day]):\n",
        "        print(\"There is difference at: \" + str(day) + \" index\")\n",
        "        print(\"News: \" + str(df_combined.index[day]) + \"\\tStock: \" + str(df_stock.index[day]))\n",
        "        difference.append(day)\n",
        "\n",
        "if len(difference) is 0:\n",
        "    print(\"The dates matched!\")\n",
        "\n",
        "difference = []\n",
        "\n",
        "for day in range(len(df_stock)):\n",
        "    # label should be 1 -> rise\n",
        "    if int(df_stock[\"Adj Close\"][day]) >= int(df_stock[\"Adj Close\"][day - 1]):\n",
        "        if df_combined[\"Label\"][day] != 1:\n",
        "            difference.append(str(df_stock.index[day]))\n",
        "            print(\"Problem at day \" + str(df_stock.index[day]))\n",
        "            print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\n",
        "\n",
        "    # label should be 0 -> fall\n",
        "    if int(df_stock[\"Adj Close\"][day]) < int(df_stock[\"Adj Close\"][day - 1]):\n",
        "        if df_combined[\"Label\"][day] != 0:\n",
        "            difference.append(str(df_stock.index[day]))\n",
        "            print(\"Problem at day \" + str(df_stock.index[day]))\n",
        "            print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\n",
        "\n",
        "print(\"All differences: \" + str(len(difference)))  \n",
        "\n",
        "# correct the wrong labels\n",
        "for row in difference:\n",
        "    if df_combined.loc[row, \"Label\"] == 0:\n",
        "        df_combined.loc[row, \"Label\"] = 1\n",
        "    else:\n",
        "        df_combined.loc[row, \"Label\"] = 0\n",
        "\n",
        "# check them\n",
        "for row in difference:\n",
        "    print(str(row) + \"\\t\\t\" + str(df_combined.loc[row, \"Label\"]))\n",
        "\n",
        "# Find the cells with NaN and after the rows for them\n",
        "is_NaN = df_combined.isnull()\n",
        "row_has_NaN = is_NaN.any(axis = 1)\n",
        "rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "# Replace them\n",
        "df_combined = df_combined.replace(np.nan, \" \")\n",
        "\n",
        "# Check the process\n",
        "is_NaN = df_combined.isnull()\n",
        "row_has_NaN = is_NaN.any(axis = 1)\n",
        "rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "assert len(rows_with_NaN) is 0\n",
        "\n",
        "# Get column names\n",
        "combined_column_names = []\n",
        "for column in df_combined.columns:\n",
        "  combined_column_names.append(column)\n",
        "\n",
        "# 2D array creation for the news based on macros\n",
        "COLUMNS = len(df_combined)\n",
        "news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \n",
        "\n",
        "# Show the column names\n",
        "print(\"Column names of the dataset:\") \n",
        "print(combined_column_names)\n",
        "\n",
        "# Merge the news\n",
        "for row in range(len(df_combined)):\n",
        "  for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "    temp = \"\"\n",
        "    news = \"\"\n",
        "    for word in range(ROWS):\n",
        "      news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\n",
        "      # Remove the b character at the begining of the string\n",
        "      if news[0] is \"b\":\n",
        "        news = \" \" + news[1:]\n",
        "      temp = temp + \" \" + news\n",
        "    news_sum[column][row] = temp\n",
        "\n",
        "# Show the first day second package of the news\n",
        "print(\"\\nThe first day second package of the news:\")\n",
        "print(news_sum[0][0])\n",
        "\n",
        "# Drop the old columns\n",
        "for column in range(len(combined_column_names) - 1):\n",
        "  df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\n",
        "\n",
        "# Create the new columns with the merged news\n",
        "for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "  colum_name = \"News_\" + str(column + 1)\n",
        "  df_combined[colum_name] = news_sum[column]\n",
        "\n",
        "# The label column \n",
        "LABEL_COLUMN = 0\n",
        "\n",
        "news_sum = []\n",
        "label_sum = []\n",
        "\n",
        "# Get the column names\n",
        "combined_column_names = []\n",
        "for column in df_combined.columns:\n",
        "  combined_column_names.append(column)\n",
        "\n",
        "# Write out the column names \n",
        "print(combined_column_names)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Connect the merged news with the labels\n",
        "for column in range(len(df_combined)):\n",
        "  for row in range(len(combined_column_names) - 1):\n",
        "    news_sum.append(df_combined[combined_column_names[row + 1]][column])\n",
        "    label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\n",
        "\n",
        "# Create the new DataFrame\n",
        "df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\n",
        "df_sum_news_labels[\"News\"] = news_sum  \n",
        "\n",
        "# Removing punctuations\n",
        "temp_news = []\n",
        "for line in news_sum:\n",
        "  temp_attach = \"\"\n",
        "  for word in line:\n",
        "    temp = \" \"\n",
        "    if word not in string.punctuation:\n",
        "      temp = word\n",
        "    temp_attach = temp_attach + \"\".join(temp)\n",
        "  temp_news.append(temp_attach)\n",
        "\n",
        "news_sum = temp_news\n",
        "temp_news = []\n",
        "\n",
        "# Remove numbers\n",
        "for line in news_sum:\n",
        "  temp_attach = \"\"\n",
        "  for word in line:\n",
        "    temp = \" \"\n",
        "    if not word.isdigit():\n",
        "      temp = word\n",
        "    temp_attach = temp_attach + \"\".join(temp)\n",
        "  temp_news.append(temp_attach)\n",
        "\n",
        "# Remove space\n",
        "for line in range(len(temp_news)):    \n",
        "  temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "# Converting headlines to lower case\n",
        "for line in range(len(temp_news)): \n",
        "    temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "# Update the data frame\n",
        "df_sum_news_labels[\"News\"] = temp_news\n",
        "\n",
        "# Load the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_sentence = []\n",
        "news_sum = df_sum_news_labels[\"News\"]\n",
        "\n",
        "# Remove stop words\n",
        "for line in news_sum:\n",
        "  word_tokens = word_tokenize(line)\n",
        "  temp_attach = \"\"\n",
        "  for word in word_tokens:\n",
        "    temp = \" \"\n",
        "    if not word in stop_words:\n",
        "      temp = temp + word\n",
        "    temp_attach = temp_attach + \"\".join(temp)\n",
        "  filtered_sentence.append(temp_attach)\n",
        "\n",
        "# Remove space\n",
        "for line in range(len(filtered_sentence)):    \n",
        "  filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "# Update the data frame\n",
        "df_sum_news_labels[\"News\"] = filtered_sentence\n",
        "\n",
        "news_sum = df_sum_news_labels[\"News\"]\n",
        "null_indexes = []\n",
        "index = 0\n",
        "\n",
        "for line in news_sum:\n",
        "  if line is \"\":\n",
        "    null_indexes.append(index)\n",
        "  index = index + 1\n",
        "\n",
        "print(null_indexes)\n",
        "\n",
        "for row in null_indexes:\n",
        "  df_sum_news_labels = df_sum_news_labels.drop(row)\n",
        "\n",
        "news_sum = df_sum_news_labels[\"News\"]\n",
        "null_indexes = []\n",
        "index = 0\n",
        "\n",
        "for line in news_sum:\n",
        "  if line is \"\":\n",
        "    null_indexes.append(index)\n",
        "  index = index + 1\n",
        "  \n",
        "assert len(null_indexes) is 0\n",
        "\n",
        "# Do the shuffle\n",
        "for i in range(SHUFFLE_CYCLE):\n",
        "  df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\n",
        "\n",
        "# Reset the index\n",
        "df_sum_news_labels.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Show the data frame\n",
        "print(df_sum_news_labels.head())\n",
        "\n",
        "INPUT_SIZE = len(df_sum_news_labels)\n",
        "TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\n",
        "\n",
        "# Split the dataset\n",
        "train = df_sum_news_labels[:TRAIN_SIZE] \n",
        "test = df_sum_news_labels[TRAIN_SIZE:]\n",
        "\n",
        "# Print out the length\n",
        "print(\"Train data set length: \" + str(len(train)))\n",
        "print(\"Test data set length: \" + str(len(test)))\n",
        "print(\"Split summa: \" + str(len(train) + len(test)))\n",
        "print(\"Dataset summa before split: \" + str(len(df_sum_news_labels)))\n",
        "\n",
        "# check\n",
        "split_sum = len(train) + len(test)\n",
        "sum = len(df_sum_news_labels)\n",
        "assert split_sum == sum\n",
        "\n",
        "print(train.tail(1))\n",
        "\n",
        "print(test.head(1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lengths are the same!\n",
            "The dates matched!\n",
            "Problem at day 2010-10-14\n",
            "Today: 11096.919921875\t\tYesterday: 11096.080078125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-12\n",
            "Today: 12815.080078125\t\tYesterday: 12815.3896484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-15\n",
            "Today: 12570.9501953125\t\tYesterday: 12570.9501953125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2013-04-12\n",
            "Today: 14865.0595703125\t\tYesterday: 14865.1396484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2014-04-24\n",
            "Today: 16501.650390625\t\tYesterday: 16501.650390625\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-08-12\n",
            "Today: 17402.509765625\t\tYesterday: 17402.83984375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-11-27\n",
            "Today: 17813.390625\t\tYesterday: 17813.390625\t\tLabel: 0\n",
            "\n",
            "All differences: 7\n",
            "2010-10-14\t\t1\n",
            "2012-11-12\t\t1\n",
            "2012-11-15\t\t1\n",
            "2013-04-12\t\t1\n",
            "2014-04-24\t\t1\n",
            "2015-08-12\t\t1\n",
            "2015-11-27\t\t1\n",
            "Column names of the dataset:\n",
            "['Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7', 'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15', 'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23', 'Top24', 'Top25']\n",
            "\n",
            "The first day second package of the news:\n",
            "  \"Georgia 'downs two Russian warplanes' as countries move to brink of war\"  'BREAKING: Musharraf to be impeached.'  'Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)'  'Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire'  \"Afghan children raped with 'impunity,' U.N. official says - this is sick, a three year old was raped and they do nothing\"  '150 Russian tanks have entered South Ossetia whilst Georgia shoots down two Russian jets.'  \"Breaking: Georgia invades South Ossetia, Russia warned it would intervene on SO's side\"  \"The 'enemy combatent' trials are nothing but a sham: Salim Haman has been sentenced to 5 1/2 years, but will be kept longer anyway just because they feel like it.\"\n",
            "['Label', 'News_1', 'News_2', 'News_3']\n",
            "\n",
            "\n",
            "[]\n",
            "   Label                                               News\n",
            "0      1  air france aircraft carrying people disappeare...\n",
            "1      1  canadian dollar gt us dollar uk election today...\n",
            "2      0  nude selfie celebs dumb eu commissioner says e...\n",
            "3      0  nasa fermi satellite completes map sky finds m...\n",
            "4      1  wikileaks releases video journalist murder mis...\n",
            "Train data set length: 4475\n",
            "Test data set length: 1492\n",
            "Split summa: 5967\n",
            "Dataset summa before split: 5967\n",
            "      Label                                               News\n",
            "4474      0  rescued girls daring attack boko haram hideout...\n",
            "      Label                                               News\n",
            "4475      0  friend honduras sent email videos pictures mil...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "AhetylUFGkVX",
        "outputId": "fba2e1b7-3b01-46a0-eeeb-51c4c1673726"
      },
      "source": [
        "train[\"News\"][0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'air france aircraft carrying people disappeared radar atlantic ocean brazil tell difference israeli palestinian former german mp judge offers reward prosecution bush cheney rumsfeld blair north korea starts landing exercises using amphibious vessels may planning attack south korean island president el salvador sends son france escape violence native el salvador son stabbed death awl parisian bridge random act violence apparent motive indonesian model married malaysian prince says kidnapped drugged sexually abused royal family escapes help singaporean police attack liberty untold story israel deadly assault u spy ship book review el salvador first leftist president takes power hillary clinton attended inauguration'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfNrDCKhGte5"
      },
      "source": [
        "train_X = train[\"News\"]\n",
        "train_X = train_X.to_numpy()\n",
        "train_Y = train[\"Label\"]\n",
        "train_Y = train_Y.to_numpy()\n",
        "\n",
        "test_X = test[\"News\"]\n",
        "test_X = test_X.to_numpy()\n",
        "test_Y = test[\"Label\"]\n",
        "test_Y = test_Y.to_numpy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ARCTZEnXRIo3",
        "outputId": "26567246-0fb3-4e48-9f30-19a32d5f1e22"
      },
      "source": [
        "train_X[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'air france aircraft carrying people disappeared radar atlantic ocean brazil tell difference israeli palestinian former german mp judge offers reward prosecution bush cheney rumsfeld blair north korea starts landing exercises using amphibious vessels may planning attack south korean island president el salvador sends son france escape violence native el salvador son stabbed death awl parisian bridge random act violence apparent motive indonesian model married malaysian prince says kidnapped drugged sexually abused royal family escapes help singaporean police attack liberty untold story israel deadly assault u spy ship book review el salvador first leftist president takes power hillary clinton attended inauguration'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1_rhBfHRHG3",
        "outputId": "1994c1f5-93b4-4469-b879-401d802a6b36"
      },
      "source": [
        "train_Y[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "QEZaPhIdbyY_",
        "outputId": "110066d9-08df-406d-d88b-4fc80d236e5a"
      },
      "source": [
        "test_X[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'friend honduras sent email videos pictures military firing peaceful protesters uploaded eyewitness account everyone see please let disappear one else reporting watch israeli tv footage settlers attacking r npeace activists tv crew iran people hanged less week girls tricked us sex tarot card iranian reformists planning strike please let know watching fewer us killed america stops paying attention regime loses restraint mir hossein moussavi mehdi karroubi amp mohammad khatami joint statement iranian opposition leaders criticized describe security state asking arrested released official internet cut xinjiang prevent riot spreading'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5cxyILLb0Ai",
        "outputId": "b0ebfcf0-f93c-4443-a715-86c06f5fbd4f"
      },
      "source": [
        "test_Y[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9mLGR5GCwe"
      },
      "source": [
        "## **Classifiers try out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t600--tZH5eC"
      },
      "source": [
        "names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVC\", \n",
        "         \"Linear SVC 2\", \"SVC\", \"Decision Tree\", \"Random Forest\", \"Neural Net\"]\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER),\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025, random_state=RANDOM_SEED),\n",
        "    LinearSVC(random_state=RANDOM_SEED),\n",
        "    SVC(gamma=2, C=1, random_state=RANDOM_SEED),\n",
        "    DecisionTreeClassifier(max_depth=10, random_state=RANDOM_SEED),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=RANDOM_SEED),\n",
        "    MLPClassifier(alpha=1, max_iter=10, random_state=RANDOM_SEED)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwAnzTdBpAAd"
      },
      "source": [
        "text_clf_all = []\n",
        "\n",
        "for classifier in classifiers:\n",
        "    text_clf_temp = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('clf', classifier),\n",
        "    ])\n",
        "    text_clf_all.append(text_clf_temp)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoBS3y5MH_QW",
        "outputId": "9b2fdcfe-d331-4bdc-80f9-68e64f7d3947"
      },
      "source": [
        "score_array = []\n",
        "\n",
        "# iterate over classifiers\n",
        "for name, text_clf in zip(names, text_clf_all):\n",
        "    print(name)\n",
        "    text_clf = text_clf.fit(train_X, train_Y)\n",
        "\n",
        "    prediction = text_clf.predict(test_X)\n",
        "\n",
        "    print(classification_report(test_Y, prediction))\n",
        "    print(accuracy_score(test_Y, prediction))\n",
        "\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "\n",
        "    score = text_clf.score(test_X, test_Y)\n",
        "    score_array.append(score)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.53      0.50       632\n",
            "           1       0.62      0.58      0.60       860\n",
            "\n",
            "    accuracy                           0.56      1492\n",
            "   macro avg       0.55      0.55      0.55      1492\n",
            "weighted avg       0.56      0.56      0.56      1492\n",
            "\n",
            "0.5563002680965148\n",
            "------------------------------------------------------------------\n",
            "Linear SVC\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       632\n",
            "           1       0.58      1.00      0.73       860\n",
            "\n",
            "    accuracy                           0.58      1492\n",
            "   macro avg       0.29      0.50      0.37      1492\n",
            "weighted avg       0.33      0.58      0.42      1492\n",
            "\n",
            "0.5764075067024129\n",
            "------------------------------------------------------------------\n",
            "Linear SVC 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.43      0.43       632\n",
            "           1       0.59      0.59      0.59       860\n",
            "\n",
            "    accuracy                           0.52      1492\n",
            "   macro avg       0.51      0.51      0.51      1492\n",
            "weighted avg       0.52      0.52      0.52      1492\n",
            "\n",
            "0.5227882037533512\n",
            "------------------------------------------------------------------\n",
            "SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       632\n",
            "           1       0.58      1.00      0.73       860\n",
            "\n",
            "    accuracy                           0.58      1492\n",
            "   macro avg       0.29      0.50      0.37      1492\n",
            "weighted avg       0.33      0.58      0.42      1492\n",
            "\n",
            "0.5750670241286864\n",
            "------------------------------------------------------------------\n",
            "Decision Tree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.14      0.22       632\n",
            "           1       0.58      0.86      0.69       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.50      0.50      0.45      1492\n",
            "weighted avg       0.51      0.55      0.49      1492\n",
            "\n",
            "0.5549597855227882\n",
            "------------------------------------------------------------------\n",
            "Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.00      0.01       632\n",
            "           1       0.58      0.99      0.73       860\n",
            "\n",
            "    accuracy                           0.58      1492\n",
            "   macro avg       0.48      0.50      0.37      1492\n",
            "weighted avg       0.49      0.58      0.42      1492\n",
            "\n",
            "0.5750670241286864\n",
            "------------------------------------------------------------------\n",
            "Neural Net\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       632\n",
            "           1       0.58      1.00      0.73       860\n",
            "\n",
            "    accuracy                           0.58      1492\n",
            "   macro avg       0.29      0.50      0.37      1492\n",
            "weighted avg       0.33      0.58      0.42      1492\n",
            "\n",
            "0.5764075067024129\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-mWGZeuQa9C"
      },
      "source": [
        "## **Classifiers grid search** (commented out, saved to onenote)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj6ENBscgPrt",
        "outputId": "afe8bb33-339f-426f-e225-317e7c93c27f"
      },
      "source": [
        "#text_clf_all[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 2), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=1000,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=1234,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hFAIgTaTiyG",
        "outputId": "834e6054-a833-4409-e375-e34498f44595"
      },
      "source": [
        "# try out with logistic regression\n",
        "#parameters_svm = {\n",
        "#                  'vect__ngram_range': [(1,1), (1,2), (1,3), (2,3), (3,3)],\n",
        "#                  'tfidf__use_idf': (True, False),\n",
        "#                  'tfidf__norm': ('l1', 'l2'),     \n",
        "#                  'clf__penalty': ('l1', 'l2', 'elasticnet', 'none'),   \n",
        "#                  'clf__C': np.linspace(start=0, stop=10, num=101), \n",
        "#                  'clf__solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
        "#                  'clf__l1_ratio': np.linspace(0, 1, 101)\n",
        "#}\n",
        "\n",
        "#keys = parameters_svm.keys()\n",
        "\n",
        "#parameters_array = []\n",
        "\n",
        "#for key in keys:\n",
        "#      dict_temp = {key:parameters_svm[key]}\n",
        "#      parameters_array.append(dict_temp)\n",
        "\n",
        "#best_params_array = []\n",
        "#df_cv_results_array = []\n",
        "#score_array = []\n",
        "\n",
        "#for parameter_svm in parameters_array:\n",
        "#    gs_clf = GridSearchCV(text_clf_all[0], parameter_svm, n_jobs=-1, verbose=4, cv=3)\n",
        "#    gs_clf = gs_clf.fit(train_X, train_Y)\n",
        "\n",
        "#    print(\"\")\n",
        "#    print(gs_clf.best_score_)\n",
        "#    print(\"\")\n",
        "#    print(gs_clf.best_params_)\n",
        "#    best_params_array.append(gs_clf.best_params_)\n",
        "\n",
        "#    df_cv_results_array.append(pd.DataFrame(gs_clf.cv_results_))\n",
        "\n",
        "#    prediction = gs_clf.predict(test_X)\n",
        "\n",
        "#    print(classification_report(test_Y, prediction))\n",
        "#    print(accuracy_score(test_Y, prediction))\n",
        "#    score_array.append(accuracy_score(test_Y, prediction))\n",
        "\n",
        "#    print(\"------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   36.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5356440100238008\n",
            "\n",
            "{'vect__ngram_range': (2, 3)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.06      0.10       632\n",
            "           1       0.57      0.93      0.71       860\n",
            "\n",
            "    accuracy                           0.56      1492\n",
            "   macro avg       0.47      0.49      0.40      1492\n",
            "weighted avg       0.49      0.56      0.45      1492\n",
            "\n",
            "0.5603217158176944\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   16.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5325162173517722\n",
            "\n",
            "{'tfidf__use_idf': True}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   13.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5325162173517722\n",
            "\n",
            "{'tfidf__norm': 'l2'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   23.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5325162173517722\n",
            "\n",
            "{'clf__penalty': 'l2'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 101 candidates, totalling 303 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   42.9s\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  9.5min\n",
            "[Parallel(n_jobs=-1)]: Done 303 out of 303 | elapsed: 13.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5363144011522217\n",
            "\n",
            "{'clf__C': 0.8}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.22      0.29       632\n",
            "           1       0.58      0.79      0.67       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.51      0.51      0.48      1492\n",
            "weighted avg       0.52      0.55      0.51      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   29.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5325162173517722\n",
            "\n",
            "{'clf__solver': 'newton-cg'}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n",
            "Fitting 3 folds for each of 101 candidates, totalling 303 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   53.0s\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=-1)]: Done 303 out of 303 | elapsed: 12.2min finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1501: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  \"(penalty={})\".format(self.penalty))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5325162173517722\n",
            "\n",
            "{'clf__l1_ratio': 0.0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.28      0.34       632\n",
            "           1       0.59      0.75      0.66       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.52      0.51      0.50      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5495978552278821\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrA0u3EqHRt-",
        "outputId": "e88b219b-1b79-44bc-82e3-f280f809db64"
      },
      "source": [
        "#best_params_array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'vect__ngram_range': (2, 3)},\n",
              " {'tfidf__use_idf': True},\n",
              " {'tfidf__norm': 'l2'},\n",
              " {'clf__penalty': 'l2'},\n",
              " {'clf__C': 0.8},\n",
              " {'clf__solver': 'newton-cg'},\n",
              " {'clf__l1_ratio': 0.0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyb_Lr1bHU4Z",
        "outputId": "41d6149b-2198-4de3-dd8f-ffc380e96c39"
      },
      "source": [
        "#test_clf = Pipeline([('vect', CountVectorizer(ngram_range=(2,3))),\n",
        "#                    ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),\n",
        "#                    ('clf', LogisticRegression(penalty='l2', C=0.8, solver='newton-cg',\n",
        "#                                               l1_ratio=0, random_state=RANDOM_SEED, max_iter=MAX_ITER),),\n",
        "#])\n",
        "\n",
        "#test_clf = test_clf.fit(train_X, train_Y)\n",
        "\n",
        "#prediction = test_clf.predict(test_X)\n",
        "\n",
        "#print(classification_report(test_Y, prediction))\n",
        "#print(accuracy_score(test_Y, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1501: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  \"(penalty={})\".format(self.penalty))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.03      0.06       632\n",
            "           1       0.57      0.96      0.72       860\n",
            "\n",
            "    accuracy                           0.57      1492\n",
            "   macro avg       0.48      0.50      0.39      1492\n",
            "weighted avg       0.49      0.57      0.44      1492\n",
            "\n",
            "0.5670241286863271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW2j7vJSMslH"
      },
      "source": [
        "## **Vote classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UctHDngyOwFK"
      },
      "source": [
        "names = [\"Logistic Regression\", \"Nearest Neighbors\", \n",
        "         \"Linear SVC 2\"]\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER),\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(max_depth=10, random_state=RANDOM_SEED)\n",
        "    ]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DXgXe_0O0KL",
        "outputId": "79c3e54d-ec69-4cec-cf29-de38d9cfcbf6"
      },
      "source": [
        "# soft vs hard\n",
        "estimators_tuple = []\n",
        "for name, classifier in zip(names, classifiers):\n",
        "    estimators_tuple.append((name, classifier))\n",
        "\n",
        "estimators_tuple = tuple(estimators_tuple)  \n",
        "\n",
        "voting_array = ['hard', 'soft']\n",
        "eclf_array = []\n",
        "text_eclf_array = []\n",
        "\n",
        "for index in range(2):\n",
        "    eclf = VotingClassifier(estimators=estimators_tuple, voting=voting_array[index])\n",
        "    eclf_array.append(eclf)\n",
        "    text_eclf_array.append(Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                              ('tfidf', TfidfTransformer()),\n",
        "                              ('clf', eclf),\n",
        "    ]))\n",
        "\n",
        "score_vote_array = []\n",
        "\n",
        "for index in range(2):\n",
        "    text_eclf_array[index] = text_eclf_array[index].fit(train_X, train_Y)\n",
        "\n",
        "    prediction = text_eclf_array[index].predict(test_X)\n",
        "\n",
        "    print(classification_report(test_Y, prediction))\n",
        "    print(accuracy_score(test_Y, prediction))\n",
        "    print(\"------------------------------------------------------------------\")   \n",
        "\n",
        "    score_vote_array.append(accuracy_score(test_Y, prediction))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.23      0.30       632\n",
            "           1       0.58      0.79      0.67       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.51      0.51      0.49      1492\n",
            "weighted avg       0.53      0.55      0.52      1492\n",
            "\n",
            "0.5529490616621984\n",
            "------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.49      0.48       632\n",
            "           1       0.61      0.59      0.60       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.54      0.54      0.54      1492\n",
            "weighted avg       0.55      0.55      0.55      1492\n",
            "\n",
            "0.546916890080429\n",
            "------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVn1fV7lNfr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3712e23-e02c-4525-ea5e-6060d43bedcd"
      },
      "source": [
        "# play with weights\n",
        "# soft\n",
        "text_eclf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                          ('tfidf', TfidfTransformer()),\n",
        "                          ('clf', VotingClassifier(estimators=estimators_tuple, voting='soft', weights=None)),\n",
        "])\n",
        "\n",
        "# 0, 0.5, 1.0, 1.5, 2.0 -> 5*5*5 = 125\n",
        "weight_element = [0, 0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "weight_element = [0, 0.5, 1.0, 1.5, 2.0]\n",
        "weight_elemet_array = []\n",
        "\n",
        "for i in range(len(weight_element)):\n",
        "    for j in range(len(weight_element)):\n",
        "        for k in range(len(weight_element)):\n",
        "            if weight_element[i] == weight_element[j] == 0:\n",
        "                pass\n",
        "            elif weight_element[i] == weight_element[k] == 0:\n",
        "                pass\n",
        "            elif weight_element[j] == weight_element[k] == 0:\n",
        "                pass\n",
        "            else:             \n",
        "                weight_elemet_array.append((weight_element[i],weight_element[j],weight_element[k]),)\n",
        "\n",
        "weight_elemet_array\n",
        "\n",
        "parameters_svm = {\n",
        "                  'clf__weights': weight_elemet_array,\n",
        "}\n",
        "\n",
        "best_params = []\n",
        "df_cv_results = []\n",
        "score = []\n",
        "\n",
        "gs_clf = GridSearchCV(text_eclf, parameters_svm, n_jobs=-1, verbose=4, cv=3)\n",
        "gs_clf = gs_clf.fit(train_X, train_Y)\n",
        "\n",
        "print(\"\")\n",
        "print(gs_clf.best_score_)\n",
        "print(\"\")\n",
        "print(gs_clf.best_params_)\n",
        "best_params = gs_clf.best_params_\n",
        "\n",
        "df_cv_results = pd.DataFrame(gs_clf.cv_results_)\n",
        "\n",
        "prediction = gs_clf.predict(test_X)\n",
        "\n",
        "print(classification_report(test_Y, prediction))\n",
        "print(accuracy_score(test_Y, prediction))\n",
        "score = accuracy_score(test_Y, prediction)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.3min\n",
            "[Parallel(n_jobs=-1)]: Done 336 out of 336 | elapsed: 22.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5472634736030121\n",
            "\n",
            "{'clf__weights': (2.0, 0.5, 0)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.41      0.44       632\n",
            "           1       0.60      0.66      0.63       860\n",
            "\n",
            "    accuracy                           0.55      1492\n",
            "   macro avg       0.54      0.53      0.53      1492\n",
            "weighted avg       0.55      0.55      0.55      1492\n",
            "\n",
            "0.5536193029490617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sPS92YxyA7k",
        "outputId": "b6b59134-7e28-4a01-9662-e4dd9fe07ebe"
      },
      "source": [
        "# play with weights\n",
        "# hard\n",
        "text_eclf= Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
        "                          ('tfidf', TfidfTransformer()),\n",
        "                          ('clf', VotingClassifier(estimators=estimators_tuple, voting='hard', weights=None)),\n",
        "])\n",
        "\n",
        "# 0, 0.5, 1.0, 1.5, 2.0 -> 5*5*5 = 125\n",
        "weight_element = [0, 0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "weight_element = [0, 0.5, 1.0, 1.5, 2.0]\n",
        "weight_elemet_array = []\n",
        "\n",
        "for i in range(len(weight_element)):\n",
        "    for j in range(len(weight_element)):\n",
        "        for k in range(len(weight_element)):\n",
        "            if weight_element[i] == weight_element[j] == 0:\n",
        "                pass\n",
        "            elif weight_element[i] == weight_element[k] == 0:\n",
        "                pass\n",
        "            elif weight_element[j] == weight_element[k] == 0:\n",
        "                pass\n",
        "            else:             \n",
        "                weight_elemet_array.append((weight_element[i],weight_element[j],weight_element[k]),)\n",
        "\n",
        "weight_elemet_array\n",
        "\n",
        "parameters_svm = {\n",
        "                  'clf__weights': weight_elemet_array,\n",
        "}\n",
        "\n",
        "best_params = []\n",
        "df_cv_results = []\n",
        "score = []\n",
        "\n",
        "gs_clf = GridSearchCV(text_eclf, parameters_svm, n_jobs=-1, verbose=4, cv=3)\n",
        "gs_clf = gs_clf.fit(train_X, train_Y)\n",
        "\n",
        "print(\"\")\n",
        "print(gs_clf.best_score_)\n",
        "print(\"\")\n",
        "print(gs_clf.best_params_)\n",
        "best_params = gs_clf.best_params_\n",
        "\n",
        "df_cv_results = pd.DataFrame(gs_clf.cv_results_)\n",
        "\n",
        "prediction = gs_clf.predict(test_X)\n",
        "\n",
        "print(classification_report(test_Y, prediction))\n",
        "print(accuracy_score(test_Y, prediction))\n",
        "score = accuracy_score(test_Y, prediction)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 112 candidates, totalling 336 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed: 14.5min\n",
            "[Parallel(n_jobs=-1)]: Done 336 out of 336 | elapsed: 22.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.5436873549908326\n",
            "\n",
            "{'clf__weights': (0, 1.0, 0.5)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.53      0.50       632\n",
            "           1       0.62      0.58      0.60       860\n",
            "\n",
            "    accuracy                           0.56      1492\n",
            "   macro avg       0.55      0.55      0.55      1492\n",
            "weighted avg       0.56      0.56      0.56      1492\n",
            "\n",
            "0.5563002680965148\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}