{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogReg_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ygDjIPBVTLl"
      },
      "source": [
        "# **Stock market news feed semantic analysis** *(Baseline LogReg)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kLvL-AJVg3C"
      },
      "source": [
        "Ebben a notebookban az eddigi általam kibányászott, megszerzett adathalmazokat fogom a hagyományos bag of words és logistic regression módszerrel megvizsgálni. Ezek után n-gram modelleket is ki fogok próbálni. Az általa használt források és referenciák az eredményekhez:\n",
        "\n",
        "\n",
        "*   https://colab.research.google.com/drive/1QPrBkh-KwX6qcUtiNWKp9rJoneBfGEVh#scrollTo=bQUJwMjYYN4- *(saját munka - átdolgozott)*\n",
        "*   https://colab.research.google.com/drive/1MdpXGCj2fb3g1BI_XfF54OWLkYQCZBBy#scrollTo=LndWT2Kn-UMK *(saját baseline munka)*\n",
        "*   https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit#Basic-Model-Training-and-Testing\n",
        "*   https://www.kaggle.com/lseiyjg/use-news-to-predict-stock-markets\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvCnWXPfXgZi"
      },
      "source": [
        "A használt adathalmazok alapján külön fejezeteket készítek és mindenhol jelzem a forrását és a megszerzésének a módját, ha saját bányászás eredménye."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsAeJYBEXzW_"
      },
      "source": [
        "## **A projekt előkészítése**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuHwIVcIX2Zp"
      },
      "source": [
        "A Drive csatlakoztatása a szükséges fájlok későbbi betöltésére. A betöltés közvetlen a használat előtt fogom megtenni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvU8LOxdKH5-",
        "outputId": "c6c5ce34-cf4a-4877-812f-f9e52ff72ad7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOuSSouYIb7"
      },
      "source": [
        "A szükséges könyvtárak betöltése a projekthez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgUBe7_VYLjt",
        "outputId": "07f37350-f728-4345-d8cc-5e32d6f7e564"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "from numpy.random import MT19937\n",
        "from numpy.random import RandomState, SeedSequence\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiFx9dUgBsN"
      },
      "source": [
        "A projektben használt makrók definiálása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bULVBJPegGcc"
      },
      "source": [
        "# Shuffle cycle number for the dataframe\n",
        "SHUFFLE_CYCLE = 500\n",
        "\n",
        "# Which dataset will be used\n",
        "DATASET = 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfWZE7fZzcF"
      },
      "source": [
        "A reprodukálhatóság miatt definiálok egy seed-et a véletlen szám generátorhoz, amit a továbbiakban használni fogok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEjH22olZ1L_"
      },
      "source": [
        "# Random seed\n",
        "RANDOM_SEED = 1234\n",
        "\n",
        "# Numpy random seed\n",
        "NP_SEED = 1234\n",
        "\n",
        "# Max iteration for training\n",
        "MAX_ITER = 100000\n",
        "\n",
        "# Train size\n",
        "TRAIN_SPLIT = 0.80\n",
        "\n",
        "# Test size\n",
        "TEST_SPLIT = 0.2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw003siyimeD"
      },
      "source": [
        "np.random.seed(NP_SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoE6btiUYg-a"
      },
      "source": [
        "## **KAG_REDDIT_WRLD_DJIA_DF (1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jP3czjYt6c"
      },
      "source": [
        "Ez az adathalmaz a top25 hírt tartalmazza a Reddit World News kategóriából 2008.08.08-2016.07.01 időtartamban. Ez nem általam gyűjtött adathalmaz, a forrása:\n",
        "Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved 2021.02.19. from https://www.kaggle.com/aaron7sun/stocknews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_wdhA1ZSrt"
      },
      "source": [
        "Az adathalmaz betöltése a csatlakoztatott Drive-omból."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw00CWOBZbeF"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Copy the dataset to the local environment\n",
        "    !cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\n",
        "\n",
        "    # Check the copy is succesfull -> good if no assertation error\n",
        "    read = !ls\n",
        "    assert read[0].find(\"Combined_News_DJIA.csv\") != -1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_uQPC2Mt6F"
      },
      "source": [
        "Az eredmények elmentésére és indexelésére az alábbi két tömböt fogom hasnzálni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ria16HC2MyZa"
      },
      "source": [
        "if DATASET == 1:\n",
        "    model_type = [\"Bag of words\", \"1,2 n-gram\", \"2,2 n-gram\", \n",
        "                  \"1,3 n-gram\", \"2,3 n-gram\", \"3,3 n-gram\"]\n",
        "\n",
        "    result = []              "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z73YJGnjYxAz"
      },
      "source": [
        "Makró definiálás."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lMyUJerYzAg"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Number of merged news into one string\n",
        "    ROWS = 2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1DAhyIob0Bm"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KSUSX1b4-z"
      },
      "source": [
        "Az adathalmaz betöltése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPmTqk3Gb2Fo",
        "outputId": "d1aba3f6-d8a9-4b37-994b-d4ed28470d19"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Load the dataset \n",
        "    df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\n",
        "\n",
        "    # Show the dataframe\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
            "2008-08-11      1  ...  b\"So this is what it's come to: trading sex fo...\n",
            "2008-08-12      0  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
            "2008-08-13      0  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
            "2008-08-14      1  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
            "\n",
            "[5 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdvD8SIQcPB6"
      },
      "source": [
        "Érdekességképpen a következőkben megvizsgálom, hogy az adathalmaz címkéi megfelelőek. A forrás szerint a címke 1, ha nőtt vagy azonos maradt az érték azon a napon, illetve 0, ha csökkent. (Adj Close adott napi értéke az előző napihoz viszonyítva)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj26KF2ncgji",
        "outputId": "c381e28f-fab3-46ed-8daa-b6016b236fce"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Load the stock data\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \n",
        "                              end=\"2016-07-01\")\n",
        "    \n",
        "    # Show the stock data\n",
        "    print(df_stock.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    High           Low  ...      Volume     Adj Close\n",
            "Date                                    ...                          \n",
            "2008-08-08  11808.490234  11344.230469  ...  4966810000  11734.320312\n",
            "2008-08-11  11933.549805  11580.190430  ...  5067310000  11782.349609\n",
            "2008-08-12  11830.389648  11541.429688  ...  4711290000  11642.469727\n",
            "2008-08-13  11689.049805  11377.370117  ...  4787600000  11532.959961\n",
            "2008-08-14  11744.330078  11399.839844  ...  4064000000  11615.929688\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9UbGo05jACh"
      },
      "source": [
        "Az dátumok formátumát egységesre hozom az összehasonlítás érdekében."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6UkTz6Vg23F",
        "outputId": "c0b54938-826e-4906-b160-54d3d3be1eeb"
      },
      "source": [
        "if DATASET == 1:\n",
        "    temp_day = []\n",
        "\n",
        "    for day in range(len(df_stock)):\n",
        "        temp_day.append(df_stock.index[day].date())\n",
        "\n",
        "    df_stock.index = temp_day\n",
        "\n",
        "    # Show the stock data\n",
        "    print(df_stock.head())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    High           Low  ...      Volume     Adj Close\n",
            "2008-08-08  11808.490234  11344.230469  ...  4966810000  11734.320312\n",
            "2008-08-11  11933.549805  11580.190430  ...  5067310000  11782.349609\n",
            "2008-08-12  11830.389648  11541.429688  ...  4711290000  11642.469727\n",
            "2008-08-13  11689.049805  11377.370117  ...  4787600000  11532.959961\n",
            "2008-08-14  11744.330078  11399.839844  ...  4064000000  11615.929688\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWYe1PpdCK3"
      },
      "source": [
        "Először a dátumok ellenőzöm, hogy megegyeznek-e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77SDqWWGdF31",
        "outputId": "43645312-a643-42e9-e204-3b71aa41ecf8"
      },
      "source": [
        "if DATASET == 1:\n",
        "    difference = []\n",
        "\n",
        "    if len(df_combined) == len(df_stock):\n",
        "        print(\"The lengths are the same!\")\n",
        "\n",
        "    for day in range(max(len(df_combined), len(df_stock))):\n",
        "        if str(df_combined.index[day]) != str(df_stock.index[day]):\n",
        "            print(\"There is difference at: \" + str(day) + \" index\")\n",
        "            print(\"News: \" + str(df_combined.index[day]) + \"\\tStock: \" + str(df_stock.index[day]))\n",
        "            difference.append(day)\n",
        "\n",
        "    if len(difference) is 0:\n",
        "        print(\"The dates matched!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lengths are the same!\n",
            "The dates matched!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJogdlwsjJm3"
      },
      "source": [
        "A labelek ellenőrzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyySRYjNjLwt",
        "outputId": "8dcd4eb7-5bd8-469e-ee72-389e94adf80f"
      },
      "source": [
        "if DATASET == 1:\n",
        "    difference = []\n",
        "\n",
        "    for day in range(len(df_stock)):\n",
        "        # label should be 1 -> rise\n",
        "        if int(df_stock[\"Adj Close\"][day]) >= int(df_stock[\"Adj Close\"][day - 1]):\n",
        "            if df_combined[\"Label\"][day] != 1:\n",
        "                difference.append(str(df_stock.index[day]))\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\n",
        "\n",
        "        # label should be 0 -> fall\n",
        "        if int(df_stock[\"Adj Close\"][day]) < int(df_stock[\"Adj Close\"][day - 1]):\n",
        "            if df_combined[\"Label\"][day] != 0:\n",
        "                difference.append(str(df_stock.index[day]))\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\n",
        "\n",
        "    print(\"All differences: \" + str(len(difference)))      "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem at day 2010-10-14\n",
            "Today: 11096.919921875\t\tYesterday: 11096.080078125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-12\n",
            "Today: 12815.080078125\t\tYesterday: 12815.3896484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-15\n",
            "Today: 12570.9501953125\t\tYesterday: 12570.9501953125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2013-04-12\n",
            "Today: 14865.0595703125\t\tYesterday: 14865.1396484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2014-04-24\n",
            "Today: 16501.650390625\t\tYesterday: 16501.650390625\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-08-12\n",
            "Today: 17402.509765625\t\tYesterday: 17402.83984375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-11-27\n",
            "Today: 17813.390625\t\tYesterday: 17813.390625\t\tLabel: 0\n",
            "\n",
            "All differences: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOI1OZJPU6Wh"
      },
      "source": [
        "Látható, hogy rossz a label pár helyen. Egy kis kutakodás után megtaláltam, hogy maga az árfolyam lekérdezésük volt hibás pár nap esetében, ezért ezeket javítom, majd elmentem a drive-omon a javítottat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHvqT2FRtBkW",
        "outputId": "94acbfc2-325f-4c38-a6dc-c6c51aad067c"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # correct the wrong labels\n",
        "    for row in difference:\n",
        "        if df_combined.loc[row, \"Label\"] == 0:\n",
        "            df_combined.loc[row, \"Label\"] = 1\n",
        "        else:\n",
        "            df_combined.loc[row, \"Label\"] = 0\n",
        "\n",
        "    # check them\n",
        "    for row in difference:\n",
        "        print(str(row) + \"\\t\\t\" + str(df_combined.loc[row, \"Label\"]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2010-10-14\t\t1\n",
            "2012-11-12\t\t1\n",
            "2012-11-15\t\t1\n",
            "2013-04-12\t\t1\n",
            "2014-04-24\t\t1\n",
            "2015-08-12\t\t1\n",
            "2015-11-27\t\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTIqNqucuiBO",
        "outputId": "31e31623-7a62-4a7e-9b5e-95b1a4ed3775"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # save to drive\n",
        "    df_combined.to_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv')\n",
        "\n",
        "    # Show the dataset\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
            "2008-08-11      1  ...  b\"So this is what it's come to: trading sex fo...\n",
            "2008-08-12      0  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
            "2008-08-13      0  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
            "2008-08-14      1  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
            "\n",
            "[5 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhg7d5Xj2kt"
      },
      "source": [
        "A következőkben az esetleges adat nélküli napokat, illetve cellákat keresem meg és helyettesítem őket egy üres sztringgel. Ez a későbbi szövegfeldolgozás hibamentességéhez szükséges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrFufwo6j5_H"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Find the cells with NaN and after the rows for them\n",
        "    is_NaN = df_combined.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "    # Replace them\n",
        "    df_combined = df_combined.replace(np.nan, \" \")\n",
        "\n",
        "    # Check the process\n",
        "    is_NaN = df_combined.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "    assert len(rows_with_NaN) is 0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl_3zICJFowc"
      },
      "source": [
        "if DATASET == 1:    \n",
        "    # the last 10 day save out for testing and comparing the models\n",
        "    df_for_test = df_combined.tail(10)\n",
        "    df_combined.drop(df_combined.tail(10).index,inplace=True) # drop last n rows"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxCOud0ki5M-"
      },
      "source": [
        "Ezek után az egy naphoz tartozó híreket közös sztringekbe fűzöm. Az egy sztringbe tartozó hírek számát makróval definiálom:\n",
        "\n",
        "\n",
        "*   ROWS - egymásba fűzött hírek száma\n",
        "\n",
        "Itt megtalálható már az első előkészítő algoritmusom, méghozzá a sztringek elején található b karakter eltávolítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqv3bDcAi6Bf",
        "outputId": "ee8a6e2d-3fc3-4cae-f2df-4dc4cfa55956"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Get column names\n",
        "    combined_column_names = []\n",
        "    for column in df_combined.columns:\n",
        "      combined_column_names.append(column)\n",
        "\n",
        "    # 2D array creation for the news based on macros\n",
        "    COLUMNS = len(df_combined)\n",
        "    news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \n",
        "\n",
        "    # Show the column names\n",
        "    print(\"Column names of the dataset:\") \n",
        "    print(combined_column_names)\n",
        "\n",
        "    # Merge the news\n",
        "    for row in range(len(df_combined)):\n",
        "      for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "        temp = \"\"\n",
        "        news = \"\"\n",
        "        for word in range(ROWS):\n",
        "          news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\n",
        "          # Remove the b character at the begining of the string\n",
        "          if news[0] is \"b\":\n",
        "            news = \" \" + news[1:]\n",
        "          temp = temp + \" \" + news\n",
        "        news_sum[column][row] = temp\n",
        "\n",
        "    # Show the first day second package of the news\n",
        "    print(\"\\nThe first day second package of the news:\")\n",
        "    print(news_sum[1][0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column names of the dataset:\n",
            "['Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7', 'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15', 'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23', 'Top24', 'Top25']\n",
            "\n",
            "The first day second package of the news:\n",
            "  'Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)'  'Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvOqG6vckIcF"
      },
      "source": [
        "Ezek után a korábbi oszlopokat(Top1, Top2...) kicserélem a csoportosításnak megfelelő számú oszlopokra és nevekre (News_1, News_2...), majd feltöltöm őket az összevont hírcsomagokkal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xljXPUwmkKrZ",
        "outputId": "dfd29400-e50b-4364-d7ff-ec470a3a0de3"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Drop the old columns\n",
        "    for column in range(len(combined_column_names) - 1):\n",
        "      df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\n",
        "\n",
        "    # Create the new columns with the merged news\n",
        "    for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "      colum_name = \"News_\" + str(column + 1)\n",
        "      df_combined[colum_name] = news_sum[column]\n",
        "\n",
        "    # Show the DataFrame\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                            News_12\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...    'Indian shoe manufactory  - And again in a s...\n",
            "2008-08-11      1  ...    'Perhaps *the* question about the Georgia - ...\n",
            "2008-08-12      0  ...    'Christopher King argues that the US and NAT...\n",
            "2008-08-13      0  ...    ' Quarter of Russians blame U.S. for conflic...\n",
            "2008-08-14      1  ...    'Russia: World  \"can forget about\" Georgia\\'...\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gmqmlpkqhm"
      },
      "source": [
        "Egy új dataframebe újracsoportosítom a hír blokkokat a címkéjükkel, már a dátumok nélkül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoeoCzPekrN2",
        "outputId": "378f73c4-ef94-4f97-c7e6-1400d7d564d6"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # The label column \n",
        "    LABEL_COLUMN = 0\n",
        "\n",
        "    news_sum = []\n",
        "    label_sum = []\n",
        "\n",
        "    # Get the column names\n",
        "    combined_column_names = []\n",
        "    for column in df_combined.columns:\n",
        "      combined_column_names.append(column)\n",
        "\n",
        "    # Write out the column names \n",
        "    print(combined_column_names)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Connect the merged news with the labels\n",
        "    for column in range(len(df_combined)):\n",
        "      for row in range(len(combined_column_names) - 1):\n",
        "        news_sum.append(df_combined[combined_column_names[row + 1]][column])\n",
        "        label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\n",
        "\n",
        "    # Create the new DataFrame\n",
        "    df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\n",
        "    df_sum_news_labels[\"News\"] = news_sum\n",
        "\n",
        "    # Show it\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Label', 'News_1', 'News_2', 'News_3', 'News_4', 'News_5', 'News_6', 'News_7', 'News_8', 'News_9', 'News_10', 'News_11', 'News_12']\n",
            "\n",
            "\n",
            "   Label                                               News\n",
            "0      0    \"Georgia 'downs two Russian warplanes' as co...\n",
            "1      0    'Russia Today: Columns of troops roll into S...\n",
            "2      0    \"Afghan children raped with 'impunity,' U.N....\n",
            "3      0    \"Breaking: Georgia invades South Ossetia, Ru...\n",
            "4      0    'Georgian troops retreat from S. Osettain ca...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQSgwmslqkO"
      },
      "source": [
        "Először a szövegek előfeldolgozásával kezdem: írásjelek eltávolítása, számok eltávolítása, felesleges szóközök eltávolítása, aztán minden szót kis kezdőbetűjü szóvá konvertálom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJT565FZlsgZ",
        "outputId": "2685927c-401f-44ac-d8a7-46a33c779253"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Removing punctuations\n",
        "    temp_news = []\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if word not in string.punctuation:\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    news_sum = temp_news\n",
        "    temp_news = []\n",
        "\n",
        "    # Remove numbers\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if not word.isdigit():\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(temp_news)):    \n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "    # Converting headlines to lower case\n",
        "    for line in range(len(temp_news)): \n",
        "        temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "    # Update the data frame\n",
        "    df_sum_news_labels[\"News\"] = temp_news\n",
        "\n",
        "    # Show it\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      0  georgia downs two russian warplanes as countri...\n",
            "1      0  russia today columns of troops roll into south...\n",
            "2      0  afghan children raped with impunity u n offici...\n",
            "3      0  breaking georgia invades south ossetia russia ...\n",
            "4      0  georgian troops retreat from s osettain capita...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCvCsvel989"
      },
      "source": [
        "A következőkben az úgy nevezett töltelék szavakat (stop words) fogom eltávolítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3WjnHkAmA4e",
        "outputId": "8b7bc798-309c-4294-f7c4-e364875d5bd9"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Load the stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_sentence = []\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "\n",
        "    # Remove stop words\n",
        "    for line in news_sum:\n",
        "      word_tokens = word_tokenize(line)\n",
        "      temp_attach = \"\"\n",
        "      for word in word_tokens:\n",
        "        temp = \" \"\n",
        "        if not word in stop_words:\n",
        "          temp = temp + word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      filtered_sentence.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(filtered_sentence)):    \n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "    # Update the data frame\n",
        "    df_sum_news_labels[\"News\"] = filtered_sentence\n",
        "\n",
        "    # Show the DataFrame\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      0  georgia downs two russian warplanes countries ...\n",
            "1      0  russia today columns troops roll south ossetia...\n",
            "2      0  afghan children raped impunity u n official sa...\n",
            "3      0  breaking georgia invades south ossetia russia ...\n",
            "4      0  georgian troops retreat osettain capital presu...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DHwHi3mbDn"
      },
      "source": [
        "Az adathalmazban lévő nulla hosszú sztring csomagok megkeresése és a hozzájuk tartozó cellák törlése következik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUqfwAe1mbqM",
        "outputId": "834f2fc4-cce9-4396-c4bf-2dfc9dfe9e04"
      },
      "source": [
        "if DATASET == 1:\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "\n",
        "    print(null_indexes)\n",
        "\n",
        "    for row in null_indexes:\n",
        "      df_sum_news_labels = df_sum_news_labels.drop(row)\n",
        "\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "      \n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3335]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UW_rMpOmlSH"
      },
      "source": [
        "Az adathalmaz véletlenszerű sorbarendezése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdW_tECmlxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ca1358-d34c-47ff-c83d-584aeb72c477"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Do the shuffle\n",
        "    for i in range(SHUFFLE_CYCLE):\n",
        "      df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\n",
        "\n",
        "    # Reset the index\n",
        "    df_sum_news_labels.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Show the data frame\n",
        "    print(df_sum_news_labels.head())\n",
        "    # Show the test data frame\n",
        "    print(\"\\n\\nTest frame\")\n",
        "    print(df_for_test.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      1  north korea claims us government made intervie...\n",
            "1      0  afghanistan pipe dreams peace son also rises f...\n",
            "2      1  reportedly killed israeli strikes gaza reminde...\n",
            "3      1  austerity struck paris france hit wave street ...\n",
            "4      1  fukushima kids diagnosed thyroid cancer second...\n",
            "\n",
            "\n",
            "Test frame\n",
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2016-06-20      1  ...  Wikileaks founder Julian Assange marks 5 years...\n",
            "2016-06-21      1  ...  Russian football fan leader Alexander Shprygin...\n",
            "2016-06-22      0  ...  N. Korea launches what appears to be Musudan m...\n",
            "2016-06-23      1  ...  The prime minister of India is set to get a br...\n",
            "2016-06-24      0  ...  A Turkish man has been found guilty of insulti...\n",
            "\n",
            "[5 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9cUBq_WnJbH"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és validáló/tesztelő adathalmazokra, majd a szétbontás ellenőrzése mérettel és első elem kiíratásával."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Mg2z7LnKH1",
        "outputId": "2825a6a1-2a4a-49e9-f65f-5b71701734b9"
      },
      "source": [
        "if DATASET == 1:\n",
        "    INPUT_SIZE = len(df_sum_news_labels)\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\n",
        "\n",
        "    # Split the dataset\n",
        "    train = df_sum_news_labels[:TRAIN_SIZE] \n",
        "    test = df_sum_news_labels[TRAIN_SIZE:]\n",
        "\n",
        "    # Print out the length\n",
        "    print(\"Train data set length: \" + str(len(train)))\n",
        "    print(\"Test data set length: \" + str(len(test)))\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\n",
        "    print(\"Dataset summa before split: \" + str(len(df_sum_news_labels)))\n",
        "\n",
        "    # check\n",
        "    split_sum = len(train) + len(test)\n",
        "    sum = len(df_sum_news_labels)\n",
        "    assert split_sum == sum"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data set length: 18997\n",
            "Test data set length: 4750\n",
            "Split summa: 23747\n",
            "Dataset summa before split: 23747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s03PFsFQsgpS",
        "outputId": "105dfcbc-e3f6-4256-8729-f3ee8a9bced8"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(train.tail(1))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Label                                               News\n",
            "18996      1  murdoch may lose grip news corp libyan rebels ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au3VLLVzsh0U",
        "outputId": "75a13bfc-7f41-493d-e769-b54ad5a488fd"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(test.head(1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Label                                               News\n",
            "18997      0  dengue world fastest spreading tropical diseas...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgA6S5muPbE"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsDa53wcxRdG"
      },
      "source": [
        "Először a tanító adathalmaz híreit fűzöm össze egy tömbbe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RhOIEt3wmHn",
        "outputId": "7ec630c1-a294-4f04-d70b-f351ece8ce21"
      },
      "source": [
        "if DATASET == 1:\n",
        "    train_headlines = []\n",
        "\n",
        "    for row in range(0, len(train.index)):\n",
        "        train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "    # show the first\n",
        "    print(train_headlines[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "north korea claims us government made interview uk officials named pedophile dossier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83MuhLlOxYK5"
      },
      "source": [
        "Ezek után vektorizálom őket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N3hIvbQxgr6",
        "outputId": "fd1a1c07-bf40-4e10-f3e4-666814f1c52a"
      },
      "source": [
        "if DATASET == 1:\n",
        "    bow_vectorizer = CountVectorizer()\n",
        "    bow_train = bow_vectorizer.fit_transform(train_headlines)\n",
        "    print(bow_train.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 29603)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExOpaN_Ix3r_"
      },
      "source": [
        "Egy logistic regression modellt fogok erre a tanító halmazra betanítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D36NSmICyOCC"
      },
      "source": [
        "if DATASET == 1:\n",
        "    bow_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    bow_model = bow_model.fit(bow_train, train[\"Label\"])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EYr5_yygsr"
      },
      "source": [
        "A teszt adathalmaz előkészítése, majd becslés a modell segítségével a következő lépés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkxsor9uygSa"
      },
      "source": [
        "if DATASET == 1:\n",
        "    test_headlines = []\n",
        "\n",
        "    for row in range(0,len(test.index)):\n",
        "        test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "    bow_test = bow_vectorizer.transform(test_headlines)\n",
        "    bow_predictions = bow_model.predict(bow_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFBYXRil2nS2"
      },
      "source": [
        "Az eredmények megjelenítése egy táblázatban."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfgw4P9-2rkL"
      },
      "source": [
        "if DATASET == 1:\n",
        "    pd.crosstab(test[\"Label\"], bow_predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9uWaAGm3E_J"
      },
      "source": [
        "A pontossága a modellnek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxtBIe23HqK",
        "outputId": "3119e7c3-7b30-47d0-ea0f-febeafec12e6"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print (classification_report(test[\"Label\"], bow_predictions))\n",
        "    print (accuracy_score(test[\"Label\"], bow_predictions))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], bow_predictions))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.43      0.45      2200\n",
            "           1       0.55      0.59      0.57      2550\n",
            "\n",
            "    accuracy                           0.52      4750\n",
            "   macro avg       0.51      0.51      0.51      4750\n",
            "weighted avg       0.51      0.52      0.52      4750\n",
            "\n",
            "0.5178947368421053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQZQqPar4Im8"
      },
      "source": [
        "A következőkben a top 10 legbefolyásolóbb sztringet jelenítem meg mind pozítiv és mind negatív irányba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oveqbdwt4Q-9",
        "outputId": "97f94eeb-96b6-4bc3-f80f-0fdeffc164c5"
      },
      "source": [
        "if DATASET == 1:\n",
        "    bow_words = bow_vectorizer.get_feature_names()\n",
        "    bow_coeffs = bow_model.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : bow_words, \n",
        "                            'Coefficient' : bow_coeffs})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Word  Coefficient\n",
            "20524  promising     1.632536\n",
            "22417     riyadh     1.595175\n",
            "23209      scrap     1.584696\n",
            "7680     donetsk     1.570100\n",
            "22885      sanaa     1.533908\n",
            "4676     clashed     1.507165\n",
            "6623   defending     1.382395\n",
            "21882     repeal     1.379722\n",
            "14528    landing     1.378869\n",
            "22333       rift     1.377071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKPketqV4uVw",
        "outputId": "3a031543-3f75-4ad6-f85c-5d00feaa3abb"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Word  Coefficient\n",
            "5787   counterparts    -1.407548\n",
            "4520       choppers    -1.409674\n",
            "11973         hints    -1.421742\n",
            "19566        picked    -1.483659\n",
            "25188       stomach    -1.496732\n",
            "652         airways    -1.534589\n",
            "12191         horns    -1.579956\n",
            "7017        detects    -1.612830\n",
            "14628     launchers    -1.620675\n",
            "9924        focused    -1.647796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytoEWvZ_48_x"
      },
      "source": [
        "### 2-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_11F44gu5TdJ"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1reDgLAU5jQd",
        "outputId": "bdcccb08-07ad-4afd-c985-937c0d8ee983"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_vectorizer_12 = CountVectorizer(ngram_range=(1,2))\n",
        "    train_vectorizer_12 = gram_vectorizer_12.fit_transform(train_headlines)\n",
        "\n",
        "    print(train_vectorizer_12.shape)\n",
        "\n",
        "    gram_model_12 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    gram_model_12 = gram_model_12.fit(train_vectorizer_12, train[\"Label\"])\n",
        "\n",
        "    gram_test_12 = gram_vectorizer_12.transform(test_headlines)\n",
        "    gram_predictions_12 = gram_model_12.predict(gram_test_12)\n",
        "\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_12))\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_12))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_12))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 357890)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.42      0.45      2200\n",
            "           1       0.55      0.62      0.58      2550\n",
            "\n",
            "    accuracy                           0.53      4750\n",
            "   macro avg       0.52      0.52      0.52      4750\n",
            "weighted avg       0.52      0.53      0.52      4750\n",
            "\n",
            "0.527578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JxbXWM7wss",
        "outputId": "3f7f9737-065e-481a-ec15-655cbe7eb7cc"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_words_12 = gram_vectorizer_12.get_feature_names()\n",
        "    gram_coeffs_12 = gram_model_12.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_12, \n",
        "                            'Coefficient' : gram_coeffs_12})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Word  Coefficient\n",
            "91588       donetsk     0.971925\n",
            "270789        ruled     0.903815\n",
            "289089  significant     0.863195\n",
            "100623      enemies     0.852514\n",
            "175020      landing     0.837202\n",
            "300773         step     0.836689\n",
            "172205           km     0.835461\n",
            "243354   presidents     0.829768\n",
            "279316        scrap     0.825647\n",
            "205643       mumbai     0.824550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g96l9WK57xMH",
        "outputId": "3de40fd8-9a8a-4789-c2fd-7cc7860c3da0"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      Word  Coefficient\n",
            "24052            authority    -0.818779\n",
            "334644             us army    -0.833292\n",
            "207131          mysterious    -0.842346\n",
            "302563            stranded    -0.843124\n",
            "109051           extremism    -0.846904\n",
            "301359               stock    -0.847559\n",
            "2911                  acta    -0.848259\n",
            "114958              filmed    -0.920522\n",
            "212300  news international    -0.957999\n",
            "186710                 low    -1.094469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFrdv3Um9CoD"
      },
      "source": [
        "Másodjára a (2,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IdZdzJl9Ftm",
        "outputId": "81b715c7-f264-4ac3-a76d-beee384b149a"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_vectorizer_22 = CountVectorizer(ngram_range=(2,2))\n",
        "    train_vectorizer_22 = gram_vectorizer_22.fit_transform(train_headlines)\n",
        "\n",
        "    print(train_vectorizer_22.shape)\n",
        "\n",
        "    gram_model_22 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    gram_model_22 = gram_model_22.fit(train_vectorizer_22, train[\"Label\"])\n",
        "\n",
        "    gram_test_22 = gram_vectorizer_22.transform(test_headlines)\n",
        "    gram_predictions_22 = gram_model_22.predict(gram_test_22)\n",
        "\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_22, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_22))\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_22))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_22))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 328287)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.34      0.41      2200\n",
            "           1       0.56      0.72      0.63      2550\n",
            "\n",
            "    accuracy                           0.54      4750\n",
            "   macro avg       0.53      0.53      0.52      4750\n",
            "weighted avg       0.53      0.54      0.52      4750\n",
            "\n",
            "0.5416842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snpLViV79GlO",
        "outputId": "7f6424a1-9352-4cfd-c7bf-93af2fb7ecbf"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_words_22 = gram_vectorizer_22.get_feature_names()\n",
        "    gram_coeffs_22 = gram_model_22.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_22, \n",
        "                            'Coefficient' : gram_coeffs_22})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Word  Coefficient\n",
            "92649      england wales     0.850880\n",
            "104367      fidel castro     0.805384\n",
            "249571      russia sends     0.781674\n",
            "161669     latin america     0.770149\n",
            "257536  security council     0.765139\n",
            "4883      afghan soldier     0.756483\n",
            "159320        korean war     0.752447\n",
            "30235        big brother     0.720896\n",
            "50950     china military     0.720644\n",
            "196252     north america     0.720637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiSp21nl9Gai",
        "outputId": "8d938f9b-25f0-4cfc-e291-c46b9eb50e67"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      Word  Coefficient\n",
            "17189         around world    -0.741042\n",
            "68254      crimes humanity    -0.779260\n",
            "277316       strait hormuz    -0.791220\n",
            "276199        stock market    -0.798599\n",
            "197951         nytimes com    -0.806997\n",
            "4905          afghan woman    -0.813143\n",
            "181720      military bases    -0.820401\n",
            "306682             us army    -0.847734\n",
            "252343          saudi king    -0.914661\n",
            "194725  news international    -1.085222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUxZOqjo5Eg7"
      },
      "source": [
        "### 3-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX7vV_715ckj"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNHw3tAMSjp",
        "outputId": "1045aa26-5df6-464a-b5a1-50130cb43f60"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_vectorizer_13 = CountVectorizer(ngram_range=(1,3))\n",
        "    train_vectorizer_13 = gram_vectorizer_13.fit_transform(train_headlines)\n",
        "\n",
        "    print(train_vectorizer_13.shape)\n",
        "\n",
        "    gram_model_13 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    gram_model_13 = gram_model_13.fit(train_vectorizer_13, train[\"Label\"])\n",
        "\n",
        "    gram_test_13 = gram_vectorizer_13.transform(test_headlines)\n",
        "    gram_predictions_13 = gram_model_13.predict(gram_test_13)\n",
        "\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_13))\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_13))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_13))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 751362)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.40      0.44      2200\n",
            "           1       0.55      0.64      0.59      2550\n",
            "\n",
            "    accuracy                           0.53      4750\n",
            "   macro avg       0.52      0.52      0.52      4750\n",
            "weighted avg       0.52      0.53      0.52      4750\n",
            "\n",
            "0.5296842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-xubCYMiFF",
        "outputId": "029ab604-dc7a-4cec-dee2-b9c94634d025"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_words_13 = gram_vectorizer_13.get_feature_names()\n",
        "    gram_coeffs_13 = gram_model_13.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_13, \n",
        "                            'Coefficient' : gram_coeffs_13})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Word  Coefficient\n",
            "428927       mumbai     0.777815\n",
            "629908         step     0.765280\n",
            "565898        ruled     0.754003\n",
            "189045      donetsk     0.727203\n",
            "700085        urges     0.707539\n",
            "359110           km     0.701980\n",
            "590449        seize     0.676912\n",
            "605508  significant     0.657024\n",
            "715092        votes     0.643919\n",
            "686176         turn     0.642439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DequuzvFMmH3",
        "outputId": "4eed6173-0179-47ec-b7e8-a9a7e87bc285"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      Word  Coefficient\n",
            "649869              system    -0.678350\n",
            "50433            authority    -0.679418\n",
            "547000             removed    -0.681450\n",
            "449100             nothing    -0.694423\n",
            "432132          mysterious    -0.704376\n",
            "6020                  acta    -0.709837\n",
            "631091               stock    -0.748948\n",
            "237308              filmed    -0.755930\n",
            "443585  news international    -0.766026\n",
            "389517                 low    -0.991269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHmKu3djMS14"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (2,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etgz8IAjMUE3",
        "outputId": "8725d1aa-0a82-41a9-dbd5-a2b62f577eca"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_vectorizer_23 = CountVectorizer(ngram_range=(2,3))\n",
        "    train_vectorizer_23 = gram_vectorizer_23.fit_transform(train_headlines)\n",
        "\n",
        "    print(train_vectorizer_23.shape)\n",
        "\n",
        "    gram_model_23 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    gram_model_23 = gram_model_23.fit(train_vectorizer_23, train[\"Label\"])\n",
        "\n",
        "    gram_test_23 = gram_vectorizer_23.transform(test_headlines)\n",
        "    gram_predictions_23 = gram_model_23.predict(gram_test_23)\n",
        "\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_23, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_23))\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_23))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_23))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 721759)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.27      0.36      2200\n",
            "           1       0.56      0.78      0.65      2550\n",
            "\n",
            "    accuracy                           0.55      4750\n",
            "   macro avg       0.54      0.53      0.50      4750\n",
            "weighted avg       0.54      0.55      0.51      4750\n",
            "\n",
            "0.5471578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVN0l54FMi1-",
        "outputId": "dd104b03-d02a-408d-f778-322b4c9d9099"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_words_23 = gram_vectorizer_23.get_feature_names()\n",
        "    gram_coeffs_23 = gram_model_23.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_23, \n",
        "                            'Coefficient' : gram_coeffs_23})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Word  Coefficient\n",
            "569628   sentenced death     0.626520\n",
            "200547     england wales     0.624720\n",
            "225542      fidel castro     0.616101\n",
            "353263     latin america     0.600078\n",
            "564961  security council     0.594668\n",
            "710389     world largest     0.562634\n",
            "546543      russia sends     0.526876\n",
            "65616        big brother     0.519188\n",
            "348008        korean war     0.517875\n",
            "432950    nuclear strike     0.517595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIO9FkYuMnNb",
        "outputId": "d91704fd-b31d-4677-fee9-2eb26eddf719"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      Word  Coefficient\n",
            "608295       strait hormuz    -0.564197\n",
            "455355       panama papers    -0.571619\n",
            "148129     crimes humanity    -0.574374\n",
            "605949        stock market    -0.590060\n",
            "396819      military bases    -0.591332\n",
            "672705             us army    -0.608258\n",
            "434116         nytimes com    -0.658205\n",
            "37537         around world    -0.668372\n",
            "553235          saudi king    -0.678523\n",
            "426010  news international    -0.899099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftajhXMMUUs"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (3,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T-9458xMaJq",
        "outputId": "e8b6c220-c407-4404-f506-f19325ce0124"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_vectorizer_33 = CountVectorizer(ngram_range=(3,3))\n",
        "    train_vectorizer_33 = gram_vectorizer_33.fit_transform(train_headlines)\n",
        "\n",
        "    print(train_vectorizer_33.shape)\n",
        "\n",
        "    gram_model_33 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    gram_model_33 = gram_model_33.fit(train_vectorizer_33, train[\"Label\"])\n",
        "\n",
        "    gram_test_33 = gram_vectorizer_33.transform(test_headlines)\n",
        "    gram_predictions_33 = gram_model_33.predict(gram_test_33)\n",
        "\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_33, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_33))\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_33))\n",
        "\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_33))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18997, 393472)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.07      0.13      2200\n",
            "           1       0.54      0.94      0.69      2550\n",
            "\n",
            "    accuracy                           0.54      4750\n",
            "   macro avg       0.53      0.51      0.41      4750\n",
            "weighted avg       0.53      0.54      0.43      4750\n",
            "\n",
            "0.5389473684210526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7_f6ay_MjhC",
        "outputId": "cdac0866-e08a-426d-fc8f-6b734e69830b"
      },
      "source": [
        "if DATASET == 1:\n",
        "    gram_words_33 = gram_vectorizer_33.get_feature_names()\n",
        "    gram_coeffs_33 = gram_model_33.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_33, \n",
        "                            'Coefficient' : gram_coeffs_33})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                   Word  Coefficient\n",
            "125287                 first time since     0.695203\n",
            "390264                     year old man     0.656931\n",
            "382194         wikileaks julian assange     0.573982\n",
            "275468                putin says russia     0.526135\n",
            "18026             approves sex marriage     0.496900\n",
            "157408                  homes west bank     0.486680\n",
            "390291             year old palestinian     0.477924\n",
            "99637   drug decriminalization portugal     0.471425\n",
            "232875                nobel peace prize     0.469363\n",
            "266193            president evo morales     0.462286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNFCAZ2pMpCY",
        "outputId": "c3b1ea23-01e0-4d9b-9b0d-710e89e0e908"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            Word  Coefficient\n",
            "218236    missile defense system    -0.525905\n",
            "186232          kills boko haram    -0.527956\n",
            "339299    syrian security forces    -0.538321\n",
            "157319      homes east jerusalem    -0.541525\n",
            "230284        new prime minister    -0.542140\n",
            "25146               aung san suu    -0.560239\n",
            "299880               san suu kyi    -0.560239\n",
            "323036     sovereign wealth fund    -0.611614\n",
            "255297     phone hacking scandal    -0.699162\n",
            "55588   chancellor angela merkel    -0.793560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpZIe-RTmk_"
      },
      "source": [
        "### 4-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y75LfFJBTtrz"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PabhGwFWUZrZ",
        "outputId": "d48f6e8a-f057-4076-e905-3d1b11932b65"
      },
      "source": [
        "if DATASET == 1:\n",
        "    for n in range(1,5):\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "              + str(n) + \",4 gram model\\n\")\n",
        "\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,4))\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "        model_type.append(str(n) + \",4 n-gram\")\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (18997, 1136449)\n",
            "\n",
            "0.5296842105263158\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (18997, 1106846)\n",
            "\n",
            "0.5469473684210526\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (18997, 778559)\n",
            "\n",
            "0.539578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (18997, 385087)\n",
            "\n",
            "0.5385263157894736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkP2pSAV6Qm"
      },
      "source": [
        "### 5-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Y4HliaV-rg"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8hdr8A-WBRq",
        "outputId": "4da82cd5-3460-4115-8b83-58cc4c32def2"
      },
      "source": [
        "if DATASET == 1:\n",
        "    MODEL_TYPE = 5\n",
        "\n",
        "    for n in range(1,MODEL_TYPE+1):\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (18997, 1504222)\n",
            "\n",
            "0.5353684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (18997, 1474619)\n",
            "\n",
            "0.5454736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (18997, 1146332)\n",
            "\n",
            "0.5387368421052632\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (18997, 752860)\n",
            "\n",
            "0.5408421052631579\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (18997, 367773)\n",
            "\n",
            "0.5383157894736842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgRyRExGWvFN"
      },
      "source": [
        "### 6-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv2plcaDWy8V"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-9kyXKFW1KI",
        "outputId": "84022e87-01fa-4d71-a482-6c7863f5d07d"
      },
      "source": [
        "if DATASET == 1:\n",
        "    MODEL_TYPE = 6\n",
        "\n",
        "    for n in range(1,MODEL_TYPE+1):\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (18997, 1853406)\n",
            "\n",
            "0.5347368421052632\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (18997, 1823803)\n",
            "\n",
            "0.5446315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (18997, 1495516)\n",
            "\n",
            "0.5383157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (18997, 1102044)\n",
            "\n",
            "0.5393684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (18997, 716957)\n",
            "\n",
            "0.5381052631578948\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (18997, 349184)\n",
            "\n",
            "0.5376842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh7BcurhRaGe"
      },
      "source": [
        "### Eredmények összegzése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr2NAOefRc1j"
      },
      "source": [
        "Az eredmények kiíratása, a legjobbat kiemelve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykPdalGBRgeL",
        "outputId": "1076b56f-1300-472c-9085-3d285c0a7414"
      },
      "source": [
        "if DATASET == 1:\n",
        "    best_model = 0\n",
        "\n",
        "    for model in range(len(model_type)):\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\n",
        "\n",
        "        if result[model] > best_model:\n",
        "            best_model = result[model]\n",
        "            best_model_index = model\n",
        "\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \n",
        "          + str(model_type[best_model_index]) + \"\\t\\t\\t\\t\\t\" + \n",
        "          str(result[best_model_index]))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of words:\t\t\t\t\t0.5178947368421053\n",
            "1,2 n-gram:\t\t\t\t\t0.527578947368421\n",
            "2,2 n-gram:\t\t\t\t\t0.5416842105263158\n",
            "1,3 n-gram:\t\t\t\t\t0.5296842105263158\n",
            "2,3 n-gram:\t\t\t\t\t0.5471578947368421\n",
            "3,3 n-gram:\t\t\t\t\t0.5389473684210526\n",
            "1,4 n-gram:\t\t\t\t\t0.5296842105263158\n",
            "2,4 n-gram:\t\t\t\t\t0.5469473684210526\n",
            "3,4 n-gram:\t\t\t\t\t0.539578947368421\n",
            "4,4 n-gram:\t\t\t\t\t0.5385263157894736\n",
            "1,5 n-gram:\t\t\t\t\t0.5353684210526316\n",
            "2,5 n-gram:\t\t\t\t\t0.5454736842105263\n",
            "3,5 n-gram:\t\t\t\t\t0.5387368421052632\n",
            "4,5 n-gram:\t\t\t\t\t0.5408421052631579\n",
            "5,5 n-gram:\t\t\t\t\t0.5383157894736842\n",
            "1,6 n-gram:\t\t\t\t\t0.5347368421052632\n",
            "2,6 n-gram:\t\t\t\t\t0.5446315789473685\n",
            "3,6 n-gram:\t\t\t\t\t0.5383157894736842\n",
            "4,6 n-gram:\t\t\t\t\t0.5393684210526316\n",
            "5,6 n-gram:\t\t\t\t\t0.5381052631578948\n",
            "6,6 n-gram:\t\t\t\t\t0.5376842105263158\n",
            "--------------------------------------------\n",
            "Best model:\n",
            "2,3 n-gram\t\t\t\t\t0.5471578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnuJNnfVYu04"
      },
      "source": [
        "### ROWS makró optimalizálás"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3gtt_TY80k"
      },
      "source": [
        "Ebben a fejezetben a különböző ROWS értékekre (mennyi napi hírt fűzünk egybe) futtatom végig egy automatizált bag of words -> 6,6 gram modell tanítást és becslést és állapítom meg, hogy melyik a legpontosabb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUJCC_EiZo63"
      },
      "source": [
        "A tesztelendő paraméterek megadása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KUt-wbZZO1i",
        "outputId": "d2d99898-7611-47d6-8b22-fdaaad878cce"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # Number of merged news into one string: 1...12, 25 \n",
        "    rows_values = []\n",
        "    for value in range(1,13):\n",
        "        rows_values.append(value)\n",
        "\n",
        "    rows_values.append(25)\n",
        "\n",
        "    print(rows_values)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIHq8mVcNvZ"
      },
      "source": [
        "A modell típusok összegyűjtése az automatizált tanításhoz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U4xwuVXcRsp",
        "outputId": "9e179453-e05c-4856-8c9b-3f46f8c5de3e"
      },
      "source": [
        "if DATASET == 1:\n",
        "    model_type_values = []\n",
        "    for value in range(1,7):\n",
        "        model_type_values.append(value)\n",
        "\n",
        "    print(model_type_values)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9UswHzer9O"
      },
      "source": [
        "A paraméterhez tartozó eredmények tárolására létrehozom az alábbi tömböket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRcPzJ-1exEu"
      },
      "source": [
        "if DATASET == 1:\n",
        "    rows_summary_value = []\n",
        "    rows_summary_accuraccy = []"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_5e2otZsMM"
      },
      "source": [
        "Automatizált tanítás és mentések."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJXKkcqGPBZ"
      },
      "source": [
        "if DATASET == 1:\n",
        "    def preprocess():\n",
        "        df_combined = pd.read_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv', \n",
        "                                index_col = \"Date\")\n",
        "\n",
        "        # Find the cells with NaN and after the rows for them\n",
        "        is_NaN = df_combined.isnull()\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "        # Replace them\n",
        "        df_combined = df_combined.replace(np.nan, \" \")\n",
        "\n",
        "        # Check the process\n",
        "        is_NaN = df_combined.isnull()\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "        assert len(rows_with_NaN) is 0\n",
        "\n",
        "        # the last 10 day save out for testing and comparing the models\n",
        "        df_for_test = df_combined.tail(10)\n",
        "        df_combined.drop(df_combined.tail(10).index,inplace=True) # drop last n rows\n",
        "\n",
        "        # Get column names\n",
        "        combined_column_names = []\n",
        "        for column in df_combined.columns:\n",
        "          combined_column_names.append(column)\n",
        "\n",
        "        # 2D array creation for the news based on macros\n",
        "        COLUMNS = len(df_combined)\n",
        "        news_sum = []\n",
        "        news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \n",
        "\n",
        "        # Merge the news\n",
        "        for row in range(len(df_combined)):\n",
        "          for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "            temp = \"\"\n",
        "            news = \"\"\n",
        "            for word in range(ROWS):\n",
        "              news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\n",
        "              # Remove the b character at the begining of the string\n",
        "              if news[0] is \"b\":\n",
        "                news = \" \" + news[1:]\n",
        "              temp = temp + \" \" + news\n",
        "            news_sum[column][row] = temp\n",
        "\n",
        "        # Drop the old columns\n",
        "        for column in range(len(combined_column_names) - 1):\n",
        "          df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\n",
        "\n",
        "        # Create the new columns with the merged news\n",
        "        for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "          colum_name = \"News_\" + str(column + 1)\n",
        "          df_combined[colum_name] = news_sum[column]          \n",
        "\n",
        "        # The label column \n",
        "        LABEL_COLUMN = 0\n",
        "\n",
        "        news_sum = []\n",
        "        label_sum = []\n",
        "\n",
        "        # Get the column names\n",
        "        combined_column_names = []\n",
        "        for column in df_combined.columns:\n",
        "          combined_column_names.append(column)\n",
        "\n",
        "        # Connect the merged news with the labels\n",
        "        for column in range(len(df_combined)):\n",
        "          for row in range(len(combined_column_names) - 1):\n",
        "            news_sum.append(df_combined[combined_column_names[row + 1]][column])\n",
        "            label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\n",
        "\n",
        "        # Create the new DataFrame\n",
        "        df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\n",
        "        df_sum_news_labels[\"News\"] = news_sum\n",
        "\n",
        "        # Removing punctuations\n",
        "        temp_news = []\n",
        "        for line in news_sum:\n",
        "          temp_attach = \"\"\n",
        "          for word in line:\n",
        "            temp = \" \"\n",
        "            if word not in string.punctuation:\n",
        "              temp = word\n",
        "            temp_attach = temp_attach + \"\".join(temp)\n",
        "          temp_news.append(temp_attach)\n",
        "\n",
        "        news_sum = temp_news\n",
        "        temp_news = []\n",
        "\n",
        "        # Remove numbers\n",
        "        for line in news_sum:\n",
        "          temp_attach = \"\"\n",
        "          for word in line:\n",
        "            temp = \" \"\n",
        "            if not word.isdigit():\n",
        "              temp = word\n",
        "            temp_attach = temp_attach + \"\".join(temp)\n",
        "          temp_news.append(temp_attach)\n",
        "\n",
        "        # Remove space\n",
        "        for line in range(len(temp_news)):    \n",
        "          temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "        # Converting headlines to lower case\n",
        "        for line in range(len(temp_news)): \n",
        "            temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "        # Update the data frame\n",
        "        df_sum_news_labels[\"News\"] = temp_news\n",
        "\n",
        "        # Load the stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        filtered_sentence = []\n",
        "        news_sum = df_sum_news_labels[\"News\"]\n",
        "\n",
        "        # Remove stop words\n",
        "        for line in news_sum:\n",
        "          word_tokens = word_tokenize(line)\n",
        "          temp_attach = \"\"\n",
        "          for word in word_tokens:\n",
        "            temp = \" \"\n",
        "            if not word in stop_words:\n",
        "              temp = temp + word\n",
        "            temp_attach = temp_attach + \"\".join(temp)\n",
        "          filtered_sentence.append(temp_attach)\n",
        "\n",
        "        # Remove space\n",
        "        for line in range(len(filtered_sentence)):    \n",
        "          filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "        # Update the data frame\n",
        "        df_sum_news_labels[\"News\"] = filtered_sentence\n",
        "\n",
        "        news_sum = df_sum_news_labels[\"News\"]\n",
        "        null_indexes = []\n",
        "        index = 0\n",
        "\n",
        "        for line in news_sum:\n",
        "          if line is \"\":\n",
        "            null_indexes.append(index)\n",
        "          index = index + 1\n",
        "\n",
        "        for row in null_indexes:\n",
        "          df_sum_news_labels = df_sum_news_labels.drop(row)\n",
        "\n",
        "        news_sum = df_sum_news_labels[\"News\"]\n",
        "        null_indexes = []\n",
        "        index = 0\n",
        "\n",
        "        for line in news_sum:\n",
        "          if line is \"\":\n",
        "            null_indexes.append(index)\n",
        "          index = index + 1\n",
        "          \n",
        "        assert len(null_indexes) is 0\n",
        "\n",
        "        # Do the shuffle\n",
        "        for i in range(SHUFFLE_CYCLE):\n",
        "          df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\n",
        "\n",
        "        # Reset the index\n",
        "        df_sum_news_labels.reset_index(inplace=True, drop=True)\n",
        "\n",
        "        return df_sum_news_labels"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zgMLtiLx6e"
      },
      "source": [
        "if DATASET == 1:\n",
        "    def split_to_train():\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "\n",
        "        # Split the dataset\n",
        "        train = df_sum_news_labels[:TRAIN_SIZE] \n",
        "\n",
        "        return train"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1lgt7dMBol"
      },
      "source": [
        "if DATASET == 1:\n",
        "    def split_to_test():\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "\n",
        "        # Split the dataset\n",
        "        test = df_sum_news_labels[TRAIN_SIZE:]\n",
        "\n",
        "        return test"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq0Oo4iHZufG",
        "outputId": "f27e0177-1573-43fe-f98e-7f960d87c7fb"
      },
      "source": [
        "if DATASET == 1:\n",
        "    for ROWS in rows_values:\n",
        "      \n",
        "        print(\"--------------------------------------------\\n\\nStart of the ROWS = \" \n",
        "          + str(ROWS) + \" sequence\\n\\n--------------------------------------------\\n\")\n",
        "        \n",
        "        model_type = []\n",
        "        result = []\n",
        "\n",
        "        df_sum_news_labels = preprocess()\n",
        "        train = split_to_train()\n",
        "        test = split_to_test()\n",
        "\n",
        "        # check\n",
        "        split_sum = len(train) + len(test)\n",
        "        sum = len(df_sum_news_labels)\n",
        "        assert split_sum == sum    \n",
        "\n",
        "        train_headlines = []\n",
        "        test_headlines = []\n",
        "\n",
        "        for row in range(0, len(train.index)):\n",
        "            train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "        for row in range(0,len(test.index)):\n",
        "            test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "        # show the first\n",
        "        print(train_headlines[0])\n",
        "\n",
        "        for MODEL_TYPE in model_type_values:\n",
        "\n",
        "            for n in range(1,MODEL_TYPE+1):\n",
        "                print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "                      + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "                _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "                _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "                print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "                _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "                _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "                _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "                _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "                print (accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "                model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "                result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "        rows_summary_value.append(ROWS)\n",
        "\n",
        "        # save the best\n",
        "        best_model_rows = 0\n",
        "\n",
        "        for model in range(len(model_type)):\n",
        "            if result[model] > best_model_rows:\n",
        "                best_model_rows = result[model]\n",
        "\n",
        "        rows_summary_accuraccy.append(best_model_rows)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 1 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "u aircraft carrier heads korean waters south korea warns north korea enormous retaliation\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (39573, 30123)\n",
            "\n",
            "0.5065696381645441\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (39573, 351580)\n",
            "\n",
            "0.5175864160097029\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (39573, 321457)\n",
            "\n",
            "0.5268849807964423\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (39573, 719811)\n",
            "\n",
            "0.5159692743076613\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (39573, 689688)\n",
            "\n",
            "0.5320396199717\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (39573, 368231)\n",
            "\n",
            "0.5363856882959369\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (39573, 1059547)\n",
            "\n",
            "0.5180917727915909\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (39573, 1029424)\n",
            "\n",
            "0.5337578330301193\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (39573, 707967)\n",
            "\n",
            "0.5392156862745098\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (39573, 339736)\n",
            "\n",
            "0.5431574691732363\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (39573, 1361981)\n",
            "\n",
            "0.5173842732969477\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (39573, 1331858)\n",
            "\n",
            "0.5361835455831817\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (39573, 1010401)\n",
            "\n",
            "0.5415403274711946\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (39573, 642170)\n",
            "\n",
            "0.5420456842530826\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (39573, 302434)\n",
            "\n",
            "0.5420456842530826\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (39573, 1627179)\n",
            "\n",
            "0.5176874873660805\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (39573, 1597056)\n",
            "\n",
            "0.5376996159288457\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (39573, 1275599)\n",
            "\n",
            "0.5411360420456842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (39573, 907368)\n",
            "\n",
            "0.541944612896705\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (39573, 567632)\n",
            "\n",
            "0.5420456842530826\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (39573, 265198)\n",
            "\n",
            "0.541944612896705\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 2 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "north korea claims us government made interview uk officials named pedophile dossier\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (18997, 29603)\n",
            "\n",
            "0.5178947368421053\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (18997, 357890)\n",
            "\n",
            "0.527578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (18997, 328287)\n",
            "\n",
            "0.5416842105263158\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (18997, 751362)\n",
            "\n",
            "0.5296842105263158\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (18997, 721759)\n",
            "\n",
            "0.5471578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (18997, 393472)\n",
            "\n",
            "0.5389473684210526\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (18997, 1136449)\n",
            "\n",
            "0.5296842105263158\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (18997, 1106846)\n",
            "\n",
            "0.5469473684210526\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (18997, 778559)\n",
            "\n",
            "0.539578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (18997, 385087)\n",
            "\n",
            "0.5385263157894736\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (18997, 1504222)\n",
            "\n",
            "0.5353684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (18997, 1474619)\n",
            "\n",
            "0.5454736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (18997, 1146332)\n",
            "\n",
            "0.5387368421052632\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (18997, 752860)\n",
            "\n",
            "0.5408421052631579\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (18997, 367773)\n",
            "\n",
            "0.5383157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (18997, 1853406)\n",
            "\n",
            "0.5347368421052632\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (18997, 1823803)\n",
            "\n",
            "0.5446315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (18997, 1495516)\n",
            "\n",
            "0.5383157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (18997, 1102044)\n",
            "\n",
            "0.5393684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (18997, 716957)\n",
            "\n",
            "0.5381052631578948\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (18997, 349184)\n",
            "\n",
            "0.5376842105263158\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 3 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "al qaeda offers ceasefire exchange withdrawal afghanistan liberia deputy ambassador travel phoenix hopes meeting year old raped minors abandoned parents liberians world much outraged want stigmatized half fresh fruit veg sold across uk contaminated toxic pesticides\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (12665, 29668)\n",
            "\n",
            "0.5071045153141774\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (12665, 363832)\n",
            "\n",
            "0.5203662772339753\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (12665, 334164)\n",
            "\n",
            "0.531101989264288\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (12665, 769848)\n",
            "\n",
            "0.5266814019576886\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (12665, 740180)\n",
            "\n",
            "0.529207451847174\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (12665, 406016)\n",
            "\n",
            "0.5326807704452163\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (12665, 1173939)\n",
            "\n",
            "0.5304704767919166\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (12665, 1144271)\n",
            "\n",
            "0.5352068203347016\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (12665, 810107)\n",
            "\n",
            "0.5314177455004736\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (12665, 404091)\n",
            "\n",
            "0.534891064098516\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (12665, 1567061)\n",
            "\n",
            "0.5342595516261446\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (12665, 1537393)\n",
            "\n",
            "0.533943795389959\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (12665, 1203229)\n",
            "\n",
            "0.5333122829175876\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (12665, 797213)\n",
            "\n",
            "0.5336280391537733\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (12665, 393122)\n",
            "\n",
            "0.5317335017366593\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (12665, 1947919)\n",
            "\n",
            "0.5336280391537733\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (12665, 1918251)\n",
            "\n",
            "0.5371013577518156\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (12665, 1584087)\n",
            "\n",
            "0.5345753078623303\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (12665, 1178071)\n",
            "\n",
            "0.5326807704452163\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (12665, 773980)\n",
            "\n",
            "0.5317335017366593\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (12665, 380858)\n",
            "\n",
            "0.532049257972845\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 4 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "dick cheney death squad killed lebanese former prime minister rafik hariri tens thousands armenians march capital commemorate th anniversary armenian genocide pedophile priest hires private detectives harass victims islamic insurgents advance closer pakistani capital\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (9499, 29729)\n",
            "\n",
            "0.5111578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (9499, 366936)\n",
            "\n",
            "0.52\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (9499, 337207)\n",
            "\n",
            "0.5473684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (9499, 779700)\n",
            "\n",
            "0.527578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (9499, 749971)\n",
            "\n",
            "0.5528421052631579\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (9499, 412764)\n",
            "\n",
            "0.5389473684210526\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (9499, 1193722)\n",
            "\n",
            "0.5364210526315789\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (9499, 1163993)\n",
            "\n",
            "0.5414736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (9499, 826786)\n",
            "\n",
            "0.535157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (9499, 414022)\n",
            "\n",
            "0.5334736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (9499, 1599920)\n",
            "\n",
            "0.535578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (9499, 1570191)\n",
            "\n",
            "0.5402105263157895\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (9499, 1232984)\n",
            "\n",
            "0.535578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (9499, 820220)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (9499, 406198)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (9499, 1997014)\n",
            "\n",
            "0.5393684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (9499, 1967285)\n",
            "\n",
            "0.5402105263157895\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (9499, 1630078)\n",
            "\n",
            "0.5343157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (9499, 1217314)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (9499, 803292)\n",
            "\n",
            "0.5334736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (9499, 397094)\n",
            "\n",
            "0.5338947368421053\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 5 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "megaupload deflect copyright liability become raid proof kim dotcom business partner mathias ortmann say plan make mega raid proof also give iron clad defense copyright infringement claims microsoft employees donated billion non profits last years give campaign update egyptian teacher fired cutting hair girls wear headscarves google stock dropped current earnings leaked new rules allow cubans freely leave country return\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (7916, 30034)\n",
            "\n",
            "0.5149065184436584\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (7916, 380029)\n",
            "\n",
            "0.5356240525517938\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (7916, 349995)\n",
            "\n",
            "0.5391611925214755\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (7916, 811617)\n",
            "\n",
            "0.5330975240020213\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (7916, 781583)\n",
            "\n",
            "0.5346134411318848\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (7916, 431588)\n",
            "\n",
            "0.5310763011622032\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (7916, 1246640)\n",
            "\n",
            "0.5300656897422941\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (7916, 1216606)\n",
            "\n",
            "0.5300656897422941\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (7916, 866611)\n",
            "\n",
            "0.5330975240020213\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (7916, 435023)\n",
            "\n",
            "0.5300656897422941\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (7916, 1675549)\n",
            "\n",
            "0.5260232440626579\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (7916, 1645515)\n",
            "\n",
            "0.5346134411318848\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (7916, 1295520)\n",
            "\n",
            "0.5295603840323395\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (7916, 863932)\n",
            "\n",
            "0.5285497726124305\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (7916, 428909)\n",
            "\n",
            "0.5285497726124305\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (7916, 2096967)\n",
            "\n",
            "0.528044466902476\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (7916, 2066933)\n",
            "\n",
            "0.5336028297119757\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (7916, 1716938)\n",
            "\n",
            "0.5305709954522486\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (7916, 1285350)\n",
            "\n",
            "0.528044466902476\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (7916, 850327)\n",
            "\n",
            "0.5275391611925214\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (7916, 421418)\n",
            "\n",
            "0.527033855482567\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 6 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "rupert murdoch news international found parliamentary committee deliberately tried block scotland yard criminal investigation phone hacking suppressed report found busted pirate site users good consumers pirate site users treat services preview buy dvds visit cinema often average spend honest counterparts box office bbc news hired apprentice gets ahead germany germany may ultra modern economy one pillars centre traditional idea people learn skill job nz quake may revealed israeli spy ring news corp tabloid employees come forth stories rebekah brooks reign harry potter correspondent wear potter costume work officially change name showed normal clothes sept chewed ordered put full potter regalia egypt fighting month factory job factory workers block road protest unpaid wages result run blood money one pounds engine revved second time third time truck lurched forward\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (6332, 29495)\n",
            "\n",
            "0.4981060606060606\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (6332, 369215)\n",
            "\n",
            "0.5082070707070707\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (6332, 339720)\n",
            "\n",
            "0.5309343434343434\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (6332, 787736)\n",
            "\n",
            "0.5069444444444444\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (6332, 758241)\n",
            "\n",
            "0.5473484848484849\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (6332, 418521)\n",
            "\n",
            "0.55239898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (6332, 1210669)\n",
            "\n",
            "0.5164141414141414\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (6332, 1181174)\n",
            "\n",
            "0.5391414141414141\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (6332, 841454)\n",
            "\n",
            "0.5505050505050505\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (6332, 422933)\n",
            "\n",
            "0.5492424242424242\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (6332, 1628955)\n",
            "\n",
            "0.5176767676767676\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (6332, 1599460)\n",
            "\n",
            "0.5372474747474747\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (6332, 1259740)\n",
            "\n",
            "0.5492424242424242\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (6332, 841219)\n",
            "\n",
            "0.5486111111111112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (6332, 418286)\n",
            "\n",
            "0.5486111111111112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (6332, 2041314)\n",
            "\n",
            "0.5220959595959596\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (6332, 2011819)\n",
            "\n",
            "0.5410353535353535\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (6332, 1672099)\n",
            "\n",
            "0.5492424242424242\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (6332, 1253578)\n",
            "\n",
            "0.5486111111111112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (6332, 830645)\n",
            "\n",
            "0.5486111111111112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (6332, 412359)\n",
            "\n",
            "0.5486111111111112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 7 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "egyptians confront police violence forced virginity tests amp torture save revolution egyptian government resigns china sentences death tycoon real estate funding scam restore consumer confidence extent injuries children private uk jails revealed tibetan monks nuns burning selves chinese occupation tibet south african state secrecy bill passed archbishop tutu condemns south africa secrecy bill nobel peace laureate archbishop desmond tutu strongly condemned new media law south african mps set vote shortly\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (4749, 27980)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (4749, 331071)\n",
            "\n",
            "0.5328282828282829\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (4749, 303091)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (4749, 701281)\n",
            "\n",
            "0.5412457912457912\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (4749, 673301)\n",
            "\n",
            "0.5303030303030303\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (4749, 370210)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (4749, 1075641)\n",
            "\n",
            "0.5429292929292929\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (4749, 1047661)\n",
            "\n",
            "0.5294612794612794\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (4749, 744570)\n",
            "\n",
            "0.5370370370370371\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (4749, 374360)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (4749, 1446616)\n",
            "\n",
            "0.5496632996632996\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (4749, 1418636)\n",
            "\n",
            "0.5395622895622896\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (4749, 1115545)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (4749, 745335)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (4749, 370975)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (4749, 1813152)\n",
            "\n",
            "0.5488215488215489\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (4749, 1785172)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (4749, 1482081)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (4749, 1111871)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (4749, 737511)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (4749, 366536)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 8 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "egyptian government resigns china sentences death tycoon real estate funding scam restore consumer confidence extent injuries children private uk jails revealed tibetan monks nuns burning selves chinese occupation tibet south african state secrecy bill passed archbishop tutu condemns south africa secrecy bill nobel peace laureate archbishop desmond tutu strongly condemned new media law south african mps set vote shortly egyptian calling second revolution death toll rises egypt military accepts cabinet resignation sets elections next year ruling military council egypt accepted resignation country entire cabinet face demonstrations increased fervor last days amp offered end rule july\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (4749, 29612)\n",
            "\n",
            "0.5084175084175084\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (4749, 369308)\n",
            "\n",
            "0.5193602693602694\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (4749, 339696)\n",
            "\n",
            "0.5328282828282829\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (4749, 789627)\n",
            "\n",
            "0.5134680134680135\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (4749, 760015)\n",
            "\n",
            "0.5437710437710438\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (4749, 420319)\n",
            "\n",
            "0.5387205387205387\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (4749, 1215926)\n",
            "\n",
            "0.5210437710437711\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (4749, 1186314)\n",
            "\n",
            "0.5446127946127947\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (4749, 846618)\n",
            "\n",
            "0.5345117845117845\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (4749, 426299)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (4749, 1639146)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (4749, 1609534)\n",
            "\n",
            "0.5412457912457912\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (4749, 1269838)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (4749, 849519)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (4749, 423220)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (4749, 2058011)\n",
            "\n",
            "0.5311447811447811\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (4749, 2028399)\n",
            "\n",
            "0.5370370370370371\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (4749, 1688703)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (4749, 1268384)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (4749, 842085)\n",
            "\n",
            "0.5336700336700336\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (4749, 418865)\n",
            "\n",
            "0.5328282828282829\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 9 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "hacked email climate scientists receive death threats four car bombs exploded baghdad quick succession killing least injured dinosaur robot stolen mexico exhibition judge accuses pinochet associates poisoning former chilean president eduardo frei low doses mustard gas thallium afghanistan president warned take years able pay costs maintaining security forces mongolians damages great wall china afghanistan unable pay troops years number thereabouts long running series hell going country israel justice minister starts process talibanization religious law must become binding torah contains complete solution things dealing\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3166, 26201)\n",
            "\n",
            "0.4962121212121212\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3166, 292778)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3166, 266577)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3166, 615013)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3166, 588812)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3166, 322235)\n",
            "\n",
            "0.5176767676767676\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3166, 941247)\n",
            "\n",
            "0.5164141414141414\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3166, 915046)\n",
            "\n",
            "0.5290404040404041\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3166, 648469)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3166, 326234)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3166, 1265407)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3166, 1239206)\n",
            "\n",
            "0.5366161616161617\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3166, 972629)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3166, 650394)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3166, 324160)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3166, 1586674)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3166, 1560473)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3166, 1293896)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3166, 971661)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3166, 645427)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3166, 321267)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 10 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "four car bombs exploded baghdad quick succession killing least injured dinosaur robot stolen mexico exhibition judge accuses pinochet associates poisoning former chilean president eduardo frei low doses mustard gas thallium afghanistan president warned take years able pay costs maintaining security forces mongolians damages great wall china afghanistan unable pay troops years number thereabouts long running series hell going country israel justice minister starts process talibanization religious law must become binding torah contains complete solution things dealing fundamentalists block entire paris streets praying preventing inhabitants area going returning homes shopkeepers working ordinary citizens circulating normally karzai says afghan army need help\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3166, 27400)\n",
            "\n",
            "0.49747474747474746\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3166, 319911)\n",
            "\n",
            "0.5050505050505051\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3166, 292511)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3166, 676859)\n",
            "\n",
            "0.5025252525252525\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3166, 649459)\n",
            "\n",
            "0.5340909090909091\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3166, 356948)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3166, 1038971)\n",
            "\n",
            "0.5\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3166, 1011571)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3166, 719060)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3166, 362112)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3166, 1399200)\n",
            "\n",
            "0.5050505050505051\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3166, 1371800)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3166, 1079289)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3166, 722341)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3166, 360229)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3166, 1756580)\n",
            "\n",
            "0.5126262626262627\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3166, 1729180)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3166, 1436669)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3166, 1079721)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3166, 717609)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3166, 357380)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 11 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "dinosaur robot stolen mexico exhibition judge accuses pinochet associates poisoning former chilean president eduardo frei low doses mustard gas thallium afghanistan president warned take years able pay costs maintaining security forces mongolians damages great wall china afghanistan unable pay troops years number thereabouts long running series hell going country israel justice minister starts process talibanization religious law must become binding torah contains complete solution things dealing fundamentalists block entire paris streets praying preventing inhabitants area going returning homes shopkeepers working ordinary citizens circulating normally karzai says afghan army need help series coordinated attacks including three car rigged bombs striking near government sites killing people wounding baghdad international court justice icj holding hearings examining whether kosovo declaration independence legal russia u arguing opposite sides table\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3166, 28517)\n",
            "\n",
            "0.49873737373737376\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3166, 345840)\n",
            "\n",
            "0.5303030303030303\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3166, 317323)\n",
            "\n",
            "0.5404040404040404\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3166, 736667)\n",
            "\n",
            "0.5303030303030303\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3166, 708150)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3166, 390827)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3166, 1133892)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3166, 1105375)\n",
            "\n",
            "0.5189393939393939\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3166, 788052)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3166, 397225)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3166, 1529447)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3166, 1500930)\n",
            "\n",
            "0.51010101010101\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3166, 1183607)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3166, 792780)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3166, 395555)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3166, 1922200)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3166, 1893683)\n",
            "\n",
            "0.5151515151515151\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3166, 1576360)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3166, 1185533)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3166, 788308)\n",
            "\n",
            "0.5202020202020202\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3166, 392753)\n",
            "\n",
            "0.5189393939393939\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 12 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "judge accuses pinochet associates poisoning former chilean president eduardo frei low doses mustard gas thallium afghanistan president warned take years able pay costs maintaining security forces mongolians damages great wall china afghanistan unable pay troops years number thereabouts long running series hell going country israel justice minister starts process talibanization religious law must become binding torah contains complete solution things dealing fundamentalists block entire paris streets praying preventing inhabitants area going returning homes shopkeepers working ordinary citizens circulating normally karzai says afghan army need help series coordinated attacks including three car rigged bombs striking near government sites killing people wounding baghdad international court justice icj holding hearings examining whether kosovo declaration independence legal russia u arguing opposite sides table chinese woman managed enter japan illegally plastic surgery alter fingerprints thus fooling immigration controls police claim uganda cholera yellow fever ebola plague rise\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3166, 29608)\n",
            "\n",
            "0.5328282828282829\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3166, 371493)\n",
            "\n",
            "0.5340909090909091\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3166, 341885)\n",
            "\n",
            "0.5252525252525253\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3166, 795969)\n",
            "\n",
            "0.5340909090909091\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3166, 766361)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3166, 424476)\n",
            "\n",
            "0.5265151515151515\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3166, 1228135)\n",
            "\n",
            "0.5366161616161617\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3166, 1198527)\n",
            "\n",
            "0.5366161616161617\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3166, 856642)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3166, 432166)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3166, 1658867)\n",
            "\n",
            "0.5265151515151515\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3166, 1629259)\n",
            "\n",
            "0.5290404040404041\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3166, 1287374)\n",
            "\n",
            "0.5252525252525253\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3166, 862898)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3166, 430732)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3166, 2086860)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3166, 2057252)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3166, 1715367)\n",
            "\n",
            "0.523989898989899\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3166, 1290891)\n",
            "\n",
            "0.5227272727272727\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3166, 858725)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3166, 427993)\n",
            "\n",
            "0.5214646464646465\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 25 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "al jazeera news director resigns wikileaks disclosure reveals network edited coverage iraq war pressure us government mexican president caldern hints drug legalization consumption drugs limited decision makers must seek solutionsincluding market alternativesin order reduce astronomical earnings criminal organizations italy puts scientists trial manslaughter failing predict earthquake killed bodies found road peak hour traffic mexico palestinian foreign minister says amazed us efforts persuade countries support membership effort world first unauthorized autobiography draft julian assange book published backed return money iran released jailed americans website iraqi ministry higher education gets hacked nsfw saudi arabia executes man convicted sorcery turkey come radical solution tackling crowd violence soccer matches ban men let women children could pirate party german success repeated britain scottish nuclear fuel leak never completely cleaned scottish environment protection agency abandoned trying remove contamination north coast seabed uk government considering removing living allowance terminally ill people oh sending worrying letters goes law terminally ill need extra stress explosion amsterdam turkey bombs kurdish rebels iraq typhoon heads towards fukushima storm leaves four dead central japan mph winds threaten cause damage nuclear plant al jazeera head quits cia links row al jazeera wadah khanfar quit interview al jazeera khanfar discusses decision resign dispelled suspicions linked political pressures always pressures editorial policy never swayed palestinian refugees become citizens new palestinian state according palestines ambassador lebanon china railways ministry worlds th largest employer refuses compensate high speed crash victims mexico bodies dumped busy roadway gunmen motorists watched horror thirteen killed missing typhoon hits japan hell earth detailed satellite photos show death camps north korea still deny even exist bbc news india half dollar day adequate says panel new zealand teen banned internet making bomb threats government youtube video\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (1583, 30122)\n",
            "\n",
            "0.494949494949495\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (1583, 385139)\n",
            "\n",
            "0.48737373737373735\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (1583, 355017)\n",
            "\n",
            "0.5126262626262627\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (1583, 828926)\n",
            "\n",
            "0.49747474747474746\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (1583, 798804)\n",
            "\n",
            "0.5277777777777778\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (1583, 443787)\n",
            "\n",
            "0.5429292929292929\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (1583, 1282558)\n",
            "\n",
            "0.494949494949495\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (1583, 1252436)\n",
            "\n",
            "0.5328282828282829\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (1583, 897419)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (1583, 453632)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (1583, 1736453)\n",
            "\n",
            "0.48737373737373735\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (1583, 1706331)\n",
            "\n",
            "0.5404040404040404\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (1583, 1351314)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (1583, 907527)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (1583, 453895)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (1583, 2189220)\n",
            "\n",
            "0.4823232323232323\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (1583, 2159098)\n",
            "\n",
            "0.5404040404040404\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (1583, 1804081)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (1583, 1360294)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (1583, 906662)\n",
            "\n",
            "0.5378787878787878\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (1583, 452767)\n",
            "\n",
            "0.5378787878787878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgc2E1npZvJC"
      },
      "source": [
        "Kiértékelés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWUCZnMNfebo",
        "outputId": "9d120541-028b-4dca-e85b-b3a4849ad04a"
      },
      "source": [
        "if DATASET == 1:\n",
        "    best_model_rows = 0\n",
        "\n",
        "    for model in range(len(rows_summary_value)):\n",
        "        print(str(rows_summary_value[model]) + \":\\t\\t\\t\\t\\t\" \n",
        "              + str(rows_summary_accuraccy[model]))\n",
        "\n",
        "        if rows_summary_accuraccy[model] > best_model_rows:\n",
        "            best_model_rows = rows_summary_accuraccy[model]\n",
        "            best_model_rows_index = model\n",
        "\n",
        "    print(\"--------------------------------------------\\nBest row value:\\n\" \n",
        "          + str(rows_summary_value[best_model_rows_index]) + \"\\t\\t\\t\\t\\t\" + \n",
        "          str(rows_summary_accuraccy[best_model_rows_index]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1:\t\t\t\t\t0.5431574691732363\n",
            "2:\t\t\t\t\t0.5471578947368421\n",
            "3:\t\t\t\t\t0.5371013577518156\n",
            "4:\t\t\t\t\t0.5528421052631579\n",
            "5:\t\t\t\t\t0.5391611925214755\n",
            "6:\t\t\t\t\t0.55239898989899\n",
            "7:\t\t\t\t\t0.5496632996632996\n",
            "8:\t\t\t\t\t0.5446127946127947\n",
            "9:\t\t\t\t\t0.5366161616161617\n",
            "10:\t\t\t\t\t0.5340909090909091\n",
            "11:\t\t\t\t\t0.5404040404040404\n",
            "12:\t\t\t\t\t0.5366161616161617\n",
            "25:\t\t\t\t\t0.5429292929292929\n",
            "--------------------------------------------\n",
            "Best row value:\n",
            "4\t\t\t\t\t0.5528421052631579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GyfELNcMas"
      },
      "source": [
        "A legjobb ROWS eredményeinek megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnMyo9Ljca4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1c3762-490a-4532-8e98-a05dddfdd883"
      },
      "source": [
        "if DATASET == 1:\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\n",
        "\n",
        "    model_type = []\n",
        "    result = []\n",
        "\n",
        "    df_sum_news_labels = preprocess()\n",
        "    train = split_to_train()\n",
        "    test = split_to_test()\n",
        "\n",
        "    # check\n",
        "    split_sum = len(train) + len(test)\n",
        "    sum = len(df_sum_news_labels)\n",
        "    assert split_sum == sum    \n",
        "\n",
        "    train_headlines = []\n",
        "    test_headlines = []\n",
        "\n",
        "    for row in range(0, len(train.index)):\n",
        "        train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "    for row in range(0,len(test.index)):\n",
        "        test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "    # show the first\n",
        "    print(train_headlines[0])\n",
        "\n",
        "    for MODEL_TYPE in model_type_values:\n",
        "\n",
        "        for n in range(1,MODEL_TYPE+1):\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "            print (accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "            result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\n",
        "\n",
        "    best_model_gram = 0\n",
        "\n",
        "    for model in range(len(model_type)):\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\n",
        "\n",
        "        if result[model] > best_model_gram:\n",
        "            best_model_gram = result[model]\n",
        "            best_model_gram_index = model\n",
        "\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dick cheney death squad killed lebanese former prime minister rafik hariri tens thousands armenians march capital commemorate th anniversary armenian genocide pedophile priest hires private detectives harass victims islamic insurgents advance closer pakistani capital\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (9499, 29729)\n",
            "\n",
            "0.5111578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (9499, 366936)\n",
            "\n",
            "0.52\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (9499, 337207)\n",
            "\n",
            "0.5473684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (9499, 779700)\n",
            "\n",
            "0.527578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (9499, 749971)\n",
            "\n",
            "0.5528421052631579\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (9499, 412764)\n",
            "\n",
            "0.5389473684210526\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (9499, 1193722)\n",
            "\n",
            "0.5364210526315789\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (9499, 1163993)\n",
            "\n",
            "0.5414736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (9499, 826786)\n",
            "\n",
            "0.535157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (9499, 414022)\n",
            "\n",
            "0.5334736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (9499, 1599920)\n",
            "\n",
            "0.535578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (9499, 1570191)\n",
            "\n",
            "0.5402105263157895\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (9499, 1232984)\n",
            "\n",
            "0.535578947368421\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (9499, 820220)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (9499, 406198)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (9499, 1997014)\n",
            "\n",
            "0.5393684210526316\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (9499, 1967285)\n",
            "\n",
            "0.5402105263157895\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (9499, 1630078)\n",
            "\n",
            "0.5343157894736842\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (9499, 1217314)\n",
            "\n",
            "0.5326315789473685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (9499, 803292)\n",
            "\n",
            "0.5334736842105263\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (9499, 397094)\n",
            "\n",
            "0.5338947368421053\n",
            "1,1 n-gram:\t\t\t\t\t0.5111578947368421\n",
            "1,2 n-gram:\t\t\t\t\t0.52\n",
            "2,2 n-gram:\t\t\t\t\t0.5473684210526316\n",
            "1,3 n-gram:\t\t\t\t\t0.527578947368421\n",
            "2,3 n-gram:\t\t\t\t\t0.5528421052631579\n",
            "3,3 n-gram:\t\t\t\t\t0.5389473684210526\n",
            "1,4 n-gram:\t\t\t\t\t0.5364210526315789\n",
            "2,4 n-gram:\t\t\t\t\t0.5414736842105263\n",
            "3,4 n-gram:\t\t\t\t\t0.535157894736842\n",
            "4,4 n-gram:\t\t\t\t\t0.5334736842105263\n",
            "1,5 n-gram:\t\t\t\t\t0.535578947368421\n",
            "2,5 n-gram:\t\t\t\t\t0.5402105263157895\n",
            "3,5 n-gram:\t\t\t\t\t0.535578947368421\n",
            "4,5 n-gram:\t\t\t\t\t0.5326315789473685\n",
            "5,5 n-gram:\t\t\t\t\t0.5326315789473685\n",
            "1,6 n-gram:\t\t\t\t\t0.5393684210526316\n",
            "2,6 n-gram:\t\t\t\t\t0.5402105263157895\n",
            "3,6 n-gram:\t\t\t\t\t0.5343157894736842\n",
            "4,6 n-gram:\t\t\t\t\t0.5326315789473685\n",
            "5,6 n-gram:\t\t\t\t\t0.5334736842105263\n",
            "6,6 n-gram:\t\t\t\t\t0.5338947368421053\n",
            "--------------------------------------------\n",
            "Best model:\n",
            "2,3 n-gram\t\t\t\t\t0.5528421052631579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5fXYYfZm_7W"
      },
      "source": [
        "A legjobbhoz tartozó korrelációs tényezők megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk-qyHFInIOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e82c12-922c-43dd-d5ce-4c02aa7cce07"
      },
      "source": [
        "if DATASET == 1:\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\n",
        "\n",
        "    df_sum_news_labels = preprocess()\n",
        "    train = split_to_train()\n",
        "    test = split_to_test()\n",
        "\n",
        "    # check\n",
        "    split_sum = len(train) + len(test)\n",
        "    sum = len(df_sum_news_labels)\n",
        "    assert split_sum == sum    \n",
        "\n",
        "    train_headlines = []\n",
        "    test_headlines = []\n",
        "\n",
        "    for row in range(0, len(train.index)):\n",
        "        train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "    for row in range(0,len(test.index)):\n",
        "        test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "    # show the first\n",
        "    print(train_headlines[0])\n",
        "\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\n",
        "\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "    print (accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dick cheney death squad killed lebanese former prime minister rafik hariri tens thousands armenians march capital commemorate th anniversary armenian genocide pedophile priest hires private detectives harass victims islamic insurgents advance closer pakistani capital\n",
            "The shape is: (9499, 749971)\n",
            "\n",
            "0.5528421052631579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6Q8qicpSAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e80b00-47f0-4ed5-a4f1-6ddaf93ec49c"
      },
      "source": [
        "if DATASET == 1:\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \n",
        "                            'Coefficient' : _gram_coeffs_best_})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Word  Coefficient\n",
            "16745      air pollution     0.492042\n",
            "611543      social media     0.478259\n",
            "702463         use force     0.477721\n",
            "738134     world largest     0.436365\n",
            "483525          per cent     0.423582\n",
            "257212  french president     0.412379\n",
            "31225           anti gay     0.405971\n",
            "654661          tear gas     0.391482\n",
            "479790        peace deal     0.387368\n",
            "150645       court rules     0.383496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkcfT8TFpTlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5968cc9b-458d-4f97-f12e-e7f9bc3b289e"
      },
      "source": [
        "if DATASET == 1:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  Word  Coefficient\n",
            "698949         us army    -0.374030\n",
            "70540        bin laden    -0.375205\n",
            "616033    south korean    -0.379653\n",
            "449005  nuclear plants    -0.380581\n",
            "10848     afghan woman    -0.391471\n",
            "596210    sexual abuse    -0.393440\n",
            "538466       red cross    -0.399393\n",
            "574709      saudi king    -0.411576\n",
            "567727     russia says    -0.414118\n",
            "472517   panama papers    -0.470315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb1bb6PNYJLj",
        "outputId": "d3bf7b35-c8f0-46f2-d186-011c224ff089"
      },
      "source": [
        "# test the last 10 days\n",
        "if DATASET == 1:\n",
        "    ROWS = 25\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\n",
        "\n",
        "    # Find the cells with NaN and after the rows for them\n",
        "    is_NaN = df_for_test.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\n",
        "    rows_with_NaN = df_for_test[row_has_NaN]\n",
        "\n",
        "    # Replace them\n",
        "    df_for_test = df_for_test.replace(np.nan, \" \")\n",
        "\n",
        "    # Check the process\n",
        "    is_NaN = df_for_test.isnull()\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\n",
        "    rows_with_NaN = df_for_test[row_has_NaN]\n",
        "\n",
        "    assert len(rows_with_NaN) is 0\n",
        "\n",
        "    # Get column names\n",
        "    combined_column_names = []\n",
        "    for column in df_for_test.columns:\n",
        "      combined_column_names.append(column)\n",
        "\n",
        "    # 2D array creation for the news based on macros\n",
        "    COLUMNS = len(df_for_test)\n",
        "    news_sum = []\n",
        "    news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \n",
        "\n",
        "    # Merge the news\n",
        "    for row in range(len(df_for_test)):\n",
        "      for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "        temp = \"\"\n",
        "        news = \"\"\n",
        "        for word in range(ROWS):\n",
        "          news = df_for_test[combined_column_names[(column * ROWS) + (word + 1)]][row]\n",
        "          # Remove the b character at the begining of the string\n",
        "          if news[0] is \"b\":\n",
        "            news = \" \" + news[1:]\n",
        "          temp = temp + \" \" + news\n",
        "        news_sum[column][row] = temp\n",
        "\n",
        "    # Drop the old columns\n",
        "    for column in range(len(combined_column_names) - 1):\n",
        "      df_for_test.drop(combined_column_names[column + 1], axis = 1, inplace = True)\n",
        "\n",
        "    # Create the new columns with the merged news\n",
        "    for column in range(int((len(combined_column_names) - 1) / ROWS)):\n",
        "      colum_name = \"News_\" + str(column + 1)\n",
        "      df_for_test[colum_name] = news_sum[column]          \n",
        "\n",
        "    # The label column \n",
        "    LABEL_COLUMN = 0\n",
        "\n",
        "    news_sum = []\n",
        "    label_sum = []\n",
        "\n",
        "    # Get the column names\n",
        "    combined_column_names = []\n",
        "    for column in df_for_test.columns:\n",
        "      combined_column_names.append(column)\n",
        "\n",
        "    # Connect the merged news with the labels\n",
        "    for column in range(len(df_for_test)):\n",
        "      for row in range(len(combined_column_names) - 1):\n",
        "        news_sum.append(df_for_test[combined_column_names[row + 1]][column])\n",
        "        label_sum.append(df_for_test[combined_column_names[LABEL_COLUMN]][column])\n",
        "\n",
        "    # Create the new DataFrame\n",
        "    df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\n",
        "    df_sum_news_labels[\"News\"] = news_sum\n",
        "\n",
        "    # Removing punctuations\n",
        "    temp_news = []\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if word not in string.punctuation:\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    news_sum = temp_news\n",
        "    temp_news = []\n",
        "\n",
        "    # Remove numbers\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if not word.isdigit():\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(temp_news)):    \n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "    # Converting headlines to lower case\n",
        "    for line in range(len(temp_news)): \n",
        "        temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "    # Update the data frame\n",
        "    df_sum_news_labels[\"News\"] = temp_news\n",
        "\n",
        "    # Load the stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_sentence = []\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "\n",
        "    # Remove stop words\n",
        "    for line in news_sum:\n",
        "      word_tokens = word_tokenize(line)\n",
        "      temp_attach = \"\"\n",
        "      for word in word_tokens:\n",
        "        temp = \" \"\n",
        "        if not word in stop_words:\n",
        "          temp = temp + word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      filtered_sentence.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(filtered_sentence)):    \n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "    # Update the data frame\n",
        "    df_sum_news_labels[\"News\"] = filtered_sentence\n",
        "\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "\n",
        "    for row in null_indexes:\n",
        "      df_sum_news_labels = df_sum_news_labels.drop(row)\n",
        "\n",
        "    news_sum = df_sum_news_labels[\"News\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "      \n",
        "    assert len(null_indexes) is 0\n",
        "\n",
        "    compare = []\n",
        "\n",
        "    for row in range(0, len(df_sum_news_labels.index)):\n",
        "      compare.append(df_sum_news_labels.iloc[row, 1])\n",
        "\n",
        "    print(df_sum_news_labels.head())\n",
        "\n",
        "    print(compare[0])\n",
        "\n",
        "    _gram_test_compare_ = _gram_vectorizer_.transform(compare)\n",
        "    compare_predict = _gram_model_.predict(_gram_test_compare_)\n",
        "    compare_predict_proba = _gram_model_.predict_proba(_gram_test_compare_)\n",
        "\n",
        "    print(compare_predict)    \n",
        "    print(compare_predict_proba)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      1  staggering percent venezuelans say money buy e...\n",
            "1      1  australian athlete competed six paralympic gam...\n",
            "2      0  german government agrees ban fracking indefini...\n",
            "3      1  today united kingdom decides whether remain eu...\n",
            "4      0  david cameron resign pm eu referendum bbc fore...\n",
            "staggering percent venezuelans say money buy enough food two corporate whistleblowers may enter plea bargain deal would tie brazilian lawmakers corruption cases poland together russia iran several gulf states successfully removed decriminalization homosexuality un resolution three environmental activists killed per week last year murdered defending land rights environment mining dam projects logging ontario funeral business dissolves dead pours town sewers new declassified documents reveal cia abused tortured prisoners graphic tens thousands people gathered sweltering heat japan okinawa island sunday one biggest demonstrations two decades u military bases following arrest american suspected murdering local woman japan dementia crisis hits record levels thousands go missing national police agency reports patients going missing hundreds later found dead icelands hekla volcano popular tourist destination ready blow corbyn pledges kill ttip elected venezuelans ransack stores hunger grips nation year old girl shot death street gangs fought food rome elects first female mayor saudi arabia kuwait angry hillary clintons claims fund terrorism two embassies canberra australia denounced presidential candidate remarks said also suffer terrorism professor dismissed insulting turkey president russian soldier dies syria preventing car bomb attack aid distribution point three dead injured labor union clashes police mexico indonesia vows stand firm skirmishes chinese ships china claims south china sea trillion ship borne trade passes every year philippines vietnam malaysia taiwan brunei overlapping claims study ocean plankton shown increase water temperature worlds oceans around c f scientists predict could occur soon could stop oxygen production phytoplankton disrupting process photosynthesis australia taxes foreign home buyers affordability bites sydney imposing new taxes foreigners buying homes concerns grow flood mostly chinese investors crowding locals killing great australian dream owning property paris happy amazon one hour delivery service australian man pleads guilty making sexual threats social media landmark victory opponents online harassment friends defended online alchin wrote fifty posts including rape threats saying women never given rights trudeau condemns killing canadian embassy security guards kabul vladimir putin considering selling part russias corporate crown jewels china india president struggles meet spending commitments possible election bid less two years elephant survived shot head suspected poachers zimbabwe found vets mana pools national park believed bullet lodged head six weeks wikileaks founder julian assange marks years holed ecuadorean embassy\n",
            "[0 0 1 1 1 0 1 0 1 0]\n",
            "[[0.7508097  0.2491903 ]\n",
            " [0.87543248 0.12456752]\n",
            " [0.25148159 0.74851841]\n",
            " [0.42722868 0.57277132]\n",
            " [0.14695798 0.85304202]\n",
            " [0.54693625 0.45306375]\n",
            " [0.19529709 0.80470291]\n",
            " [0.6458013  0.3541987 ]\n",
            " [0.48186021 0.51813979]\n",
            " [0.711483   0.288517  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNhn5tX5gfCP"
      },
      "source": [
        "if DATASET == 1:\n",
        "    # create dataframe template\n",
        "    df_compare = df_for_test\n",
        "\n",
        "    # Get the column names\n",
        "    combined_column_names = []\n",
        "    for column in df_for_test.columns:\n",
        "      combined_column_names.append(column)\n",
        "\n",
        "    drop_number = -1 * (len(combined_column_names) - 1)\n",
        "\n",
        "    df_dropped = df_compare.drop(df_compare.columns[drop_number:],axis=1)\n",
        "\n",
        "    df_dropped[\"0\"] = compare_predict_proba[:,0]\n",
        "    df_dropped[\"1\"] = compare_predict_proba[:,1]\n",
        "    df_dropped[\"Predict\"] = compare_predict\n",
        "\n",
        "    match = []\n",
        "    for row in range(len(df_dropped)):\n",
        "        if df_dropped[\"Label\"][row] == df_dropped[\"Predict\"][row]:\n",
        "            match.append(1)\n",
        "        else:\n",
        "            match.append(0)\n",
        "\n",
        "    df_dropped[\"Match\"] = match\n",
        "\n",
        "    df_dropped"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKZQkIdFAXfG"
      },
      "source": [
        "## **ECO_BSN_DF, ECO_FNC_DF, ECO_US_DF 2008-2016 (2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyZGBhb0CcgL"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\n",
        "\n",
        "Ezeket az adathalmazokat én magam gyűjtöttem az alábbi oldalakról:\n",
        "\n",
        "\n",
        "*   https://www.economist.com/business/ \n",
        "*   https://www.economist.com/finance-and-economics/ \n",
        "*   https://www.economist.com/united-states/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8lzj6Qs9_e"
      },
      "source": [
        "### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fORGByWVtCtj"
      },
      "source": [
        "Először betöltöm külön-külön az adathalmazokat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVNdyg2RtCR-"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Copy the dataset to the local environment\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_BSN_DF.csv\" \"ECO_BSN_DF.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_FNC_DF.csv\" \"ECO_FNC_DF.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_US_DF.csv\" \"ECO_US_DF.csv\"\n",
        "\n",
        "\n",
        "    # Check the copy is succesfull -> good if no assertation error\n",
        "    read = !ls\n",
        "    assert read[0].find(\"ECO_FNC_DF.csv\") != -1\n",
        "    assert read[0].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \n",
        "    assert read[1].find(\"ECO_BSN_DF.csv\") != -1\n",
        "    assert read[1].find(\"ECO_US_DF.csv\") != -1\n",
        "\n",
        "    # Load the datasets \n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\n",
        "    df_bsn = pd.read_csv('ECO_BSN_DF.csv', index_col = \"date\")\n",
        "    df_fnc = pd.read_csv('ECO_FNC_DF.csv', index_col = \"date\")\n",
        "    df_us = pd.read_csv('ECO_US_DF.csv', index_col = \"date\")\n",
        "\n",
        "    # Load the stock data\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkYz3ExjupOh"
      },
      "source": [
        "Az adathalmazok megvizsgálása az elemein keresztül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNBo9X2RuoaT"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Show the dataframe\n",
        "    print(\"Reddit\")\n",
        "    print(df_reddit.head())\n",
        "    print(\"\\n\\nBSN ECO\")\n",
        "    print(df_bsn.head())\n",
        "    print(\"\\n\\nFNC ECO\")\n",
        "    print(df_fnc.head())\n",
        "    print(\"\\n\\nUS ECO\")\n",
        "    print(df_us.head())"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf587n4txlu2"
      },
      "source": [
        "Azon elemek megkeresése az ECO adathalmazból ami beleesik a vizsgált időintervallumba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm4PdT-TxyV2"
      },
      "source": [
        "if DATASET == 2:\n",
        "    df_bsn_inspect = df_bsn[df_bsn.index < '2016/07/02']\n",
        "    df_bsn_inspect = df_bsn_inspect[df_bsn_inspect.index > '2008/08/07']\n",
        "    df_bsn_inspect = df_bsn_inspect.drop_duplicates()\n",
        "\n",
        "    df_fnc_inspect = df_fnc[df_fnc.index < '2016/07/02']\n",
        "    df_fnc_inspect = df_fnc_inspect[df_fnc_inspect.index > '2008/08/07']\n",
        "    df_fnc_inspect = df_fnc_inspect.drop_duplicates()\n",
        "\n",
        "    df_us_inspect = df_us[df_us.index < '2016/07/02']\n",
        "    df_us_inspect = df_us_inspect[df_us_inspect.index > '2008/08/07']\n",
        "    df_us_inspect = df_us_inspect.drop_duplicates()\n",
        "\n",
        "    print(\"BSN ECO\")\n",
        "    print(df_bsn_inspect.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_bsn_inspect.tail(2))\n",
        "    print(df_bsn_inspect.shape)\n",
        "\n",
        "    print(\"\\n\\nFNC ECO\")\n",
        "    print(df_fnc_inspect.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_fnc_inspect.tail(2))\n",
        "    print(df_fnc_inspect.shape)\n",
        "\n",
        "    print(\"\\n\\nUS ECO\")\n",
        "    print(df_us_inspect.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_us_inspect.tail(2))\n",
        "    print(df_us_inspect.shape)\n",
        "\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_bsn_inspect) + len(df_fnc_inspect) + len(df_us_inspect)))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCBuxng0j-S"
      },
      "source": [
        "Az Economist oldalról származó adathalmazok összefűzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDOluCHa1ASA"
      },
      "source": [
        "if DATASET == 2:\n",
        "    df_eco_all = pd.concat([df_bsn_inspect, df_fnc_inspect, df_us_inspect])\n",
        "\n",
        "    df_eco_all = df_eco_all.drop_duplicates()\n",
        "\n",
        "    df_eco_all.sort_index(ascending=True, inplace=True)\n",
        "\n",
        "    print(\"ECO MERGED\")\n",
        "    print(df_eco_all.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_eco_all.tail(2))\n",
        "    print(df_eco_all.shape)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9cTQSlx263t"
      },
      "source": [
        "Egy naphoz tartozó azonos hírek vizsgálata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpmSZfz2-0D"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Groupby by date\n",
        "    dates = df_eco_all.groupby(\"date\")\n",
        "\n",
        "    # Summary statistic\n",
        "    print(\"Max:\")\n",
        "    print(dates.describe().max())\n",
        "    print(\"\\n\\nMin:\")\n",
        "    print(dates.describe().min())"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DuBpOl4MWn"
      },
      "source": [
        "if DATASET == 2:\n",
        "    dates_count = [] # for count\n",
        "    dates_dates = [] # for indexing\n",
        "    df_dates = dates.describe()\n",
        "\n",
        "    for row in range(len(df_dates)):\n",
        "        dates_count.append(len(dates.get_group(df_dates.index[row])))\n",
        "        dates_dates.append(dates.get_group(df_dates.index[row]).index[0])\n",
        "\n",
        "    df_group_dates = pd.DataFrame()\n",
        "    df_group_dates[\"date\"] = dates_dates\n",
        "    df_group_dates[\"count\"] = dates_count\n",
        "    df_group_dates.set_index(\"date\", inplace=True)\n",
        "    df_group_dates.sort_index(ascending=True, inplace=True)\n",
        "\n",
        "    print(df_group_dates.head())"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDcXZfr9CS2"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Groupby by date\n",
        "    counts = df_group_dates.groupby(\"count\")\n",
        "\n",
        "    keys = list(counts.groups.keys())\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for key in keys:\n",
        "      sum = sum + key * len(counts.get_group(key))\n",
        "      print(\"Count: \" + str(key) + \"\\t\\t\" + str(len(counts.get_group(key))))\n",
        "\n",
        "    print(\"\\n\\nSummary:\\t\\t\" + str(sum))  "
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1JU31uKqLnw"
      },
      "source": [
        "Az összefűzött hírekhez a címkék generálása a részvény árfolyama alapján."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O6BT2vQKnFu"
      },
      "source": [
        "if DATASET == 2:\n",
        "    days = []\n",
        "    stock_days = []\n",
        "    wrong_days = []\n",
        "\n",
        "    # Create dates and remove duplicates\n",
        "    for day in range(len(df_eco_all.index)):\n",
        "        if day == 0:\n",
        "            days.append(str(df_eco_all.index[day]))\n",
        "        elif df_eco_all.index[day] != days[len(days) - 1]:\n",
        "            days.append(str(df_eco_all.index[day]))\n",
        "\n",
        "    # Drop not needed days\n",
        "    for day in range(len(df_stock.index)):\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\n",
        "\n",
        "    # Remove not relevant date\n",
        "    good_days = []\n",
        "    for day in days:\n",
        "        try:\n",
        "            if stock_days.index(day):\n",
        "                good_days.append(str(day))\n",
        "        except:\n",
        "            wrong_days.append(str(day))\n",
        "\n",
        "    print(\"All days:\\t\\t\" + str(len(days)))\n",
        "    print(\"Good days:\\t\\t\" + str(len(good_days)))\n",
        "    print(\"Wrong days:\\t\\t\" + str(len(wrong_days)))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBpmP0WArssx"
      },
      "source": [
        "if DATASET == 2:\n",
        "    label_eco = []\n",
        "    date_label_eco =[]\n",
        "    title_label_eco = []\n",
        "\n",
        "    for day in range(len(good_days)):\n",
        "        if day == 0:\n",
        "            title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\n",
        "            label_eco.append(0)\n",
        "            date_label_eco.append(good_days[day])      \n",
        "        # label should be 1 -> rise\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\n",
        "                    title_label_eco.append(row)\n",
        "                    label_eco.append(1)\n",
        "                    date_label_eco.append(good_days[day])\n",
        "            else:\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\n",
        "                    label_eco.append(1)\n",
        "                    date_label_eco.append(good_days[day])\n",
        "\n",
        "        # label should be 0 -> fall\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\n",
        "                    title_label_eco.append(row)\n",
        "                    label_eco.append(0)\n",
        "                    date_label_eco.append(good_days[day])\n",
        "            else:\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\n",
        "                    label_eco.append(0)\n",
        "                    date_label_eco.append(good_days[day])\n",
        "\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_eco)))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ec36DKBX9F"
      },
      "source": [
        "A címkékkel rendelkező, használható adatokból egy új adathalmaz létrehozása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEXYSpdbBVk9"
      },
      "source": [
        "if DATASET == 2:\n",
        "    df_eco = pd.DataFrame()\n",
        "    df_eco[\"date\"] = date_label_eco\n",
        "    df_eco[\"label\"] = label_eco\n",
        "    df_eco[\"title\"] = title_label_eco\n",
        "    df_eco.set_index(\"date\", inplace=True)\n",
        "    df_eco.sort_index(ascending=True, inplace=True)\n",
        "    print(df_eco.head())\n",
        "    print(len(df_eco))\n",
        "\n",
        "    # drop duplicates\n",
        "    df_eco.drop_duplicates(subset=\"title\", inplace=True)\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\n",
        "    print(df_eco.head())\n",
        "    print(len(df_eco))    "
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREcqTfFmUwJ"
      },
      "source": [
        "### Adathalmazok előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa5nBTd8OFpB"
      },
      "source": [
        "Az adathalmaz megtisztítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7GYSLxOIrX"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Removing punctuations\n",
        "    temp_news = []\n",
        "    news_sum = df_eco[\"title\"]\n",
        "\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      try:\n",
        "          for word in line:\n",
        "            temp = \" \"\n",
        "            if word not in string.punctuation:\n",
        "              temp = word\n",
        "            temp_attach = temp_attach + \"\".join(temp)\n",
        "      except:\n",
        "          temp = \" \"\n",
        "          temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    news_sum = temp_news\n",
        "    temp_news = []\n",
        "\n",
        "    # Remove numbers\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if not word.isdigit():\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(temp_news)):    \n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "    # Converting headlines to lower case\n",
        "    for line in range(len(temp_news)): \n",
        "        temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "    # Update the data frame\n",
        "    df_eco[\"title\"] = temp_news\n",
        "\n",
        "    # Load the stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_sentence = []\n",
        "    news_sum = df_eco[\"title\"]\n",
        "\n",
        "    # Remove stop words\n",
        "    for line in news_sum:\n",
        "      word_tokens = word_tokenize(line)\n",
        "      temp_attach = \"\"\n",
        "      for word in word_tokens:\n",
        "        temp = \" \"\n",
        "        if not word in stop_words:\n",
        "          temp = temp + word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      filtered_sentence.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(filtered_sentence)):    \n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "    # Update the data frame\n",
        "    df_eco[\"title\"] = filtered_sentence\n",
        "\n",
        "    # Reset the index\n",
        "    df_eco.reset_index(inplace=True)\n",
        "\n",
        "    news_sum = df_eco[\"title\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "\n",
        "    print(null_indexes)\n",
        "\n",
        "    for row in range(len(null_indexes)):\n",
        "      df_eco = df_eco.drop(df_eco.index[null_indexes[row] - row])\n",
        "\n",
        "    news_sum = df_eco[\"title\"]\n",
        "    null_indexes = []\n",
        "    index = 0\n",
        "\n",
        "    for line in news_sum:\n",
        "      if line is \"\":\n",
        "        null_indexes.append(index)\n",
        "      index = index + 1\n",
        "      \n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_b24C3cmZIQ"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és tesztelő adathalmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0BpPPB_mYAN"
      },
      "source": [
        "if DATASET == 2:\n",
        "    # Drop the dates\n",
        "    df_eco_label_title = pd.DataFrame()\n",
        "    df_eco_label_title[\"label\"] = df_eco[\"label\"]\n",
        "    df_eco_label_title[\"title\"] = df_eco[\"title\"]\n",
        "    print(\"New dataset without the dates\")\n",
        "    print(df_eco_label_title.head())\n",
        "    print(len(df_eco_label_title))\n",
        "\n",
        "    # Do the shuffle\n",
        "    for i in range(SHUFFLE_CYCLE):\n",
        "      df_eco_label_title = shuffle(df_eco_label_title, random_state = RANDOM_SEED)\n",
        "\n",
        "    # Reset the index\n",
        "    df_eco_label_title.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Show the data frame\n",
        "    print(\"\\n\\nAfter shuffle\")\n",
        "    print(df_eco_label_title.head())    \n",
        "\n",
        "    # Split the dataset\n",
        "    INPUT_SIZE = len(df_eco_label_title)\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\n",
        "\n",
        "    train = df_eco_label_title[:TRAIN_SIZE] \n",
        "    test = df_eco_label_title[TRAIN_SIZE:]\n",
        "\n",
        "    # Print out the length\n",
        "    print(\"\\n\\nAfter split\")\n",
        "    print(\"Train data set length: \" + str(len(train)))\n",
        "    print(\"Test data set length: \" + str(len(test)))\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\n",
        "    print(\"Dataset summa before split: \" + str(len(df_eco_label_title)))\n",
        "\n",
        "    # check\n",
        "    split_sum = len(train) + len(test)\n",
        "    sum = len(df_eco_label_title)\n",
        "    assert split_sum == sum"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmLc118-ozZo"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTDyz6azo5H7"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCwyjWHNo3Y1"
      },
      "source": [
        "if DATASET == 2:\n",
        "    model_type = []\n",
        "    result = []\n",
        "    model_type_values = []\n",
        "    train_headlines = []\n",
        "    test_headlines = []\n",
        "\n",
        "    # Create model type values\n",
        "    for value in range(1,7):\n",
        "        model_type_values.append(value)\n",
        "\n",
        "    for row in range(0, len(train.index)):\n",
        "        train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "    for row in range(0,len(test.index)):\n",
        "        test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "\n",
        "    for MODEL_TYPE in model_type_values:\n",
        "\n",
        "        for n in range(1,MODEL_TYPE+1):\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\n",
        "\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "    best_model_gram = 0\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    for model in range(len(model_type)):\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\n",
        "\n",
        "        if result[model] > best_model_gram:\n",
        "            best_model_gram = result[model]\n",
        "            best_model_gram_index = model\n",
        "\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9gbVqadzbUd"
      },
      "source": [
        "if DATASET == 2:\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\n",
        "\n",
        "    # show the first\n",
        "    print(train_headlines[0])\n",
        "\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\n",
        "\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU4nnwBzzmOF"
      },
      "source": [
        "if DATASET == 2:\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \n",
        "                            'Coefficient' : _gram_coeffs_best_})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXZ75tt2zobK"
      },
      "source": [
        "if DATASET == 2:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wcHp91ptLF-"
      },
      "source": [
        "## **KAG_BENZ_ANALYST_DF, KAG_BENZ_PARTNER_DF 2008-2016**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8XoC5ltwPe"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\n",
        "\n",
        "Ezen adathalmazok forrása:\n",
        "\n",
        "*   https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPbi6d_zLCn"
      },
      "source": [
        "#### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pi6pacf80r5"
      },
      "source": [
        "if DATASET == 3:\n",
        "    # Copy the dataset to the local environment\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_1.csv\" \"KAG_BENZ_ANALYST_DF_1.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_2.csv\" \"KAG_BENZ_ANALYST_DF_2.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_1.csv\" \"KAG_BENZ_PARTNER_DF_1.csv\"\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_2.csv\" \"KAG_BENZ_PARTNER_DF_2.csv\"\n",
        "\n",
        "\n",
        "    # Check the copy is succesfull -> good if no assertation error\n",
        "    read = !ls\n",
        "    assert read[1].find(\"KAG_BENZ_ANALYST_DF_1.csv\") != -1\n",
        "    assert read[2].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \n",
        "    assert read[2].find(\"KAG_BENZ_ANALYST_DF_2.csv\") != -1\n",
        "    assert read[0].find(\"KAG_BENZ_PARTNER_DF_1.csv\") != -1    \n",
        "    assert read[1].find(\"KAG_BENZ_PARTNER_DF_2.csv\") != -1\n",
        "\n",
        "    # Load the datasets \n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\n",
        "    df_benz_1 = pd.read_csv('KAG_BENZ_ANALYST_DF_1.csv', index_col = \"date\")\n",
        "    df_benz_2 = pd.read_csv('KAG_BENZ_ANALYST_DF_2.csv', index_col = \"date\")    \n",
        "    df_partner_1 = pd.read_csv('KAG_BENZ_PARTNER_DF_1.csv', index_col = \"date\")\n",
        "    df_partner_2 = pd.read_csv('KAG_BENZ_PARTNER_DF_2.csv', index_col = \"date\")\n",
        "\n",
        "    # Load the stock data\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU771dZOCrfM"
      },
      "source": [
        "A szétbontott adathalmazok összefűzése, majd azok megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1W5V63vCyA4"
      },
      "source": [
        "if DATASET == 3:\n",
        "    # Merge them\n",
        "    df_benz = pd.concat([df_benz_1, df_benz_2])\n",
        "    df_partner = pd.concat([df_partner_1, df_partner_2])\n",
        "\n",
        "    # Show the dataframe\n",
        "    print(\"BENZ\")\n",
        "    print(df_benz.head())\n",
        "    print(\"...\")\n",
        "    print(df_benz.tail())\n",
        "    print(len(df_benz))\n",
        "    print(\"\\n\\nPARTNER\")\n",
        "    print(df_partner.head())\n",
        "    print(\"...\")\n",
        "    print(df_partner.tail())\n",
        "    print(len(df_partner))"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxza21dEGyDD"
      },
      "source": [
        "A vizsgált időtartamba eső adatok kiszűrése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmab7DBG5cB"
      },
      "source": [
        "if DATASET == 3:\n",
        "    df_benz_inspect = df_benz[df_benz.index < '2016/07/02']\n",
        "    df_benz_inspect = df_benz_inspect[df_benz_inspect.index > '2008/08/07']\n",
        "    df_benz_inspect = df_benz_inspect.drop_duplicates()\n",
        "\n",
        "    df_partner_inspect = df_partner[df_partner.index < '2016/07/02']\n",
        "    df_partner_inspect = df_partner_inspect[df_partner_inspect.index > '2008/08/07']\n",
        "    df_partner_inspect = df_partner_inspect.drop_duplicates()\n",
        "\n",
        "    print(\"BENZ\")\n",
        "    print(df_benz_inspect.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_benz_inspect.tail())\n",
        "    print(df_benz_inspect.shape)\n",
        "\n",
        "    print(\"\\n\\nPARTNER\")\n",
        "    print(df_partner_inspect.head())\n",
        "    print(\"...\")\n",
        "    print(df_partner_inspect.tail())\n",
        "    print(df_partner_inspect.shape)\n",
        "\n",
        "    df_benz = pd.concat([df_benz_inspect, df_partner_inspect])\n",
        "\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_benz_inspect) + len(df_partner_inspect)))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UngRtIFVpi"
      },
      "source": [
        "Az adathalmazban található adatok kiszűrése a Dow Jonews Industrial Average alapján:\n",
        "\n",
        "\n",
        "*   Procter & Gamble, PG, 1932-05-26\n",
        "*   3M Company, MMM, 1976-08-09\n",
        "*   IBM, IBM, 1979-06-29\n",
        "*   Merck & Co., MRK, 1979-06-29\n",
        "*   American Express, AXP, 1982-08-30\n",
        "*   McDonald's, MCD, 1985-10-30\n",
        "*   Boeing, BA, 1987-03-12\n",
        "*   The Coca-Cola Company, KO, 1987-03-12\n",
        "*   Caterpillar Inc., CAT, 1991-05-06\n",
        "*   JPMorgan Chase, JPM, 1991-05-06\n",
        "*   The Walt Disney Company, DIS, 1991-05-06\n",
        "*   Johnson & Johnson, JNJ, 1997-03-17\n",
        "*   Walmart, WMT, 1997-03-17\n",
        "*   The Home Depot, HD, 1999-11-01\n",
        "*   Intel, INTC, 1999-11-01\n",
        "*   Microsoft, MSFT, 1999-11-01\n",
        "*   Verizon, VZ, 2004-04-08\n",
        "*   Chevron Corporation, CVX, 2008-02-19\n",
        "*   Cisco Systems, CSCO, 2009-06-08\n",
        "*   The Travelers Companies, \tTRV, 2009-06-08\n",
        "*   UnitedHealth Group, UNH, \t2012-09-24\n",
        "*   Goldman Sachs, GS, \t2013-09-20\t\n",
        "*   Nike, NKE, 2013-09-20\n",
        "*   Visa Inc., \tV, 2013-09-20\t\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaU5ro8Y1vl"
      },
      "source": [
        "if DATASET == 3:\n",
        "    tickers = []\n",
        "    unique_count = []\n",
        "\n",
        "    # The stock tickers which is needed\n",
        "    stock_ticker = [\"PG\", \"MMM\", \"IBM\", \"MRK\", \"AXP\", \"MCD\", \"BA\", \"KO\", \"CAT\", \"JPM\",\n",
        "                    \"DIS\", \"JNJ\", \"WMT\", \"HD\", \"INTC\", \"MSFT\", \"VZ\", \"CVX\", \"CSCO\",\n",
        "                    \"TRV\", \"UNH\", \"GS\", \"NKE\", \"V\"]\n",
        "\n",
        "    stocks_benz = df_benz.groupby(\"stock\")\n",
        "    stock_benz_df = stocks_benz.describe()\n",
        "\n",
        "    for stock in range(len(stock_benz_df.index)):\n",
        "        tickers.append(stock_benz_df.index[stock])\n",
        "\n",
        "    for stock in stock_ticker:\n",
        "        try:\n",
        "            unique_count.append(stock_benz_df.iloc[tickers.index(stock), :][1]) #unique\n",
        "        except:\n",
        "            unique_count.append(0)\n",
        "            print(str(stock) + \"\\tis not in list\")\n",
        "\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\n",
        "\n",
        "    sum_count = 0\n",
        "\n",
        "    for stock in range(len(stock_ticker)):\n",
        "        print(str(stock_ticker[stock]) + \"\\t\\t\\t\" + str(unique_count[stock]))\n",
        "        sum_count = sum_count + unique_count[stock]\n",
        "\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\n",
        "    print(\"Summary of news with the tickers:\\t\" + str(sum_count))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okO_8su2ABZL"
      },
      "source": [
        "if DATASET == 3:\n",
        "    df_benz_filtered = pd.DataFrame()\n",
        "\n",
        "    for stock in stock_ticker:\n",
        "        df_temp = df_benz[(df_benz[\"stock\"]) == stock].drop_duplicates()\n",
        "        df_benz_filtered = pd.concat([df_benz_filtered, df_temp])\n",
        "\n",
        "    df_benz_filtered.sort_index(ascending=True, inplace=True)\n",
        "\n",
        "    df_benz = df_benz_filtered\n",
        "    df_benz.drop(\"stock\", axis = 1, inplace = True)\n",
        "    df_benz.drop_duplicates(inplace=True)\n",
        "\n",
        "    print(\"BENZ\")\n",
        "    print(df_benz.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_benz.tail(2))\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_fkeQjJPtw"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVB_ZTTqKTYR"
      },
      "source": [
        "A szöveg előfeldolgozása következik, mint az írásjelek eltűvolítása, a számok eltávolítása, felesleges szóközöktől való megtisztítás, minden szó kisbetűs szóra cserélése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_03-r1XOKWsY"
      },
      "source": [
        "if DATASET == 3:\n",
        "    # Removing punctuations\n",
        "    temp_news = []\n",
        "    news_sum = df_benz[\"headline\"]\n",
        "\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if word not in string.punctuation:\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    news_sum = temp_news\n",
        "    temp_news = []\n",
        "\n",
        "    # Remove numbers\n",
        "    for line in news_sum:\n",
        "      temp_attach = \"\"\n",
        "      for word in line:\n",
        "        temp = \" \"\n",
        "        if not word.isdigit():\n",
        "          temp = word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      temp_news.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(temp_news)):    \n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\n",
        "\n",
        "    # Converting headlines to lower case\n",
        "    for line in range(len(temp_news)): \n",
        "        temp_news[line] = temp_news[line].lower()\n",
        "\n",
        "    # Update the data frame\n",
        "    df_benz[\"headline\"] = temp_news\n",
        "\n",
        "    # Load the stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_sentence = []\n",
        "    news_sum = df_benz[\"headline\"]\n",
        "\n",
        "    # Remove stop words\n",
        "    for line in news_sum:\n",
        "      word_tokens = word_tokenize(line)\n",
        "      temp_attach = \"\"\n",
        "      for word in word_tokens:\n",
        "        temp = \" \"\n",
        "        if not word in stop_words:\n",
        "          temp = temp + word\n",
        "        temp_attach = temp_attach + \"\".join(temp)\n",
        "      filtered_sentence.append(temp_attach)\n",
        "\n",
        "    # Remove space\n",
        "    for line in range(len(filtered_sentence)):    \n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\n",
        "\n",
        "    # Update the data frame\n",
        "    df_benz[\"headline\"] = filtered_sentence\n",
        "\n",
        "    print(\"BENZ\")\n",
        "    print(df_benz.head(2))\n",
        "    print(\"...\")\n",
        "    print(df_benz.tail(2))\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxxhSMNJSP-"
      },
      "source": [
        "### Címke létrehozása, adathalmaz felbontása"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bgzTSWYLV1r"
      },
      "source": [
        "A következőkben a címkék generálása és az adathalmaz felbontása történik tanító és validáló halmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDFCry1TLbYn"
      },
      "source": [
        "if DATASET == 3:\n",
        "    days = []\n",
        "    stock_days = []\n",
        "    wrong_days = []\n",
        "\n",
        "    # Create dates and remove duplicates\n",
        "    for day in range(len(df_benz.index)):\n",
        "        temp = str(df_benz.index[day])[0:10].replace(\"-\",\"/\")\n",
        "        if day == 0:\n",
        "            days.append(temp)\n",
        "        elif df_benz.index[day] != df_benz.index[day - 1]:\n",
        "            days.append(temp)\n",
        "\n",
        "    # Update the dataframe date column\n",
        "    df_benz.reset_index(inplace=True)\n",
        "    temp_days = df_benz[\"date\"]\n",
        "    days_to_update = []\n",
        "    for date in range(len(temp_days)):\n",
        "        temp = str(temp_days[date])[0:10].replace(\"-\",\"/\")\n",
        "        days_to_update.append(temp)\n",
        "\n",
        "    df_benz[\"date\"] = days_to_update\n",
        "    df_benz.set_index(\"date\", inplace=True, drop=True)    \n",
        "\n",
        "    # Drop not needed days\n",
        "    for day in range(len(df_stock.index)):\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\n",
        "\n",
        "    # Remove not relevant date\n",
        "    good_days = []\n",
        "    for day in days:\n",
        "        try:\n",
        "            if stock_days.index(day):\n",
        "                good_days.append(str(day))\n",
        "        except:\n",
        "            wrong_days.append(str(day))\n",
        "\n",
        "    print(\"All days:\\t\\t\\t\\t\" + str(len(days)))\n",
        "    print(\"Good days:\\t\\t\\t\\t\" + str(len(good_days)))\n",
        "    print(\"Wrong days:\\t\\t\\t\\t\" + str(len(wrong_days)))\n",
        "\n",
        "    label_benz = []\n",
        "    date_label_benz =[]\n",
        "    title_label_benz = []\n",
        "\n",
        "    for day in range(len(good_days)):\n",
        "        if day == 0:\n",
        "            title_label_benz.append(df_benz[\"headline\"][good_days[day]])\n",
        "            label_benz.append(0)\n",
        "            date_label_benz.append(good_days[day])      \n",
        "        # label should be 1 -> rise\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\n",
        "                    title_label_benz.append(row)\n",
        "                    label_benz.append(1)\n",
        "                    date_label_benz.append(good_days[day])\n",
        "            else:\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\n",
        "                    label_benz.append(1)\n",
        "                    date_label_benz.append(good_days[day])\n",
        "\n",
        "        # label should be 0 -> fall\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\n",
        "                    title_label_benz.append(row)\n",
        "                    label_benz.append(0)\n",
        "                    date_label_benz.append(good_days[day])\n",
        "            else:\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\n",
        "                    label_benz.append(0)\n",
        "                    date_label_benz.append(good_days[day])\n",
        "\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_benz)))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8BBw1Zok7W"
      },
      "source": [
        "if DATASET == 3:\n",
        "    df_benz = pd.DataFrame()\n",
        "    df_benz[\"date\"] = date_label_benz\n",
        "    df_benz[\"label\"] = label_benz\n",
        "    df_benz[\"title\"] = title_label_benz\n",
        "    df_benz.set_index(\"date\", inplace=True)\n",
        "    df_benz.sort_index(ascending=True, inplace=True)\n",
        "    print(df_benz.head())\n",
        "    print(\"...\")\n",
        "    print(df_benz.tail())\n",
        "    print(df_benz.shape)\n",
        "\n",
        "    # drop duplicates\n",
        "    df_benz.drop_duplicates(subset=\"title\", inplace=True)\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\n",
        "    print(df_benz.head())\n",
        "    print(\"...\")\n",
        "    print(df_benz.tail())\n",
        "    print(df_benz.shape) "
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dq2UTnqrJAw"
      },
      "source": [
        "Az adathalmaz felbontása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSkAzvfVrWGJ"
      },
      "source": [
        "if DATASET == 3:\n",
        "    # Drop the dates\n",
        "    df_benz_label_title = pd.DataFrame()\n",
        "    df_benz_label_title[\"label\"] = df_benz[\"label\"]\n",
        "    df_benz_label_title[\"title\"] = df_benz[\"title\"]\n",
        "    # Reset the index\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\n",
        "    print(\"New dataset without the dates\")\n",
        "    print(df_benz_label_title.head())\n",
        "    print(len(df_benz_label_title))\n",
        "\n",
        "    # Do the shuffle\n",
        "    for i in range(SHUFFLE_CYCLE):\n",
        "      df_benz_label_title = shuffle(df_benz_label_title, random_state = RANDOM_SEED)\n",
        "\n",
        "    # Reset the index\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # Show the data frame\n",
        "    print(\"\\n\\nAfter shuffle\")\n",
        "    print(df_benz_label_title.head())    \n",
        "\n",
        "    # Split the dataset\n",
        "    INPUT_SIZE = len(df_benz_label_title)\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\n",
        "\n",
        "    train = df_benz_label_title[:TRAIN_SIZE] \n",
        "    test = df_benz_label_title[TRAIN_SIZE:]\n",
        "\n",
        "    # Print out the length\n",
        "    print(\"\\n\\nAfter split\")\n",
        "    print(\"Train data set length: \" + str(len(train)))\n",
        "    print(\"Test data set length: \" + str(len(test)))\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\n",
        "    print(\"Dataset summa before split: \" + str(len(df_benz_label_title)))\n",
        "\n",
        "    # check\n",
        "    split_sum = len(train) + len(test)\n",
        "    sum = len(df_benz_label_title)\n",
        "    assert split_sum == sum"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQA38WaDJWul"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGrMzP5r88E"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO8dmAdhr-28"
      },
      "source": [
        "if DATASET == 3:\n",
        "    model_type = []\n",
        "    result = []\n",
        "    model_type_values = []\n",
        "    train_headlines = []\n",
        "    test_headlines = []\n",
        "\n",
        "    # Create model type values\n",
        "    for value in range(1,7):\n",
        "        model_type_values.append(value)\n",
        "\n",
        "    for row in range(0, len(train.index)):\n",
        "        train_headlines.append(train.iloc[row, 1])\n",
        "\n",
        "    for row in range(0,len(test.index)):\n",
        "        test_headlines.append(test.iloc[row, 1])\n",
        "\n",
        "\n",
        "    for MODEL_TYPE in model_type_values:\n",
        "\n",
        "        for n in range(1,MODEL_TYPE+1):\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\n",
        "\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\n",
        "\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "    best_model_gram = 0\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    for model in range(len(model_type)):\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\n",
        "\n",
        "        if result[model] > best_model_gram:\n",
        "            best_model_gram = result[model]\n",
        "            best_model_gram_index = model\n",
        "\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCTns-8JsVZp"
      },
      "source": [
        "if DATASET == 3:\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\n",
        "\n",
        "    # show the first\n",
        "    print(train_headlines[0])\n",
        "\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\n",
        "\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\n",
        "\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\n",
        "\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\n",
        "\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\n",
        "\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIGQBKMgsYID"
      },
      "source": [
        "if DATASET == 3:\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\n",
        "\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \n",
        "                            'Coefficient' : _gram_coeffs_best_})\n",
        "\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
        "\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIsCcJzgsa5W"
      },
      "source": [
        "if DATASET == 3:\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 97,
      "outputs": []
    }
  ]
}