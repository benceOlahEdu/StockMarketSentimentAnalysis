{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogReg_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ygDjIPBVTLl"
      },
      "source": [
        "# **Stock market news feed semantic analysis** *(Baseline LogReg)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kLvL-AJVg3C"
      },
      "source": [
        "Ebben a notebookban az eddigi általam kibányászott, megszerzett adathalmazokat fogom a hagyományos bag of words és logistic regression módszerrel megvizsgálni. Ezek után n-gram modelleket is ki fogok próbálni. Az általa használt források és referenciák az eredményekhez:\r\n",
        "\r\n",
        "\r\n",
        "*   https://colab.research.google.com/drive/1QPrBkh-KwX6qcUtiNWKp9rJoneBfGEVh#scrollTo=bQUJwMjYYN4- *(saját munka - átdolgozott)*\r\n",
        "*   https://colab.research.google.com/drive/1MdpXGCj2fb3g1BI_XfF54OWLkYQCZBBy#scrollTo=LndWT2Kn-UMK *(saját baseline munka)*\r\n",
        "*   https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit#Basic-Model-Training-and-Testing\r\n",
        "*   https://www.kaggle.com/lseiyjg/use-news-to-predict-stock-markets\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvCnWXPfXgZi"
      },
      "source": [
        "A használt adathalmazok alapján külön fejezeteket készítek és mindenhol jelzem a forrását és a megszerzésének a módját, ha saját bányászás eredménye."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsAeJYBEXzW_"
      },
      "source": [
        "## **A projekt előkészítése**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuHwIVcIX2Zp"
      },
      "source": [
        "A Drive csatlakoztatása a szükséges fájlok későbbi betöltésére. A betöltés közvetlen a használat előtt fogom megtenni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvU8LOxdKH5-",
        "outputId": "571458c0-c485-454e-b83a-d27a84b51b24"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOuSSouYIb7"
      },
      "source": [
        "A szükséges könyvtárak betöltése a projekthez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgUBe7_VYLjt",
        "outputId": "f5206e95-7528-4065-a8d5-ddcf6e0aa225"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pandas_datareader as web\r\n",
        "from numpy.random import MT19937\r\n",
        "from numpy.random import RandomState, SeedSequence\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize  \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import accuracy_score \r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiFx9dUgBsN"
      },
      "source": [
        "A projektben használt makrók definiálása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bULVBJPegGcc"
      },
      "source": [
        "# Shuffle cycle number for the dataframe\r\n",
        "SHUFFLE_CYCLE = 500\r\n",
        "\r\n",
        "# Which dataset will be used\r\n",
        "DATASET = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfWZE7fZzcF"
      },
      "source": [
        "A reprodukálhatóság miatt definiálok egy seed-et a véletlen szám generátorhoz, amit a továbbiakban használni fogok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEjH22olZ1L_"
      },
      "source": [
        "# Random seed\r\n",
        "RANDOM_SEED = 1234\r\n",
        "\r\n",
        "# Numpy random seed\r\n",
        "NP_SEED = 1234\r\n",
        "\r\n",
        "# Max iteration for training\r\n",
        "MAX_ITER = 100000\r\n",
        "\r\n",
        "# Train size\r\n",
        "TRAIN_SPLIT = 0.80\r\n",
        "\r\n",
        "# Test size\r\n",
        "TEST_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw003siyimeD"
      },
      "source": [
        "np.random.seed(NP_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoE6btiUYg-a"
      },
      "source": [
        "## **KAG_REDDIT_WRLD_DJIA_DF (1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jP3czjYt6c"
      },
      "source": [
        "Ez az adathalmaz a top25 hírt tartalmazza a Reddit World News kategóriából 2008.08.08-2016.07.01 időtartamban. Ez nem általam gyűjtött adathalmaz, a forrása:\r\n",
        "Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved 2021.02.19. from https://www.kaggle.com/aaron7sun/stocknews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_wdhA1ZSrt"
      },
      "source": [
        "Az adathalmaz betöltése a csatlakoztatott Drive-omból."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw00CWOBZbeF"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[0].find(\"Combined_News_DJIA.csv\") != -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_uQPC2Mt6F"
      },
      "source": [
        "Az eredmények elmentésére és indexelésére az alábbi két tömböt fogom hasnzálni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ria16HC2MyZa"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    model_type = [\"Bag of words\", \"1,2 n-gram\", \"2,2 n-gram\", \r\n",
        "                  \"1,3 n-gram\", \"2,3 n-gram\", \"3,3 n-gram\"]\r\n",
        "\r\n",
        "    result = []              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z73YJGnjYxAz"
      },
      "source": [
        "Makró definiálás."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lMyUJerYzAg"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Number of merged news into one string\r\n",
        "    ROWS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1DAhyIob0Bm"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KSUSX1b4-z"
      },
      "source": [
        "Az adathalmaz betöltése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPmTqk3Gb2Fo",
        "outputId": "dc35b795-8f8b-45f9-f6ef-3abe5d393c72"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the dataset \r\n",
        "    df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\r\n",
        "\r\n",
        "    # Show the dataframe\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
            "2008-08-11      1  ...  b\"So this is what it's come to: trading sex fo...\n",
            "2008-08-12      0  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
            "2008-08-13      0  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
            "2008-08-14      1  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
            "\n",
            "[5 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdvD8SIQcPB6"
      },
      "source": [
        "Érdekességképpen a következőkben megvizsgálom, hogy az adathalmaz címkéi megfelelőek. A forrás szerint a címke 1, ha nőtt vagy azonos maradt az érték azon a napon, illetve 0, ha csökkent. (Adj Close adott napi értéke az előző napihoz viszonyítva)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj26KF2ncgji",
        "outputId": "ab865fe4-1243-46b8-a825-b5061f9863cd"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")\r\n",
        "    \r\n",
        "    # Show the stock data\r\n",
        "    print(df_stock.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    High           Low  ...      Volume     Adj Close\n",
            "Date                                    ...                          \n",
            "2008-08-08  11808.490234  11344.230469  ...  4966810000  11734.320312\n",
            "2008-08-11  11933.549805  11580.190430  ...  5067310000  11782.349609\n",
            "2008-08-12  11830.389648  11541.429688  ...  4711290000  11642.469727\n",
            "2008-08-13  11689.049805  11377.370117  ...  4787600000  11532.959961\n",
            "2008-08-14  11744.330078  11399.839844  ...  4064000000  11615.929688\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9UbGo05jACh"
      },
      "source": [
        "Az dátumok formátumát egységesre hozom az összehasonlítás érdekében."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6UkTz6Vg23F",
        "outputId": "4078009a-b0e3-4f01-eca4-67a3f3b3ad98"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    temp_day = []\r\n",
        "\r\n",
        "    for day in range(len(df_stock)):\r\n",
        "        temp_day.append(df_stock.index[day].date())\r\n",
        "\r\n",
        "    df_stock.index = temp_day\r\n",
        "\r\n",
        "    # Show the stock data\r\n",
        "    print(df_stock.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    High           Low  ...      Volume     Adj Close\n",
            "2008-08-08  11808.490234  11344.230469  ...  4966810000  11734.320312\n",
            "2008-08-11  11933.549805  11580.190430  ...  5067310000  11782.349609\n",
            "2008-08-12  11830.389648  11541.429688  ...  4711290000  11642.469727\n",
            "2008-08-13  11689.049805  11377.370117  ...  4787600000  11532.959961\n",
            "2008-08-14  11744.330078  11399.839844  ...  4064000000  11615.929688\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWYe1PpdCK3"
      },
      "source": [
        "Először a dátumok ellenőzöm, hogy megegyeznek-e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77SDqWWGdF31",
        "outputId": "55b423f2-888e-4504-b1e7-e0c936617078"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    difference = []\r\n",
        "\r\n",
        "    if len(df_combined) == len(df_stock):\r\n",
        "        print(\"The lengths are the same!\")\r\n",
        "\r\n",
        "    for day in range(max(len(df_combined), len(df_stock))):\r\n",
        "        if str(df_combined.index[day]) != str(df_stock.index[day]):\r\n",
        "            print(\"There is difference at: \" + str(day) + \" index\")\r\n",
        "            print(\"News: \" + str(df_combined.index[day]) + \"\\tStock: \" + str(df_stock.index[day]))\r\n",
        "            difference.append(day)\r\n",
        "\r\n",
        "    if len(difference) is 0:\r\n",
        "        print(\"The dates matched!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lengths are the same!\n",
            "The dates matched!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJogdlwsjJm3"
      },
      "source": [
        "A labelek ellenőrzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyySRYjNjLwt",
        "outputId": "d6344bcc-af62-43d9-9bfe-d8eb419aca36"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    difference = []\r\n",
        "\r\n",
        "    for day in range(len(df_stock)):\r\n",
        "        # label should be 1 -> rise\r\n",
        "        if int(df_stock[\"Adj Close\"][day]) >= int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "            if df_combined[\"Label\"][day] != 1:\r\n",
        "                difference.append(str(df_stock.index[day]))\r\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        if int(df_stock[\"Adj Close\"][day]) < int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "            if df_combined[\"Label\"][day] != 0:\r\n",
        "                difference.append(str(df_stock.index[day]))\r\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\r\n",
        "\r\n",
        "    print(\"All differences: \" + str(len(difference)))      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Problem at day 2010-10-14\n",
            "Today: 11096.919921875\t\tYesterday: 11096.080078125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-12\n",
            "Today: 12815.080078125\t\tYesterday: 12815.3896484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-15\n",
            "Today: 12570.9501953125\t\tYesterday: 12570.9501953125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2013-04-12\n",
            "Today: 14865.0595703125\t\tYesterday: 14865.1396484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2014-04-24\n",
            "Today: 16501.650390625\t\tYesterday: 16501.650390625\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-08-12\n",
            "Today: 17402.509765625\t\tYesterday: 17402.83984375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-11-27\n",
            "Today: 17813.390625\t\tYesterday: 17813.390625\t\tLabel: 0\n",
            "\n",
            "All differences: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOI1OZJPU6Wh"
      },
      "source": [
        "Látható, hogy rossz a label pár helyen. Egy kis kutakodás után megtaláltam, hogy maga az árfolyam lekérdezésük volt hibás pár nap esetében, ezért ezeket javítom, majd elmentem a drive-omon a javítottat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHvqT2FRtBkW",
        "outputId": "9d134cbc-88b0-4a65-ce6e-712ceb1b4269"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # correct the wrong labels\r\n",
        "    for row in difference:\r\n",
        "        if df_combined.loc[row, \"Label\"] == 0:\r\n",
        "            df_combined.loc[row, \"Label\"] = 1\r\n",
        "        else:\r\n",
        "            df_combined.loc[row, \"Label\"] = 0\r\n",
        "\r\n",
        "    # check them\r\n",
        "    for row in difference:\r\n",
        "        print(str(row) + \"\\t\\t\" + str(df_combined.loc[row, \"Label\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2010-10-14\t\t1\n",
            "2012-11-12\t\t1\n",
            "2012-11-15\t\t1\n",
            "2013-04-12\t\t1\n",
            "2014-04-24\t\t1\n",
            "2015-08-12\t\t1\n",
            "2015-11-27\t\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTIqNqucuiBO",
        "outputId": "ac054fbd-8633-44ab-ba73-4ff3ba0155ad"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # save to drive\r\n",
        "    df_combined.to_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv')\r\n",
        "\r\n",
        "    # Show the dataset\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
            "2008-08-11      1  ...  b\"So this is what it's come to: trading sex fo...\n",
            "2008-08-12      0  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
            "2008-08-13      0  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
            "2008-08-14      1  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
            "\n",
            "[5 rows x 26 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhg7d5Xj2kt"
      },
      "source": [
        "A következőkben az esetleges adat nélküli napokat, illetve cellákat keresem meg és helyettesítem őket egy üres sztringgel. Ez a későbbi szövegfeldolgozás hibamentességéhez szükséges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrFufwo6j5_H"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Find the cells with NaN and after the rows for them\r\n",
        "    is_NaN = df_combined.isnull()\r\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "    # Replace them\r\n",
        "    df_combined = df_combined.replace(np.nan, \" \")\r\n",
        "\r\n",
        "    # Check the process\r\n",
        "    is_NaN = df_combined.isnull()\r\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "    assert len(rows_with_NaN) is 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxCOud0ki5M-"
      },
      "source": [
        "Ezek után az egy naphoz tartozó híreket közös sztringekbe fűzöm. Az egy sztringbe tartozó hírek számát makróval definiálom:\r\n",
        "\r\n",
        "\r\n",
        "*   ROWS - egymásba fűzött hírek száma\r\n",
        "\r\n",
        "Itt megtalálható már az első előkészítő algoritmusom, méghozzá a sztringek elején található b karakter eltávolítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqv3bDcAi6Bf",
        "outputId": "9924f9a7-1c44-4a6c-f936-1b66199413cb"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Get column names\r\n",
        "    combined_column_names = []\r\n",
        "    for column in df_combined.columns:\r\n",
        "      combined_column_names.append(column)\r\n",
        "\r\n",
        "    # 2D array creation for the news based on macros\r\n",
        "    COLUMNS = len(df_combined)\r\n",
        "    news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \r\n",
        "\r\n",
        "    # Show the column names\r\n",
        "    print(\"Column names of the dataset:\") \r\n",
        "    print(combined_column_names)\r\n",
        "\r\n",
        "    # Merge the news\r\n",
        "    for row in range(len(df_combined)):\r\n",
        "      for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "        temp = \"\"\r\n",
        "        news = \"\"\r\n",
        "        for word in range(ROWS):\r\n",
        "          news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\r\n",
        "          # Remove the b character at the begining of the string\r\n",
        "          if news[0] is \"b\":\r\n",
        "            news = \" \" + news[1:]\r\n",
        "          temp = temp + \" \" + news\r\n",
        "        news_sum[column][row] = temp\r\n",
        "\r\n",
        "    # Show the first day second package of the news\r\n",
        "    print(\"\\nThe first day second package of the news:\")\r\n",
        "    print(news_sum[1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column names of the dataset:\n",
            "['Label', 'Top1', 'Top2', 'Top3', 'Top4', 'Top5', 'Top6', 'Top7', 'Top8', 'Top9', 'Top10', 'Top11', 'Top12', 'Top13', 'Top14', 'Top15', 'Top16', 'Top17', 'Top18', 'Top19', 'Top20', 'Top21', 'Top22', 'Top23', 'Top24', 'Top25']\n",
            "\n",
            "The first day second package of the news:\n",
            "  'Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)'  'Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvOqG6vckIcF"
      },
      "source": [
        "Ezek után a korábbi oszlopokat(Top1, Top2...) kicserélem a csoportosításnak megfelelő számú oszlopokra és nevekre (News_1, News_2...), majd feltöltöm őket az összevont hírcsomagokkal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xljXPUwmkKrZ",
        "outputId": "c415dde5-c32e-4f77-dc00-35dcd5ff2bd3"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Drop the old columns\r\n",
        "    for column in range(len(combined_column_names) - 1):\r\n",
        "      df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\r\n",
        "\r\n",
        "    # Create the new columns with the merged news\r\n",
        "    for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "      colum_name = \"News_\" + str(column + 1)\r\n",
        "      df_combined[colum_name] = news_sum[column]\r\n",
        "\r\n",
        "    # Show the DataFrame\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Label  ...                                            News_12\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...    'Indian shoe manufactory  - And again in a s...\n",
            "2008-08-11      1  ...    'Perhaps *the* question about the Georgia - ...\n",
            "2008-08-12      0  ...    'Christopher King argues that the US and NAT...\n",
            "2008-08-13      0  ...    ' Quarter of Russians blame U.S. for conflic...\n",
            "2008-08-14      1  ...    'Russia: World  \"can forget about\" Georgia\\'...\n",
            "\n",
            "[5 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gmqmlpkqhm"
      },
      "source": [
        "Egy új dataframebe újracsoportosítom a hír blokkokat a címkéjükkel, már a dátumok nélkül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoeoCzPekrN2",
        "outputId": "a334b2cb-6e39-4b6f-91b5-79ad64819a81"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # The label column \r\n",
        "    LABEL_COLUMN = 0\r\n",
        "\r\n",
        "    news_sum = []\r\n",
        "    label_sum = []\r\n",
        "\r\n",
        "    # Get the column names\r\n",
        "    combined_column_names = []\r\n",
        "    for column in df_combined.columns:\r\n",
        "      combined_column_names.append(column)\r\n",
        "\r\n",
        "    # Write out the column names \r\n",
        "    print(combined_column_names)\r\n",
        "    print(\"\\n\")\r\n",
        "\r\n",
        "    # Connect the merged news with the labels\r\n",
        "    for column in range(len(df_combined)):\r\n",
        "      for row in range(len(combined_column_names) - 1):\r\n",
        "        news_sum.append(df_combined[combined_column_names[row + 1]][column])\r\n",
        "        label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\r\n",
        "\r\n",
        "    # Create the new DataFrame\r\n",
        "    df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\r\n",
        "    df_sum_news_labels[\"News\"] = news_sum\r\n",
        "\r\n",
        "    # Show it\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Label', 'News_1', 'News_2', 'News_3', 'News_4', 'News_5', 'News_6', 'News_7', 'News_8', 'News_9', 'News_10', 'News_11', 'News_12']\n",
            "\n",
            "\n",
            "   Label                                               News\n",
            "0      0    \"Georgia 'downs two Russian warplanes' as co...\n",
            "1      0    'Russia Today: Columns of troops roll into S...\n",
            "2      0    \"Afghan children raped with 'impunity,' U.N....\n",
            "3      0    \"Breaking: Georgia invades South Ossetia, Ru...\n",
            "4      0    'Georgian troops retreat from S. Osettain ca...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQSgwmslqkO"
      },
      "source": [
        "Először a szövegek előfeldolgozásával kezdem: írásjelek eltávolítása, számok eltávolítása, felesleges szóközök eltávolítása, aztán minden szót kis kezdőbetűjü szóvá konvertálom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJT565FZlsgZ",
        "outputId": "ca5d403f-ecab-4298-a669-5da43ab90345"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if word not in string.punctuation:\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_sum_news_labels[\"News\"] = temp_news\r\n",
        "\r\n",
        "    # Show it\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      0  georgia downs two russian warplanes as countri...\n",
            "1      0  russia today columns of troops roll into south...\n",
            "2      0  afghan children raped with impunity u n offici...\n",
            "3      0  breaking georgia invades south ossetia russia ...\n",
            "4      0  georgian troops retreat from s osettain capita...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCvCsvel989"
      },
      "source": [
        "A következőkben az úgy nevezett töltelék szavakat (stop words) fogom eltávolítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3WjnHkAmA4e",
        "outputId": "463b96e5-160e-414e-e463-87ade4578b77"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_sum_news_labels[\"News\"] = filtered_sentence\r\n",
        "\r\n",
        "    # Show the DataFrame\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Label                                               News\n",
            "0      0  georgia downs two russian warplanes countries ...\n",
            "1      0  russia today columns troops roll south ossetia...\n",
            "2      0  afghan children raped impunity u n official sa...\n",
            "3      0  breaking georgia invades south ossetia russia ...\n",
            "4      0  georgian troops retreat osettain capital presu...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DHwHi3mbDn"
      },
      "source": [
        "Az adathalmazban lévő nulla hosszú sztring csomagok megkeresése és a hozzájuk tartozó cellák törlése következik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUqfwAe1mbqM",
        "outputId": "d13590ec-a84e-43c1-8f84-8b2b368e6736"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "\r\n",
        "    print(null_indexes)\r\n",
        "\r\n",
        "    for row in null_indexes:\r\n",
        "      df_sum_news_labels = df_sum_news_labels.drop(row)\r\n",
        "\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "      \r\n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3335]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UW_rMpOmlSH"
      },
      "source": [
        "Az adathalmaz véletlenszerű sorbarendezése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdW_tECmlxs"
      },
      "source": [
        "#if DATASET == 1: ## commented out for thinking\r\n",
        "    # Do the shuffle\r\n",
        "    #for i in range(SHUFFLE_CYCLE):\r\n",
        "    #  df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    #df_sum_news_labels.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    #print(df_sum_news_labels.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9cUBq_WnJbH"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és validáló/tesztelő adathalmazokra, majd a szétbontás ellenőrzése mérettel és első elem kiíratásával."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Mg2z7LnKH1",
        "outputId": "e79370ce-3a57-41b2-de77-b7c0322e37e3"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    train = df_sum_news_labels[:TRAIN_SIZE] \r\n",
        "    test = df_sum_news_labels[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_sum_news_labels)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data set length: 19093\n",
            "Test data set length: 4774\n",
            "Split summa: 23867\n",
            "Dataset summa before split: 23867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s03PFsFQsgpS",
        "outputId": "43fb545b-222b-4c46-d9f5-311c333c8d6b"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(train.tail(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Label                                               News\n",
            "19093      1  ukraine pm discloses accident nuclear plant re...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au3VLLVzsh0U",
        "outputId": "1a2de8ce-a029-4232-f506-9f847fe237d8"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(test.head(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Label                                               News\n",
            "19094      1  dna confirms king richard iii remains parking ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgA6S5muPbE"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsDa53wcxRdG"
      },
      "source": [
        "Először a tanító adathalmaz híreit fűzöm össze egy tömbbe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RhOIEt3wmHn",
        "outputId": "07469afe-29cf-4c2c-b2b5-1dfbce0cad3a"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    train_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83MuhLlOxYK5"
      },
      "source": [
        "Ezek után vektorizálom őket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N3hIvbQxgr6",
        "outputId": "47ffe0d0-9b6e-45b2-f7fe-f4f76d20e645"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_vectorizer = CountVectorizer()\r\n",
        "    bow_train = bow_vectorizer.fit_transform(train_headlines)\r\n",
        "    print(bow_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 29620)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExOpaN_Ix3r_"
      },
      "source": [
        "Egy logistic regression modellt fogok erre a tanító halmazra betanítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D36NSmICyOCC"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    bow_model = bow_model.fit(bow_train, train[\"Label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EYr5_yygsr"
      },
      "source": [
        "A teszt adathalmaz előkészítése, majd becslés a modell segítségével a következő lépés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkxsor9uygSa"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    bow_test = bow_vectorizer.transform(test_headlines)\r\n",
        "    bow_predictions = bow_model.predict(bow_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFBYXRil2nS2"
      },
      "source": [
        "Az eredmények megjelenítése egy táblázatban."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfgw4P9-2rkL"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    pd.crosstab(test[\"Label\"], bow_predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9uWaAGm3E_J"
      },
      "source": [
        "A pontossága a modellnek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxtBIe23HqK",
        "outputId": "0932a21c-e40d-432f-f6b2-9da671331f0a"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print (classification_report(test[\"Label\"], bow_predictions))\r\n",
        "    print (accuracy_score(test[\"Label\"], bow_predictions))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], bow_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.42      0.45      2328\n",
            "           1       0.51      0.58      0.54      2446\n",
            "\n",
            "    accuracy                           0.50      4774\n",
            "   macro avg       0.50      0.50      0.50      4774\n",
            "weighted avg       0.50      0.50      0.50      4774\n",
            "\n",
            "0.5006284038542103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQZQqPar4Im8"
      },
      "source": [
        "A következőkben a top 10 legbefolyásolóbb sztringet jelenítem meg mind pozítiv és mind negatív irányba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oveqbdwt4Q-9",
        "outputId": "cf979b19-209c-4d1e-bc4b-084e9acefa98"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_words = bow_vectorizer.get_feature_names()\r\n",
        "    bow_coeffs = bow_model.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : bow_words, \r\n",
        "                            'Coefficient' : bow_coeffs})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              Word  Coefficient\n",
            "14588      landing     1.768526\n",
            "7682       donetsk     1.460838\n",
            "19162     pathetic     1.420720\n",
            "23230        scrap     1.417642\n",
            "6636     defending     1.408051\n",
            "18381  oktoberfest     1.402782\n",
            "9880      floating     1.393611\n",
            "113         abroad     1.366030\n",
            "26320        tents     1.364711\n",
            "28907      whistle     1.350252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKPketqV4uVw",
        "outputId": "e1dcf7b7-108b-4e31-82a3-f4442d24a225"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Word  Coefficient\n",
            "11114    greenpeace    -1.321665\n",
            "19606        picked    -1.324817\n",
            "2295        battery    -1.369384\n",
            "14804       lecture    -1.454539\n",
            "26850      toulouse    -1.457961\n",
            "24798       spilled    -1.467365\n",
            "25277      stranded    -1.470640\n",
            "12232         horns    -1.491533\n",
            "12015         hints    -1.498605\n",
            "25666  supermarkets    -1.502472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytoEWvZ_48_x"
      },
      "source": [
        "### 2-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_11F44gu5TdJ"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1reDgLAU5jQd",
        "outputId": "6643dbc5-2074-4836-f32b-410d32da7c9d"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_12 = CountVectorizer(ngram_range=(1,2))\r\n",
        "    train_vectorizer_12 = gram_vectorizer_12.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_12.shape)\r\n",
        "\r\n",
        "    gram_model_12 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_12 = gram_model_12.fit(train_vectorizer_12, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_12 = gram_vectorizer_12.transform(test_headlines)\r\n",
        "    gram_predictions_12 = gram_model_12.predict(gram_test_12)\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_12))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_12))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_12))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 356120)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.39      0.43      2328\n",
            "           1       0.51      0.61      0.56      2446\n",
            "\n",
            "    accuracy                           0.50      4774\n",
            "   macro avg       0.50      0.50      0.50      4774\n",
            "weighted avg       0.50      0.50      0.50      4774\n",
            "\n",
            "0.5031420192710515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_JxbXWM7wss",
        "outputId": "d143ed17-8733-47dd-a4df-cb4c044a6c01"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_12 = gram_vectorizer_12.get_feature_names()\r\n",
        "    gram_coeffs_12 = gram_model_12.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_12, \r\n",
        "                            'Coefficient' : gram_coeffs_12})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Word  Coefficient\n",
            "173957       landing     1.134792\n",
            "91032        donetsk     0.856744\n",
            "117803      floating     0.849619\n",
            "280235         seize     0.839355\n",
            "277685         scrap     0.831859\n",
            "106031      executes     0.827051\n",
            "291374  social media     0.819436\n",
            "228586    peace deal     0.808734\n",
            "81988      defending     0.807512\n",
            "327637       ugandan     0.798901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g96l9WK57xMH",
        "outputId": "4a5b4aa2-7693-4bbd-f170-8953be665472"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Word  Coefficient\n",
            "135185       greenpeace    -0.795740\n",
            "213443          nothing    -0.801032\n",
            "125852            games    -0.811726\n",
            "173021           kuwait    -0.821276\n",
            "98823           emerges    -0.838328\n",
            "50921            census    -0.857432\n",
            "301037         stranded    -0.864019\n",
            "143389            hints    -0.864715\n",
            "261189  reported killed    -0.911985\n",
            "292426          somalia    -0.975792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFrdv3Um9CoD"
      },
      "source": [
        "Másodjára a (2,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IdZdzJl9Ftm",
        "outputId": "4be3911b-d5a6-44ef-aaaf-a418059cee03"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_22 = CountVectorizer(ngram_range=(2,2))\r\n",
        "    train_vectorizer_22 = gram_vectorizer_22.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_22.shape)\r\n",
        "\r\n",
        "    gram_model_22 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_22 = gram_model_22.fit(train_vectorizer_22, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_22 = gram_vectorizer_22.transform(test_headlines)\r\n",
        "    gram_predictions_22 = gram_model_22.predict(gram_test_22)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_22, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_22))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_22))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_22))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 326500)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.28      0.36      2328\n",
            "           1       0.52      0.75      0.61      2446\n",
            "\n",
            "    accuracy                           0.52      4774\n",
            "   macro avg       0.52      0.51      0.49      4774\n",
            "weighted avg       0.52      0.52      0.49      4774\n",
            "\n",
            "0.5190615835777126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snpLViV79GlO",
        "outputId": "531d67fb-bbca-4fbc-e101-2032704559bc"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_22 = gram_vectorizer_22.get_feature_names()\r\n",
        "    gram_coeffs_22 = gram_model_22.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_22, \r\n",
        "                            'Coefficient' : gram_coeffs_22})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         Word  Coefficient\n",
            "266903           social media     0.893075\n",
            "142827  intelligence agencies     0.865598\n",
            "209344             peace deal     0.826616\n",
            "190274              nato says     0.791314\n",
            "310685         volcano erupts     0.774734\n",
            "255871       security council     0.767691\n",
            "259989        sexual violence     0.758669\n",
            "52133          church england     0.726185\n",
            "196245            nuclear war     0.713862\n",
            "113255       french president     0.706731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiSp21nl9Gai",
        "outputId": "e457cd45-b902-494f-9a37-ea3d5996ce3d"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        Word  Coefficient\n",
            "221749     presidential race    -0.737167\n",
            "52041   christopher hitchens    -0.743371\n",
            "190145              nato air    -0.747195\n",
            "4925            afghan woman    -0.750351\n",
            "126760      haiti earthquake    -0.759674\n",
            "112224           france says    -0.765828\n",
            "265292              sky news    -0.767848\n",
            "312600              war iran    -0.796021\n",
            "304810               us army    -0.810270\n",
            "239243       reported killed    -0.959290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUxZOqjo5Eg7"
      },
      "source": [
        "### 3-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX7vV_715ckj"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRNHw3tAMSjp",
        "outputId": "7814de86-fed7-45ec-d58a-a19431268c9d"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_13 = CountVectorizer(ngram_range=(1,3))\r\n",
        "    train_vectorizer_13 = gram_vectorizer_13.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_13.shape)\r\n",
        "\r\n",
        "    gram_model_13 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_13 = gram_model_13.fit(train_vectorizer_13, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_13 = gram_vectorizer_13.transform(test_headlines)\r\n",
        "    gram_predictions_13 = gram_model_13.predict(gram_test_13)\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_13))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_13))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_13))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 746775)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.36      0.41      2328\n",
            "           1       0.51      0.64      0.57      2446\n",
            "\n",
            "    accuracy                           0.50      4774\n",
            "   macro avg       0.50      0.50      0.49      4774\n",
            "weighted avg       0.50      0.50      0.49      4774\n",
            "\n",
            "0.5023041474654378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG-xubCYMiFF",
        "outputId": "fad1b60f-1dfc-44dd-8057-f6a96455dafd"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_13 = gram_vectorizer_13.get_feature_names()\r\n",
        "    gram_coeffs_13 = gram_model_13.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_13, \r\n",
        "                            'Coefficient' : gram_coeffs_13})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Word  Coefficient\n",
            "362451       landing     0.877524\n",
            "586404         seize     0.730224\n",
            "442380       nigeria     0.714239\n",
            "426163        mumbai     0.699297\n",
            "218992      executes     0.648102\n",
            "332496         irish     0.646494\n",
            "684959       ugandan     0.636752\n",
            "187693       donetsk     0.635941\n",
            "243190      floating     0.626092\n",
            "609190  social media     0.625800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DequuzvFMmH3",
        "outputId": "6c6e37ca-b233-43aa-8223-18e2e98852c3"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Word  Coefficient\n",
            "204188          emerges    -0.646007\n",
            "110373          chevron    -0.647476\n",
            "545973  reported killed    -0.649998\n",
            "49914         authority    -0.651500\n",
            "105274           census    -0.665821\n",
            "260333            games    -0.699765\n",
            "616612           speech    -0.705862\n",
            "64422           beijing    -0.725339\n",
            "446152          nothing    -0.731301\n",
            "611468          somalia    -0.922126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHmKu3djMS14"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (2,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etgz8IAjMUE3",
        "outputId": "255409fb-8b1f-4307-f6ba-acefe32ad557"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_23 = CountVectorizer(ngram_range=(2,3))\r\n",
        "    train_vectorizer_23 = gram_vectorizer_23.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_23.shape)\r\n",
        "\r\n",
        "    gram_model_23 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_23 = gram_model_23.fit(train_vectorizer_23, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_23 = gram_vectorizer_23.transform(test_headlines)\r\n",
        "    gram_predictions_23 = gram_model_23.predict(gram_test_23)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_23, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_23))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_23))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_23))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 717155)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.21      0.30      2328\n",
            "           1       0.52      0.82      0.64      2446\n",
            "\n",
            "    accuracy                           0.52      4774\n",
            "   macro avg       0.52      0.51      0.47      4774\n",
            "weighted avg       0.52      0.52      0.47      4774\n",
            "\n",
            "0.5215751989945538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVN0l54FMi1-",
        "outputId": "846868ac-d6f3-47fc-fd75-734fb9679a9a"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_23 = gram_vectorizer_23.get_feature_names()\r\n",
        "    gram_coeffs_23 = gram_model_23.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_23, \r\n",
        "                            'Coefficient' : gram_coeffs_23})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         Word  Coefficient\n",
            "584719           social media     0.706446\n",
            "560870       security council     0.601769\n",
            "309844  intelligence agencies     0.601736\n",
            "459110             peace deal     0.593660\n",
            "245395       french president     0.581806\n",
            "681250         volcano erupts     0.571477\n",
            "635910            three years     0.551867\n",
            "705949          world largest     0.548354\n",
            "570052        sexual violence     0.545781\n",
            "415352              nato says     0.544080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIO9FkYuMnNb",
        "outputId": "b98cb2a4-df64-493a-884e-6d183ff30d1d"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        Word  Coefficient\n",
            "113177  christopher hitchens    -0.543275\n",
            "243024           france says    -0.547297\n",
            "515733             red cross    -0.554964\n",
            "275459      haiti earthquake    -0.556198\n",
            "667990               us army    -0.590623\n",
            "37248           around world    -0.594445\n",
            "685579              war iran    -0.595173\n",
            "422882    news international    -0.599777\n",
            "589164          south korean    -0.660394\n",
            "524027       reported killed    -0.666164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftajhXMMUUs"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (3,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T-9458xMaJq",
        "outputId": "e310844d-5bc9-4bfc-f8b9-f5511704b292"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_33 = CountVectorizer(ngram_range=(3,3))\r\n",
        "    train_vectorizer_33 = gram_vectorizer_33.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_33.shape)\r\n",
        "\r\n",
        "    gram_model_33 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_33 = gram_model_33.fit(train_vectorizer_33, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_33 = gram_vectorizer_33.transform(test_headlines)\r\n",
        "    gram_predictions_33 = gram_model_33.predict(gram_test_33)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_33, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_33))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_33))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_33))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19093, 390655)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.04      0.07      2328\n",
            "           1       0.51      0.97      0.67      2446\n",
            "\n",
            "    accuracy                           0.51      4774\n",
            "   macro avg       0.52      0.50      0.37      4774\n",
            "weighted avg       0.52      0.51      0.38      4774\n",
            "\n",
            "0.5140343527440302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7_f6ay_MjhC",
        "outputId": "c45297bb-8cd6-45ea-fd09-25762898a8a2"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_33 = gram_vectorizer_33.get_feature_names()\r\n",
        "    gram_coeffs_33 = gram_model_33.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_33, \r\n",
        "                            'Coefficient' : gram_coeffs_33})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         Word  Coefficient\n",
            "124147       first time since     0.731322\n",
            "240385   open fire protesters     0.628014\n",
            "359534    un security council     0.609239\n",
            "231117      nobel peace prize     0.589133\n",
            "355093  turkey prime minister     0.571646\n",
            "264436  president evo morales     0.506588\n",
            "387591           year old man     0.500524\n",
            "10466      alleged war crimes     0.491144\n",
            "9434       al jazeera english     0.473660\n",
            "156032        homes west bank     0.468706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNFCAZ2pMpCY",
        "outputId": "6e8b2d1f-5ae0-4590-bdc2-ac7633edc4c1"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          Word  Coefficient\n",
            "128829     former mossad chief    -0.477614\n",
            "116128        faces years jail    -0.490935\n",
            "290538          rio de janeiro    -0.532372\n",
            "224907         nato air strike    -0.544116\n",
            "155938    homes east jerusalem    -0.558256\n",
            "137572     german pirate party    -0.561264\n",
            "123599     first country world    -0.583245\n",
            "336732  syrian security forces    -0.599908\n",
            "253432   phone hacking scandal    -0.602573\n",
            "216614  missile defense system    -0.690368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpZIe-RTmk_"
      },
      "source": [
        "### 4-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y75LfFJBTtrz"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PabhGwFWUZrZ",
        "outputId": "86bce079-7180-4e71-e7df-2c1947c9325b"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    for n in range(1,5):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",4 gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,4))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",4 n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (19093, 1128993)\n",
            "\n",
            "0.5018852115626309\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (19093, 1099373)\n",
            "\n",
            "0.5232509426057813\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (19093, 772873)\n",
            "\n",
            "0.5136154168412232\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (19093, 382218)\n",
            "\n",
            "0.5131964809384164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkP2pSAV6Qm"
      },
      "source": [
        "### 5-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Y4HliaV-rg"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8hdr8A-WBRq",
        "outputId": "aed8c60b-a67b-43fc-eea2-69c33812abb1"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    MODEL_TYPE = 5\r\n",
        "\r\n",
        "    for n in range(1,MODEL_TYPE+1):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (19093, 1493904)\n",
            "\n",
            "0.5035609551738583\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (19093, 1464284)\n",
            "\n",
            "0.5219941348973607\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (19093, 1137784)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (19093, 747129)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (19093, 364911)\n",
            "\n",
            "0.5121491411813992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgRyRExGWvFN"
      },
      "source": [
        "### 6-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv2plcaDWy8V"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-9kyXKFW1KI",
        "outputId": "d10364db-8d30-4ba8-a3af-c19c0ffcbf9c"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    MODEL_TYPE = 6\r\n",
        "\r\n",
        "    for n in range(1,MODEL_TYPE+1):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (19093, 1840189)\n",
            "\n",
            "0.5033514872224549\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (19093, 1810569)\n",
            "\n",
            "0.5215751989945538\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (19093, 1484069)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (19093, 1093414)\n",
            "\n",
            "0.5127775450356096\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (19093, 711196)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (19093, 346285)\n",
            "\n",
            "0.5123586091328027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh7BcurhRaGe"
      },
      "source": [
        "### Eredmények összegzése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr2NAOefRc1j"
      },
      "source": [
        "Az eredmények kiíratása, a legjobbat kiemelve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykPdalGBRgeL",
        "outputId": "cb7488b3-c5e0-45b7-ae6a-880d41106d4e"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    best_model = 0\r\n",
        "\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model:\r\n",
        "            best_model = result[model]\r\n",
        "            best_model_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of words:\t\t\t\t\t0.5006284038542103\n",
            "1,2 n-gram:\t\t\t\t\t0.5031420192710515\n",
            "2,2 n-gram:\t\t\t\t\t0.5190615835777126\n",
            "1,3 n-gram:\t\t\t\t\t0.5023041474654378\n",
            "2,3 n-gram:\t\t\t\t\t0.5215751989945538\n",
            "3,3 n-gram:\t\t\t\t\t0.5140343527440302\n",
            "1,4 n-gram:\t\t\t\t\t0.5018852115626309\n",
            "2,4 n-gram:\t\t\t\t\t0.5232509426057813\n",
            "3,4 n-gram:\t\t\t\t\t0.5136154168412232\n",
            "4,4 n-gram:\t\t\t\t\t0.5131964809384164\n",
            "1,5 n-gram:\t\t\t\t\t0.5035609551738583\n",
            "2,5 n-gram:\t\t\t\t\t0.5219941348973607\n",
            "3,5 n-gram:\t\t\t\t\t0.512987012987013\n",
            "4,5 n-gram:\t\t\t\t\t0.512987012987013\n",
            "5,5 n-gram:\t\t\t\t\t0.5121491411813992\n",
            "1,6 n-gram:\t\t\t\t\t0.5033514872224549\n",
            "2,6 n-gram:\t\t\t\t\t0.5215751989945538\n",
            "3,6 n-gram:\t\t\t\t\t0.512987012987013\n",
            "4,6 n-gram:\t\t\t\t\t0.5127775450356096\n",
            "5,6 n-gram:\t\t\t\t\t0.5123586091328027\n",
            "6,6 n-gram:\t\t\t\t\t0.5123586091328027\n",
            "--------------------------------------------\n",
            "Best model:\n",
            "2,4 n-gram\t\t\t\t\t0.5232509426057813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnuJNnfVYu04"
      },
      "source": [
        "### ROWS makró optimalizálás"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3gtt_TY80k"
      },
      "source": [
        "Ebben a fejezetben a különböző ROWS értékekre (mennyi napi hírt fűzünk egybe) futtatom végig egy automatizált bag of words -> 6,6 gram modell tanítást és becslést és állapítom meg, hogy melyik a legpontosabb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUJCC_EiZo63"
      },
      "source": [
        "A tesztelendő paraméterek megadása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KUt-wbZZO1i",
        "outputId": "6c2d95be-226e-4ac6-ce8c-66d816e61e1f"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Number of merged news into one string: 1...12, 25 \r\n",
        "    rows_values = []\r\n",
        "    for value in range(1,13):\r\n",
        "        rows_values.append(value)\r\n",
        "\r\n",
        "    rows_values.append(25)\r\n",
        "\r\n",
        "    print(rows_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIHq8mVcNvZ"
      },
      "source": [
        "A modell típusok összegyűjtése az automatizált tanításhoz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U4xwuVXcRsp",
        "outputId": "9bf30b91-6d6d-44bd-8368-8f2bd1838bd9"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    model_type_values = []\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    print(model_type_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9UswHzer9O"
      },
      "source": [
        "A paraméterhez tartozó eredmények tárolására létrehozom az alábbi tömböket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRcPzJ-1exEu"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    rows_summary_value = []\r\n",
        "    rows_summary_accuraccy = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_5e2otZsMM"
      },
      "source": [
        "Automatizált tanítás és mentések."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJXKkcqGPBZ"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def preprocess():\r\n",
        "        df_combined = pd.read_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv', \r\n",
        "                                index_col = \"Date\")\r\n",
        "\r\n",
        "        # Find the cells with NaN and after the rows for them\r\n",
        "        is_NaN = df_combined.isnull()\r\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "        # Replace them\r\n",
        "        df_combined = df_combined.replace(np.nan, \" \")\r\n",
        "\r\n",
        "        # Check the process\r\n",
        "        is_NaN = df_combined.isnull()\r\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "        assert len(rows_with_NaN) is 0\r\n",
        "\r\n",
        "        # Get column names\r\n",
        "        combined_column_names = []\r\n",
        "        for column in df_combined.columns:\r\n",
        "          combined_column_names.append(column)\r\n",
        "\r\n",
        "        # 2D array creation for the news based on macros\r\n",
        "        COLUMNS = len(df_combined)\r\n",
        "        news_sum = []\r\n",
        "        news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \r\n",
        "\r\n",
        "        # Merge the news\r\n",
        "        for row in range(len(df_combined)):\r\n",
        "          for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "            temp = \"\"\r\n",
        "            news = \"\"\r\n",
        "            for word in range(ROWS):\r\n",
        "              news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\r\n",
        "              # Remove the b character at the begining of the string\r\n",
        "              if news[0] is \"b\":\r\n",
        "                news = \" \" + news[1:]\r\n",
        "              temp = temp + \" \" + news\r\n",
        "            news_sum[column][row] = temp\r\n",
        "\r\n",
        "        # Drop the old columns\r\n",
        "        for column in range(len(combined_column_names) - 1):\r\n",
        "          df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\r\n",
        "\r\n",
        "        # Create the new columns with the merged news\r\n",
        "        for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "          colum_name = \"News_\" + str(column + 1)\r\n",
        "          df_combined[colum_name] = news_sum[column]          \r\n",
        "\r\n",
        "        # The label column \r\n",
        "        LABEL_COLUMN = 0\r\n",
        "\r\n",
        "        news_sum = []\r\n",
        "        label_sum = []\r\n",
        "\r\n",
        "        # Get the column names\r\n",
        "        combined_column_names = []\r\n",
        "        for column in df_combined.columns:\r\n",
        "          combined_column_names.append(column)\r\n",
        "\r\n",
        "        # Connect the merged news with the labels\r\n",
        "        for column in range(len(df_combined)):\r\n",
        "          for row in range(len(combined_column_names) - 1):\r\n",
        "            news_sum.append(df_combined[combined_column_names[row + 1]][column])\r\n",
        "            label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\r\n",
        "\r\n",
        "        # Create the new DataFrame\r\n",
        "        df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\r\n",
        "        df_sum_news_labels[\"News\"] = news_sum\r\n",
        "\r\n",
        "        # Removing punctuations\r\n",
        "        temp_news = []\r\n",
        "        for line in news_sum:\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if word not in string.punctuation:\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          temp_news.append(temp_attach)\r\n",
        "\r\n",
        "        news_sum = temp_news\r\n",
        "        temp_news = []\r\n",
        "\r\n",
        "        # Remove numbers\r\n",
        "        for line in news_sum:\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if not word.isdigit():\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          temp_news.append(temp_attach)\r\n",
        "\r\n",
        "        # Remove space\r\n",
        "        for line in range(len(temp_news)):    \r\n",
        "          temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "        # Converting headlines to lower case\r\n",
        "        for line in range(len(temp_news)): \r\n",
        "            temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "        # Update the data frame\r\n",
        "        df_sum_news_labels[\"News\"] = temp_news\r\n",
        "\r\n",
        "        # Load the stop words\r\n",
        "        stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "        filtered_sentence = []\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "\r\n",
        "        # Remove stop words\r\n",
        "        for line in news_sum:\r\n",
        "          word_tokens = word_tokenize(line)\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in word_tokens:\r\n",
        "            temp = \" \"\r\n",
        "            if not word in stop_words:\r\n",
        "              temp = temp + word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "        # Remove space\r\n",
        "        for line in range(len(filtered_sentence)):    \r\n",
        "          filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "        # Update the data frame\r\n",
        "        df_sum_news_labels[\"News\"] = filtered_sentence\r\n",
        "\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "        null_indexes = []\r\n",
        "        index = 0\r\n",
        "\r\n",
        "        for line in news_sum:\r\n",
        "          if line is \"\":\r\n",
        "            null_indexes.append(index)\r\n",
        "          index = index + 1\r\n",
        "\r\n",
        "        for row in null_indexes:\r\n",
        "          df_sum_news_labels = df_sum_news_labels.drop(row)\r\n",
        "\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "        null_indexes = []\r\n",
        "        index = 0\r\n",
        "\r\n",
        "        for line in news_sum:\r\n",
        "          if line is \"\":\r\n",
        "            null_indexes.append(index)\r\n",
        "          index = index + 1\r\n",
        "          \r\n",
        "        assert len(null_indexes) is 0\r\n",
        "\r\n",
        "        # Do the shuffle #comment out for test\r\n",
        "        #for i in range(SHUFFLE_CYCLE):\r\n",
        "        #  df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "        # Reset the index\r\n",
        "        #df_sum_news_labels.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "        return df_sum_news_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zgMLtiLx6e"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def split_to_train():\r\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "\r\n",
        "        # Split the dataset\r\n",
        "        train = df_sum_news_labels[:TRAIN_SIZE] \r\n",
        "\r\n",
        "        return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1lgt7dMBol"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def split_to_test():\r\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "\r\n",
        "        # Split the dataset\r\n",
        "        test = df_sum_news_labels[TRAIN_SIZE:]\r\n",
        "\r\n",
        "        return test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq0Oo4iHZufG",
        "outputId": "c31754eb-c7d2-4722-e114-ac35ba539e2b"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    for ROWS in rows_values:\r\n",
        "      \r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the ROWS = \" \r\n",
        "          + str(ROWS) + \" sequence\\n\\n--------------------------------------------\\n\")\r\n",
        "        \r\n",
        "        model_type = []\r\n",
        "        result = []\r\n",
        "\r\n",
        "        df_sum_news_labels = preprocess()\r\n",
        "        train = split_to_train()\r\n",
        "        test = split_to_test()\r\n",
        "\r\n",
        "        # check\r\n",
        "        split_sum = len(train) + len(test)\r\n",
        "        sum = len(df_sum_news_labels)\r\n",
        "        assert split_sum == sum    \r\n",
        "\r\n",
        "        train_headlines = []\r\n",
        "        test_headlines = []\r\n",
        "\r\n",
        "        for row in range(0, len(train.index)):\r\n",
        "            train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "        for row in range(0,len(test.index)):\r\n",
        "            test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "        # show the first\r\n",
        "        print(train_headlines[0])\r\n",
        "\r\n",
        "        for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "            for n in range(1,MODEL_TYPE+1):\r\n",
        "                print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                      + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "                _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "                _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "                print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "                _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "                _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "                _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "                _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "                print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "                model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "                result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        rows_summary_value.append(ROWS)\r\n",
        "\r\n",
        "        # save the best\r\n",
        "        best_model_rows = 0\r\n",
        "\r\n",
        "        for model in range(len(model_type)):\r\n",
        "            if result[model] > best_model_rows:\r\n",
        "                best_model_rows = result[model]\r\n",
        "\r\n",
        "        rows_summary_accuraccy.append(best_model_rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 1 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (39773, 30152)\n",
            "\n",
            "0.5005028157683025\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (39773, 350139)\n",
            "\n",
            "0.504424778761062\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (39773, 319987)\n",
            "\n",
            "0.5135760257441674\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (39773, 716193)\n",
            "\n",
            "0.5069388576025744\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (39773, 686041)\n",
            "\n",
            "0.5161906677393403\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (39773, 366054)\n",
            "\n",
            "0.5160901045856798\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (39773, 1053670)\n",
            "\n",
            "0.5105591311343524\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (39773, 1023518)\n",
            "\n",
            "0.5167940466613034\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (39773, 703531)\n",
            "\n",
            "0.5150844730490748\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (39773, 337477)\n",
            "\n",
            "0.5114641995172968\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (39773, 1353875)\n",
            "\n",
            "0.5091512469831054\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (39773, 1323723)\n",
            "\n",
            "0.5163917940466614\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (39773, 1003736)\n",
            "\n",
            "0.5140788415124699\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (39773, 637682)\n",
            "\n",
            "0.5116653258246179\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (39773, 300205)\n",
            "\n",
            "0.5119670152855994\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (39773, 1616993)\n",
            "\n",
            "0.5087489943684634\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (39773, 1586841)\n",
            "\n",
            "0.5172968624296058\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (39773, 1266854)\n",
            "\n",
            "0.5145816572807723\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (39773, 900800)\n",
            "\n",
            "0.5121681415929203\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (39773, 563323)\n",
            "\n",
            "0.5120675784392599\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (39773, 263118)\n",
            "\n",
            "0.5122687047465808\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 2 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (19093, 29620)\n",
            "\n",
            "0.5006284038542103\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (19093, 356120)\n",
            "\n",
            "0.5031420192710515\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (19093, 326500)\n",
            "\n",
            "0.5190615835777126\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (19093, 746775)\n",
            "\n",
            "0.5023041474654378\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (19093, 717155)\n",
            "\n",
            "0.5215751989945538\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (19093, 390655)\n",
            "\n",
            "0.5140343527440302\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (19093, 1128993)\n",
            "\n",
            "0.5018852115626309\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (19093, 1099373)\n",
            "\n",
            "0.5232509426057813\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (19093, 772873)\n",
            "\n",
            "0.5136154168412232\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (19093, 382218)\n",
            "\n",
            "0.5131964809384164\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (19093, 1493904)\n",
            "\n",
            "0.5035609551738583\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (19093, 1464284)\n",
            "\n",
            "0.5219941348973607\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (19093, 1137784)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (19093, 747129)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (19093, 364911)\n",
            "\n",
            "0.5121491411813992\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (19093, 1840189)\n",
            "\n",
            "0.5033514872224549\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (19093, 1810569)\n",
            "\n",
            "0.5215751989945538\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (19093, 1484069)\n",
            "\n",
            "0.512987012987013\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (19093, 1093414)\n",
            "\n",
            "0.5127775450356096\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (19093, 711196)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (19093, 346285)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 3 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (12729, 29618)\n",
            "\n",
            "0.489789506754634\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (12729, 361687)\n",
            "\n",
            "0.4998429154885328\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (12729, 332069)\n",
            "\n",
            "0.5146088595664468\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (12729, 764956)\n",
            "\n",
            "0.5042412818096136\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (12729, 735338)\n",
            "\n",
            "0.5183788878416589\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (12729, 403269)\n",
            "\n",
            "0.5108388312912346\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (12729, 1166222)\n",
            "\n",
            "0.4998429154885328\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (12729, 1136604)\n",
            "\n",
            "0.5243480992774112\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (12729, 804535)\n",
            "\n",
            "0.5142946905435124\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (12729, 401266)\n",
            "\n",
            "0.513980521520578\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (12729, 1556557)\n",
            "\n",
            "0.5023562676720076\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (12729, 1526939)\n",
            "\n",
            "0.5218347470939365\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (12729, 1194870)\n",
            "\n",
            "0.513980521520578\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (12729, 791601)\n",
            "\n",
            "0.5127238454288408\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (12729, 390335)\n",
            "\n",
            "0.5124096764059064\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (12729, 1934631)\n",
            "\n",
            "0.5010995915802702\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (12729, 1905013)\n",
            "\n",
            "0.5177505497957902\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (12729, 1572944)\n",
            "\n",
            "0.513980521520578\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (12729, 1169675)\n",
            "\n",
            "0.5127238454288408\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (12729, 768409)\n",
            "\n",
            "0.5124096764059064\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (12729, 378074)\n",
            "\n",
            "0.5124096764059064\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 4 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (9547, 29620)\n",
            "\n",
            "0.5002094679514034\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (9547, 364550)\n",
            "\n",
            "0.5111018014243821\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (9547, 334930)\n",
            "\n",
            "0.5295349811478843\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (9547, 774172)\n",
            "\n",
            "0.5043988269794721\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (9547, 744552)\n",
            "\n",
            "0.5249266862170088\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (9547, 409622)\n",
            "\n",
            "0.5127775450356096\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (9547, 1185014)\n",
            "\n",
            "0.5060745705906996\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (9547, 1155394)\n",
            "\n",
            "0.5182237117720989\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (9547, 820464)\n",
            "\n",
            "0.5161290322580645\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (9547, 410842)\n",
            "\n",
            "0.5136154168412232\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (9547, 1588105)\n",
            "\n",
            "0.5031420192710515\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (9547, 1558485)\n",
            "\n",
            "0.5165479681608713\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (9547, 1223555)\n",
            "\n",
            "0.5136154168412232\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (9547, 813933)\n",
            "\n",
            "0.5127775450356096\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (9547, 403091)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (9547, 1982116)\n",
            "\n",
            "0.5018852115626309\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (9547, 1952496)\n",
            "\n",
            "0.5148722245496439\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (9547, 1617566)\n",
            "\n",
            "0.5140343527440302\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (9547, 1207944)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (9547, 797102)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (9547, 394011)\n",
            "\n",
            "0.5123586091328027\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 5 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (7956, 30151)\n",
            "\n",
            "0.4987430869783811\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (7956, 378562)\n",
            "\n",
            "0.4962292609351433\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (7956, 348411)\n",
            "\n",
            "0.5213675213675214\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (7956, 807846)\n",
            "\n",
            "0.4962292609351433\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (7956, 777695)\n",
            "\n",
            "0.5253896430367019\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (7956, 429284)\n",
            "\n",
            "0.5143288084464555\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (7956, 1240597)\n",
            "\n",
            "0.5002513826043238\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (7956, 1210446)\n",
            "\n",
            "0.5188536953242836\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (7956, 862035)\n",
            "\n",
            "0.5168426344896934\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (7956, 432751)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (7956, 1667311)\n",
            "\n",
            "0.5027652086475616\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (7956, 1637160)\n",
            "\n",
            "0.5193564605329312\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (7956, 1288749)\n",
            "\n",
            "0.5128205128205128\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (7956, 859465)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (7956, 426714)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (7956, 2086572)\n",
            "\n",
            "0.4987430869783811\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (7956, 2056421)\n",
            "\n",
            "0.5168426344896934\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (7956, 1708010)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (7956, 1278726)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (7956, 845975)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (7956, 419261)\n",
            "\n",
            "0.5123177476118652\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 6 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (6364, 29617)\n",
            "\n",
            "0.4899497487437186\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (6364, 367327)\n",
            "\n",
            "0.4861809045226131\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (6364, 337710)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (6364, 783208)\n",
            "\n",
            "0.48304020100502515\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (6364, 753591)\n",
            "\n",
            "0.5194723618090452\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (6364, 415881)\n",
            "\n",
            "0.5169597989949749\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (6364, 1203538)\n",
            "\n",
            "0.4849246231155779\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (6364, 1173921)\n",
            "\n",
            "0.5157035175879398\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (6364, 836211)\n",
            "\n",
            "0.5144472361809045\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (6364, 420330)\n",
            "\n",
            "0.5131909547738693\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (6364, 1619305)\n",
            "\n",
            "0.4861809045226131\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (6364, 1589688)\n",
            "\n",
            "0.5131909547738693\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (6364, 1251978)\n",
            "\n",
            "0.5131909547738693\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (6364, 836097)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (6364, 415767)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (6364, 2029176)\n",
            "\n",
            "0.4880653266331658\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (6364, 1999559)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (6364, 1661849)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (6364, 1245968)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (6364, 825638)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (6364, 409871)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 7 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (4773, 27977)\n",
            "\n",
            "0.4748743718592965\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (4773, 330466)\n",
            "\n",
            "0.47571189279731996\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (4773, 302489)\n",
            "\n",
            "0.5201005025125628\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (4773, 699501)\n",
            "\n",
            "0.47571189279731996\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (4773, 671524)\n",
            "\n",
            "0.5175879396984925\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (4773, 369035)\n",
            "\n",
            "0.5159128978224455\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (4773, 1072740)\n",
            "\n",
            "0.46817420435510887\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (4773, 1044763)\n",
            "\n",
            "0.5192629815745393\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (4773, 742274)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (4773, 373239)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (4773, 1442675)\n",
            "\n",
            "0.4648241206030151\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (4773, 1414698)\n",
            "\n",
            "0.5142378559463987\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (4773, 1112209)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (4773, 743174)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (4773, 369935)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (4773, 1808211)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (4773, 1780234)\n",
            "\n",
            "0.5142378559463987\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (4773, 1477745)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (4773, 1108710)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (4773, 735471)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (4773, 365536)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 8 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (4773, 29617)\n",
            "\n",
            "0.46733668341708545\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (4773, 368722)\n",
            "\n",
            "0.4807370184254606\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (4773, 339105)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (4773, 787769)\n",
            "\n",
            "0.48576214405360135\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (4773, 758152)\n",
            "\n",
            "0.52428810720268\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (4773, 419047)\n",
            "\n",
            "0.5175879396984925\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (4773, 1212874)\n",
            "\n",
            "0.4815745393634841\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (4773, 1183257)\n",
            "\n",
            "0.5226130653266332\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (4773, 844152)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (4773, 425105)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (4773, 1635007)\n",
            "\n",
            "0.47571189279731996\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (4773, 1605390)\n",
            "\n",
            "0.518425460636516\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (4773, 1266285)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (4773, 847238)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (4773, 422133)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (4773, 2052834)\n",
            "\n",
            "0.47822445561139026\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (4773, 2023217)\n",
            "\n",
            "0.5159128978224455\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (4773, 1684112)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (4773, 1265065)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (4773, 839960)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (4773, 417827)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 9 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3182, 26245)\n",
            "\n",
            "0.48115577889447236\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3182, 291742)\n",
            "\n",
            "0.47361809045226133\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3182, 265497)\n",
            "\n",
            "0.5276381909547738\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3182, 612329)\n",
            "\n",
            "0.45979899497487436\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3182, 586084)\n",
            "\n",
            "0.5389447236180904\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3182, 320587)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3182, 936909)\n",
            "\n",
            "0.4685929648241206\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3182, 910664)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3182, 645167)\n",
            "\n",
            "0.5138190954773869\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3182, 324580)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3182, 1259466)\n",
            "\n",
            "0.47361809045226133\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3182, 1233221)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3182, 967724)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3182, 647137)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3182, 322557)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3182, 1579138)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3182, 1552893)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3182, 1287396)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3182, 966809)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3182, 642229)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3182, 319672)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 10 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3182, 27439)\n",
            "\n",
            "0.47738693467336685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3182, 318782)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3182, 291343)\n",
            "\n",
            "0.5376884422110553\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3182, 673968)\n",
            "\n",
            "0.45226130653266333\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3182, 646529)\n",
            "\n",
            "0.5288944723618091\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3182, 355186)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3182, 1034343)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3182, 1006904)\n",
            "\n",
            "0.5238693467336684\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3182, 715561)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3182, 360375)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3182, 1392888)\n",
            "\n",
            "0.4648241206030151\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3182, 1365449)\n",
            "\n",
            "0.5226130653266332\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3182, 1074106)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3182, 718920)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3182, 358545)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3182, 1748593)\n",
            "\n",
            "0.46608040201005024\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3182, 1721154)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3182, 1429811)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3182, 1074625)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3182, 714250)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3182, 355705)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 11 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia rice gives green light israel attack iran says u veto israeli military ops\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3182, 28531)\n",
            "\n",
            "0.47738693467336685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3182, 344623)\n",
            "\n",
            "0.4685929648241206\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3182, 316092)\n",
            "\n",
            "0.5402010050251256\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3182, 733504)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3182, 704973)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3182, 388881)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3182, 1128813)\n",
            "\n",
            "0.4685929648241206\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3182, 1100282)\n",
            "\n",
            "0.5213567839195979\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3182, 784190)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3182, 395309)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3182, 1522508)\n",
            "\n",
            "0.47613065326633164\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3182, 1493977)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3182, 1177885)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3182, 789004)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3182, 393695)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3182, 1913419)\n",
            "\n",
            "0.47613065326633164\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3182, 1884888)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3182, 1568796)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3182, 1179915)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3182, 784606)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3182, 390911)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 12 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia rice gives green light israel attack iran says u veto israeli military ops announcing class action lawsuit behalf american public fbi\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3182, 29617)\n",
            "\n",
            "0.4535175879396985\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3182, 370130)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3182, 340513)\n",
            "\n",
            "0.5276381909547738\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3182, 792329)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3182, 762712)\n",
            "\n",
            "0.5188442211055276\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3182, 422199)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3182, 1222200)\n",
            "\n",
            "0.4748743718592965\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3182, 1192583)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3182, 852070)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3182, 429871)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3182, 1650695)\n",
            "\n",
            "0.478643216080402\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3182, 1621078)\n",
            "\n",
            "0.5150753768844221\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3182, 1280565)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3182, 858366)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3182, 428495)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3182, 2076476)\n",
            "\n",
            "0.48743718592964824\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3182, 2046859)\n",
            "\n",
            "0.5138190954773869\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3182, 1706346)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3182, 1284147)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3182, 854276)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3182, 425781)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the ROWS = 25 sequence\n",
            "\n",
            "--------------------------------------------\n",
            "\n",
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia rice gives green light israel attack iran says u veto israeli military ops announcing class action lawsuit behalf american public fbi russia georgia war nyt top story opening ceremonies olympics fucking disgrace yet proof decline journalism china tells bush stay countries affairs world war iii start today georgia invades south ossetia russia gets involved nato absorb georgia unleash full scale war al qaeda faces islamist backlash condoleezza rice us would act prevent israeli strike iran israeli defense minister ehud barak israel prepared uncompromising victory case military hostilities busy day european union approved new sanctions iran protest nuclear programme georgia withdraw soldiers iraq help fight russian forces georgia breakaway region south ossetia pentagon thinks attacking iran bad idea us news amp world report caucasus crisis georgia invades south ossetia indian shoe manufactory series like work visitors suffering mental illnesses banned olympics help mexico kidnapping surge\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (1591, 30148)\n",
            "\n",
            "0.44974874371859297\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (1591, 384104)\n",
            "\n",
            "0.4899497487437186\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (1591, 353956)\n",
            "\n",
            "0.5050251256281407\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (1591, 825961)\n",
            "\n",
            "0.4748743718592965\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (1591, 795813)\n",
            "\n",
            "0.5201005025125628\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (1591, 441857)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (1591, 1277737)\n",
            "\n",
            "0.4798994974874372\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (1591, 1247589)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (1591, 893633)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (1591, 451776)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (1591, 1729848)\n",
            "\n",
            "0.4748743718592965\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (1591, 1699700)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (1591, 1345744)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (1591, 903887)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (1591, 452111)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (1591, 2180871)\n",
            "\n",
            "0.4648241206030151\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (1591, 2150723)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (1591, 1796767)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (1591, 1354910)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (1591, 903134)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (1591, 451023)\n",
            "\n",
            "0.5125628140703518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgc2E1npZvJC"
      },
      "source": [
        "Kiértékelés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWUCZnMNfebo",
        "outputId": "3a567eee-98ab-4927-f52d-ec89873dd703"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    best_model_rows = 0\r\n",
        "\r\n",
        "    for model in range(len(rows_summary_value)):\r\n",
        "        print(str(rows_summary_value[model]) + \":\\t\\t\\t\\t\\t\" \r\n",
        "              + str(rows_summary_accuraccy[model]))\r\n",
        "\r\n",
        "        if rows_summary_accuraccy[model] > best_model_rows:\r\n",
        "            best_model_rows = rows_summary_accuraccy[model]\r\n",
        "            best_model_rows_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest row value:\\n\" \r\n",
        "          + str(rows_summary_value[best_model_rows_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(rows_summary_accuraccy[best_model_rows_index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1:\t\t\t\t\t0.5172968624296058\n",
            "2:\t\t\t\t\t0.5232509426057813\n",
            "3:\t\t\t\t\t0.5243480992774112\n",
            "4:\t\t\t\t\t0.5295349811478843\n",
            "5:\t\t\t\t\t0.5253896430367019\n",
            "6:\t\t\t\t\t0.5194723618090452\n",
            "7:\t\t\t\t\t0.5201005025125628\n",
            "8:\t\t\t\t\t0.52428810720268\n",
            "9:\t\t\t\t\t0.5389447236180904\n",
            "10:\t\t\t\t\t0.5376884422110553\n",
            "11:\t\t\t\t\t0.5402010050251256\n",
            "12:\t\t\t\t\t0.5276381909547738\n",
            "25:\t\t\t\t\t0.5201005025125628\n",
            "--------------------------------------------\n",
            "Best row value:\n",
            "11\t\t\t\t\t0.5402010050251256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GyfELNcMas"
      },
      "source": [
        "A legjobb ROWS eredményeinek megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnMyo9Ljca4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec0c57e-e190-4119-fb7d-2422eaab0689"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\r\n",
        "\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "\r\n",
        "    df_sum_news_labels = preprocess()\r\n",
        "    train = split_to_train()\r\n",
        "    test = split_to_test()\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum    \r\n",
        "\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia rice gives green light israel attack iran says u veto israeli military ops\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (3182, 28531)\n",
            "\n",
            "0.47738693467336685\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (3182, 344623)\n",
            "\n",
            "0.4685929648241206\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (3182, 316092)\n",
            "\n",
            "0.5402010050251256\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (3182, 733504)\n",
            "\n",
            "0.4623115577889447\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (3182, 704973)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (3182, 388881)\n",
            "\n",
            "0.5163316582914573\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (3182, 1128813)\n",
            "\n",
            "0.4685929648241206\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (3182, 1100282)\n",
            "\n",
            "0.5213567839195979\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (3182, 784190)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (3182, 395309)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (3182, 1522508)\n",
            "\n",
            "0.47613065326633164\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (3182, 1493977)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (3182, 1177885)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (3182, 789004)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (3182, 393695)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (3182, 1913419)\n",
            "\n",
            "0.47613065326633164\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (3182, 1884888)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (3182, 1568796)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (3182, 1179915)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (3182, 784606)\n",
            "\n",
            "0.5125628140703518\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (3182, 390911)\n",
            "\n",
            "0.5125628140703518\n",
            "1,1 n-gram:\t\t\t\t\t0.47738693467336685\n",
            "1,2 n-gram:\t\t\t\t\t0.4685929648241206\n",
            "2,2 n-gram:\t\t\t\t\t0.5402010050251256\n",
            "1,3 n-gram:\t\t\t\t\t0.4623115577889447\n",
            "2,3 n-gram:\t\t\t\t\t0.5163316582914573\n",
            "3,3 n-gram:\t\t\t\t\t0.5163316582914573\n",
            "1,4 n-gram:\t\t\t\t\t0.4685929648241206\n",
            "2,4 n-gram:\t\t\t\t\t0.5213567839195979\n",
            "3,4 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "4,4 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "1,5 n-gram:\t\t\t\t\t0.47613065326633164\n",
            "2,5 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "3,5 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "4,5 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "5,5 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "1,6 n-gram:\t\t\t\t\t0.47613065326633164\n",
            "2,6 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "3,6 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "4,6 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "5,6 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "6,6 n-gram:\t\t\t\t\t0.5125628140703518\n",
            "--------------------------------------------\n",
            "Best model:\n",
            "2,2 n-gram\t\t\t\t\t0.5402010050251256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5fXYYfZm_7W"
      },
      "source": [
        "A legjobbhoz tartozó korrelációs tényezők megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk-qyHFInIOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929f6eaa-ea12-4fc5-80b9-beb1182548f5"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    df_sum_news_labels = preprocess()\r\n",
        "    train = split_to_train()\r\n",
        "    test = split_to_test()\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum    \r\n",
        "\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like georgian troops retreat osettain capital presumably leaving several hundred people killed video u prep georgia war russia rice gives green light israel attack iran says u veto israeli military ops\n",
            "The shape is: (3182, 316092)\n",
            "\n",
            "0.5402010050251256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6Q8qicpSAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e14f98cb-41ee-4b22-b64c-896c2472b169"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         Word  Coefficient\n",
            "276468               tear gas     0.396527\n",
            "280875            three years     0.366354\n",
            "247566       security council     0.364729\n",
            "103427             first time     0.320483\n",
            "258208           social media     0.312657\n",
            "138169  intelligence agencies     0.308977\n",
            "251518        sexual violence     0.306716\n",
            "311429          world largest     0.291442\n",
            "236295           rights watch     0.287929\n",
            "186754            new zealand     0.284158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkcfT8TFpTlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc33184e-04a8-4a0e-de88-d47f1fdf204b"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Word  Coefficient\n",
            "205170    phone hacking    -0.307367\n",
            "294893          us army    -0.308631\n",
            "231472  reported killed    -0.311732\n",
            "301652      wall street    -0.311944\n",
            "140902     iran nuclear    -0.336822\n",
            "189778  nuclear weapons    -0.337099\n",
            "251473     sexual abuse    -0.341060\n",
            "29918         bin laden    -0.352661\n",
            "260069     south korean    -0.487165\n",
            "16459      around world    -0.492251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKZQkIdFAXfG"
      },
      "source": [
        "## **ECO_BSN_DF, ECO_FNC_DF, ECO_US_DF 2008-2016 (2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyZGBhb0CcgL"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\r\n",
        "\r\n",
        "Ezeket az adathalmazokat én magam gyűjtöttem az alábbi oldalakról:\r\n",
        "\r\n",
        "\r\n",
        "*   https://www.economist.com/business/ \r\n",
        "*   https://www.economist.com/finance-and-economics/ \r\n",
        "*   https://www.economist.com/united-states/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8lzj6Qs9_e"
      },
      "source": [
        "### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fORGByWVtCtj"
      },
      "source": [
        "Először betöltöm külön-külön az adathalmazokat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVNdyg2RtCR-"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_BSN_DF.csv\" \"ECO_BSN_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_FNC_DF.csv\" \"ECO_FNC_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_US_DF.csv\" \"ECO_US_DF.csv\"\r\n",
        "\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[0].find(\"ECO_FNC_DF.csv\") != -1\r\n",
        "    assert read[0].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \r\n",
        "    assert read[1].find(\"ECO_BSN_DF.csv\") != -1\r\n",
        "    assert read[1].find(\"ECO_US_DF.csv\") != -1\r\n",
        "\r\n",
        "    # Load the datasets \r\n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\r\n",
        "    df_bsn = pd.read_csv('ECO_BSN_DF.csv', index_col = \"date\")\r\n",
        "    df_fnc = pd.read_csv('ECO_FNC_DF.csv', index_col = \"date\")\r\n",
        "    df_us = pd.read_csv('ECO_US_DF.csv', index_col = \"date\")\r\n",
        "\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkYz3ExjupOh"
      },
      "source": [
        "Az adathalmazok megvizsgálása az elemein keresztül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNBo9X2RuoaT"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Show the dataframe\r\n",
        "    print(\"Reddit\")\r\n",
        "    print(df_reddit.head())\r\n",
        "    print(\"\\n\\nBSN ECO\")\r\n",
        "    print(df_bsn.head())\r\n",
        "    print(\"\\n\\nFNC ECO\")\r\n",
        "    print(df_fnc.head())\r\n",
        "    print(\"\\n\\nUS ECO\")\r\n",
        "    print(df_us.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf587n4txlu2"
      },
      "source": [
        "Azon elemek megkeresése az ECO adathalmazból ami beleesik a vizsgált időintervallumba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm4PdT-TxyV2"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_bsn_inspect = df_bsn[df_bsn.index < '2016/07/02']\r\n",
        "    df_bsn_inspect = df_bsn_inspect[df_bsn_inspect.index > '2008/08/07']\r\n",
        "    df_bsn_inspect = df_bsn_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_fnc_inspect = df_fnc[df_fnc.index < '2016/07/02']\r\n",
        "    df_fnc_inspect = df_fnc_inspect[df_fnc_inspect.index > '2008/08/07']\r\n",
        "    df_fnc_inspect = df_fnc_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_us_inspect = df_us[df_us.index < '2016/07/02']\r\n",
        "    df_us_inspect = df_us_inspect[df_us_inspect.index > '2008/08/07']\r\n",
        "    df_us_inspect = df_us_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    print(\"BSN ECO\")\r\n",
        "    print(df_bsn_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_bsn_inspect.tail(2))\r\n",
        "    print(df_bsn_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nFNC ECO\")\r\n",
        "    print(df_fnc_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_fnc_inspect.tail(2))\r\n",
        "    print(df_fnc_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nUS ECO\")\r\n",
        "    print(df_us_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_us_inspect.tail(2))\r\n",
        "    print(df_us_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_bsn_inspect) + len(df_fnc_inspect) + len(df_us_inspect)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCBuxng0j-S"
      },
      "source": [
        "Az Economist oldalról származó adathalmazok összefűzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDOluCHa1ASA"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_eco_all = pd.concat([df_bsn_inspect, df_fnc_inspect, df_us_inspect])\r\n",
        "\r\n",
        "    df_eco_all = df_eco_all.drop_duplicates()\r\n",
        "\r\n",
        "    df_eco_all.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    print(\"ECO MERGED\")\r\n",
        "    print(df_eco_all.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_eco_all.tail(2))\r\n",
        "    print(df_eco_all.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9cTQSlx263t"
      },
      "source": [
        "Egy naphoz tartozó azonos hírek vizsgálata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpmSZfz2-0D"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Groupby by date\r\n",
        "    dates = df_eco_all.groupby(\"date\")\r\n",
        "\r\n",
        "    # Summary statistic\r\n",
        "    print(\"Max:\")\r\n",
        "    print(dates.describe().max())\r\n",
        "    print(\"\\n\\nMin:\")\r\n",
        "    print(dates.describe().min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DuBpOl4MWn"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    dates_count = [] # for count\r\n",
        "    dates_dates = [] # for indexing\r\n",
        "    df_dates = dates.describe()\r\n",
        "\r\n",
        "    for row in range(len(df_dates)):\r\n",
        "        dates_count.append(len(dates.get_group(df_dates.index[row])))\r\n",
        "        dates_dates.append(dates.get_group(df_dates.index[row]).index[0])\r\n",
        "\r\n",
        "    df_group_dates = pd.DataFrame()\r\n",
        "    df_group_dates[\"date\"] = dates_dates\r\n",
        "    df_group_dates[\"count\"] = dates_count\r\n",
        "    df_group_dates.set_index(\"date\", inplace=True)\r\n",
        "    df_group_dates.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    print(df_group_dates.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDcXZfr9CS2"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Groupby by date\r\n",
        "    counts = df_group_dates.groupby(\"count\")\r\n",
        "\r\n",
        "    keys = list(counts.groups.keys())\r\n",
        "\r\n",
        "    sum = 0\r\n",
        "\r\n",
        "    for key in keys:\r\n",
        "      sum = sum + key * len(counts.get_group(key))\r\n",
        "      print(\"Count: \" + str(key) + \"\\t\\t\" + str(len(counts.get_group(key))))\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary:\\t\\t\" + str(sum))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1JU31uKqLnw"
      },
      "source": [
        "Az összefűzött hírekhez a címkék generálása a részvény árfolyama alapján."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O6BT2vQKnFu"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    days = []\r\n",
        "    stock_days = []\r\n",
        "    wrong_days = []\r\n",
        "\r\n",
        "    # Create dates and remove duplicates\r\n",
        "    for day in range(len(df_eco_all.index)):\r\n",
        "        if day == 0:\r\n",
        "            days.append(str(df_eco_all.index[day]))\r\n",
        "        elif df_eco_all.index[day] != days[len(days) - 1]:\r\n",
        "            days.append(str(df_eco_all.index[day]))\r\n",
        "\r\n",
        "    # Drop not needed days\r\n",
        "    for day in range(len(df_stock.index)):\r\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\r\n",
        "\r\n",
        "    # Remove not relevant date\r\n",
        "    good_days = []\r\n",
        "    for day in days:\r\n",
        "        try:\r\n",
        "            if stock_days.index(day):\r\n",
        "                good_days.append(str(day))\r\n",
        "        except:\r\n",
        "            wrong_days.append(str(day))\r\n",
        "\r\n",
        "    print(\"All days:\\t\\t\" + str(len(days)))\r\n",
        "    print(\"Good days:\\t\\t\" + str(len(good_days)))\r\n",
        "    print(\"Wrong days:\\t\\t\" + str(len(wrong_days)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBpmP0WArssx"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    label_eco = []\r\n",
        "    date_label_eco =[]\r\n",
        "    title_label_eco = []\r\n",
        "\r\n",
        "    for day in range(len(good_days)):\r\n",
        "        if day == 0:\r\n",
        "            title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "            label_eco.append(0)\r\n",
        "            date_label_eco.append(good_days[day])      \r\n",
        "        # label should be 1 -> rise\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\r\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\r\n",
        "                    title_label_eco.append(row)\r\n",
        "                    label_eco.append(1)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "                    label_eco.append(1)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\r\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\r\n",
        "                    title_label_eco.append(row)\r\n",
        "                    label_eco.append(0)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "                    label_eco.append(0)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "\r\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_eco)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ec36DKBX9F"
      },
      "source": [
        "A címkékkel rendelkező, használható adatokból egy új adathalmaz létrehozása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEXYSpdbBVk9"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_eco = pd.DataFrame()\r\n",
        "    df_eco[\"date\"] = date_label_eco\r\n",
        "    df_eco[\"label\"] = label_eco\r\n",
        "    df_eco[\"title\"] = title_label_eco\r\n",
        "    df_eco.set_index(\"date\", inplace=True)\r\n",
        "    df_eco.sort_index(ascending=True, inplace=True)\r\n",
        "    print(df_eco.head())\r\n",
        "    print(len(df_eco))\r\n",
        "\r\n",
        "    # drop duplicates\r\n",
        "    df_eco.drop_duplicates(subset=\"title\", inplace=True)\r\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\r\n",
        "    print(df_eco.head())\r\n",
        "    print(len(df_eco))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREcqTfFmUwJ"
      },
      "source": [
        "### Adathalmazok előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa5nBTd8OFpB"
      },
      "source": [
        "Az adathalmaz megtisztítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7GYSLxOIrX"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      try:\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if word not in string.punctuation:\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      except:\r\n",
        "          temp = \" \"\r\n",
        "          temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_eco[\"title\"] = temp_news\r\n",
        "\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_eco[\"title\"] = filtered_sentence\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_eco.reset_index(inplace=True)\r\n",
        "\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "\r\n",
        "    print(null_indexes)\r\n",
        "\r\n",
        "    for row in range(len(null_indexes)):\r\n",
        "      df_eco = df_eco.drop(df_eco.index[null_indexes[row] - row])\r\n",
        "\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "      \r\n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_b24C3cmZIQ"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és tesztelő adathalmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0BpPPB_mYAN"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Drop the dates\r\n",
        "    df_eco_label_title = pd.DataFrame()\r\n",
        "    df_eco_label_title[\"label\"] = df_eco[\"label\"]\r\n",
        "    df_eco_label_title[\"title\"] = df_eco[\"title\"]\r\n",
        "    print(\"New dataset without the dates\")\r\n",
        "    print(df_eco_label_title.head())\r\n",
        "    print(len(df_eco_label_title))\r\n",
        "\r\n",
        "    # Do the shuffle\r\n",
        "    for i in range(SHUFFLE_CYCLE):\r\n",
        "      df_eco_label_title = shuffle(df_eco_label_title, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_eco_label_title.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    print(\"\\n\\nAfter shuffle\")\r\n",
        "    print(df_eco_label_title.head())    \r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    INPUT_SIZE = len(df_eco_label_title)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    train = df_eco_label_title[:TRAIN_SIZE] \r\n",
        "    test = df_eco_label_title[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"\\n\\nAfter split\")\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_eco_label_title)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_eco_label_title)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmLc118-ozZo"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTDyz6azo5H7"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCwyjWHNo3Y1"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "    model_type_values = []\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    # Create model type values\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    print(\"\\n\\n\")\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9gbVqadzbUd"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU4nnwBzzmOF"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXZ75tt2zobK"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wcHp91ptLF-"
      },
      "source": [
        "## **KAG_BENZ_ANALYST_DF, KAG_BENZ_PARTNER_DF 2008-2016**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8XoC5ltwPe"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\r\n",
        "\r\n",
        "Ezen adathalmazok forrása:\r\n",
        "\r\n",
        "*   https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPbi6d_zLCn"
      },
      "source": [
        "#### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pi6pacf80r5"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_1.csv\" \"KAG_BENZ_ANALYST_DF_1.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_2.csv\" \"KAG_BENZ_ANALYST_DF_2.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_1.csv\" \"KAG_BENZ_PARTNER_DF_1.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_2.csv\" \"KAG_BENZ_PARTNER_DF_2.csv\"\r\n",
        "\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[1].find(\"KAG_BENZ_ANALYST_DF_1.csv\") != -1\r\n",
        "    assert read[2].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \r\n",
        "    assert read[2].find(\"KAG_BENZ_ANALYST_DF_2.csv\") != -1\r\n",
        "    assert read[0].find(\"KAG_BENZ_PARTNER_DF_1.csv\") != -1    \r\n",
        "    assert read[1].find(\"KAG_BENZ_PARTNER_DF_2.csv\") != -1\r\n",
        "\r\n",
        "    # Load the datasets \r\n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\r\n",
        "    df_benz_1 = pd.read_csv('KAG_BENZ_ANALYST_DF_1.csv', index_col = \"date\")\r\n",
        "    df_benz_2 = pd.read_csv('KAG_BENZ_ANALYST_DF_2.csv', index_col = \"date\")    \r\n",
        "    df_partner_1 = pd.read_csv('KAG_BENZ_PARTNER_DF_1.csv', index_col = \"date\")\r\n",
        "    df_partner_2 = pd.read_csv('KAG_BENZ_PARTNER_DF_2.csv', index_col = \"date\")\r\n",
        "\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU771dZOCrfM"
      },
      "source": [
        "A szétbontott adathalmazok összefűzése, majd azok megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1W5V63vCyA4"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Merge them\r\n",
        "    df_benz = pd.concat([df_benz_1, df_benz_2])\r\n",
        "    df_partner = pd.concat([df_partner_1, df_partner_2])\r\n",
        "\r\n",
        "    # Show the dataframe\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(len(df_benz))\r\n",
        "    print(\"\\n\\nPARTNER\")\r\n",
        "    print(df_partner.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_partner.tail())\r\n",
        "    print(len(df_partner))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxza21dEGyDD"
      },
      "source": [
        "A vizsgált időtartamba eső adatok kiszűrése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmab7DBG5cB"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz_inspect = df_benz[df_benz.index < '2016/07/02']\r\n",
        "    df_benz_inspect = df_benz_inspect[df_benz_inspect.index > '2008/08/07']\r\n",
        "    df_benz_inspect = df_benz_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_partner_inspect = df_partner[df_partner.index < '2016/07/02']\r\n",
        "    df_partner_inspect = df_partner_inspect[df_partner_inspect.index > '2008/08/07']\r\n",
        "    df_partner_inspect = df_partner_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz_inspect.tail())\r\n",
        "    print(df_benz_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nPARTNER\")\r\n",
        "    print(df_partner_inspect.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_partner_inspect.tail())\r\n",
        "    print(df_partner_inspect.shape)\r\n",
        "\r\n",
        "    df_benz = pd.concat([df_benz_inspect, df_partner_inspect])\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_benz_inspect) + len(df_partner_inspect)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UngRtIFVpi"
      },
      "source": [
        "Az adathalmazban található adatok kiszűrése a Dow Jonews Industrial Average alapján:\r\n",
        "\r\n",
        "\r\n",
        "*   Procter & Gamble, PG, 1932-05-26\r\n",
        "*   3M Company, MMM, 1976-08-09\r\n",
        "*   IBM, IBM, 1979-06-29\r\n",
        "*   Merck & Co., MRK, 1979-06-29\r\n",
        "*   American Express, AXP, 1982-08-30\r\n",
        "*   McDonald's, MCD, 1985-10-30\r\n",
        "*   Boeing, BA, 1987-03-12\r\n",
        "*   The Coca-Cola Company, KO, 1987-03-12\r\n",
        "*   Caterpillar Inc., CAT, 1991-05-06\r\n",
        "*   JPMorgan Chase, JPM, 1991-05-06\r\n",
        "*   The Walt Disney Company, DIS, 1991-05-06\r\n",
        "*   Johnson & Johnson, JNJ, 1997-03-17\r\n",
        "*   Walmart, WMT, 1997-03-17\r\n",
        "*   The Home Depot, HD, 1999-11-01\r\n",
        "*   Intel, INTC, 1999-11-01\r\n",
        "*   Microsoft, MSFT, 1999-11-01\r\n",
        "*   Verizon, VZ, 2004-04-08\r\n",
        "*   Chevron Corporation, CVX, 2008-02-19\r\n",
        "*   Cisco Systems, CSCO, 2009-06-08\r\n",
        "*   The Travelers Companies, \tTRV, 2009-06-08\r\n",
        "*   UnitedHealth Group, UNH, \t2012-09-24\r\n",
        "*   Goldman Sachs, GS, \t2013-09-20\t\r\n",
        "*   Nike, NKE, 2013-09-20\r\n",
        "*   Visa Inc., \tV, 2013-09-20\t\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaU5ro8Y1vl"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    tickers = []\r\n",
        "    unique_count = []\r\n",
        "\r\n",
        "    # The stock tickers which is needed\r\n",
        "    stock_ticker = [\"PG\", \"MMM\", \"IBM\", \"MRK\", \"AXP\", \"MCD\", \"BA\", \"KO\", \"CAT\", \"JPM\",\r\n",
        "                    \"DIS\", \"JNJ\", \"WMT\", \"HD\", \"INTC\", \"MSFT\", \"VZ\", \"CVX\", \"CSCO\",\r\n",
        "                    \"TRV\", \"UNH\", \"GS\", \"NKE\", \"V\"]\r\n",
        "\r\n",
        "    stocks_benz = df_benz.groupby(\"stock\")\r\n",
        "    stock_benz_df = stocks_benz.describe()\r\n",
        "\r\n",
        "    for stock in range(len(stock_benz_df.index)):\r\n",
        "        tickers.append(stock_benz_df.index[stock])\r\n",
        "\r\n",
        "    for stock in stock_ticker:\r\n",
        "        try:\r\n",
        "            unique_count.append(stock_benz_df.iloc[tickers.index(stock), :][1]) #unique\r\n",
        "        except:\r\n",
        "            unique_count.append(0)\r\n",
        "            print(str(stock) + \"\\tis not in list\")\r\n",
        "\r\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\r\n",
        "\r\n",
        "    sum_count = 0\r\n",
        "\r\n",
        "    for stock in range(len(stock_ticker)):\r\n",
        "        print(str(stock_ticker[stock]) + \"\\t\\t\\t\" + str(unique_count[stock]))\r\n",
        "        sum_count = sum_count + unique_count[stock]\r\n",
        "\r\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\r\n",
        "    print(\"Summary of news with the tickers:\\t\" + str(sum_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okO_8su2ABZL"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz_filtered = pd.DataFrame()\r\n",
        "\r\n",
        "    for stock in stock_ticker:\r\n",
        "        df_temp = df_benz[(df_benz[\"stock\"]) == stock].drop_duplicates()\r\n",
        "        df_benz_filtered = pd.concat([df_benz_filtered, df_temp])\r\n",
        "\r\n",
        "    df_benz_filtered.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    df_benz = df_benz_filtered\r\n",
        "    df_benz.drop(\"stock\", axis = 1, inplace = True)\r\n",
        "    df_benz.drop_duplicates(inplace=True)\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail(2))\r\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_fkeQjJPtw"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVB_ZTTqKTYR"
      },
      "source": [
        "A szöveg előfeldolgozása következik, mint az írásjelek eltűvolítása, a számok eltávolítása, felesleges szóközöktől való megtisztítás, minden szó kisbetűs szóra cserélése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_03-r1XOKWsY"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    news_sum = df_benz[\"headline\"]\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if word not in string.punctuation:\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_benz[\"headline\"] = temp_news\r\n",
        "\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_benz[\"headline\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_benz[\"headline\"] = filtered_sentence\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail(2))\r\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxxhSMNJSP-"
      },
      "source": [
        "### Címke létrehozása, adathalmaz felbontása"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bgzTSWYLV1r"
      },
      "source": [
        "A következőkben a címkék generálása és az adathalmaz felbontása történik tanító és validáló halmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDFCry1TLbYn"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    days = []\r\n",
        "    stock_days = []\r\n",
        "    wrong_days = []\r\n",
        "\r\n",
        "    # Create dates and remove duplicates\r\n",
        "    for day in range(len(df_benz.index)):\r\n",
        "        temp = str(df_benz.index[day])[0:10].replace(\"-\",\"/\")\r\n",
        "        if day == 0:\r\n",
        "            days.append(temp)\r\n",
        "        elif df_benz.index[day] != df_benz.index[day - 1]:\r\n",
        "            days.append(temp)\r\n",
        "\r\n",
        "    # Update the dataframe date column\r\n",
        "    df_benz.reset_index(inplace=True)\r\n",
        "    temp_days = df_benz[\"date\"]\r\n",
        "    days_to_update = []\r\n",
        "    for date in range(len(temp_days)):\r\n",
        "        temp = str(temp_days[date])[0:10].replace(\"-\",\"/\")\r\n",
        "        days_to_update.append(temp)\r\n",
        "\r\n",
        "    df_benz[\"date\"] = days_to_update\r\n",
        "    df_benz.set_index(\"date\", inplace=True, drop=True)    \r\n",
        "\r\n",
        "    # Drop not needed days\r\n",
        "    for day in range(len(df_stock.index)):\r\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\r\n",
        "\r\n",
        "    # Remove not relevant date\r\n",
        "    good_days = []\r\n",
        "    for day in days:\r\n",
        "        try:\r\n",
        "            if stock_days.index(day):\r\n",
        "                good_days.append(str(day))\r\n",
        "        except:\r\n",
        "            wrong_days.append(str(day))\r\n",
        "\r\n",
        "    print(\"All days:\\t\\t\\t\\t\" + str(len(days)))\r\n",
        "    print(\"Good days:\\t\\t\\t\\t\" + str(len(good_days)))\r\n",
        "    print(\"Wrong days:\\t\\t\\t\\t\" + str(len(wrong_days)))\r\n",
        "\r\n",
        "    label_benz = []\r\n",
        "    date_label_benz =[]\r\n",
        "    title_label_benz = []\r\n",
        "\r\n",
        "    for day in range(len(good_days)):\r\n",
        "        if day == 0:\r\n",
        "            title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "            label_benz.append(0)\r\n",
        "            date_label_benz.append(good_days[day])      \r\n",
        "        # label should be 1 -> rise\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\r\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\r\n",
        "                    title_label_benz.append(row)\r\n",
        "                    label_benz.append(1)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "                    label_benz.append(1)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\r\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\r\n",
        "                    title_label_benz.append(row)\r\n",
        "                    label_benz.append(0)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "                    label_benz.append(0)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "\r\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_benz)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8BBw1Zok7W"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz = pd.DataFrame()\r\n",
        "    df_benz[\"date\"] = date_label_benz\r\n",
        "    df_benz[\"label\"] = label_benz\r\n",
        "    df_benz[\"title\"] = title_label_benz\r\n",
        "    df_benz.set_index(\"date\", inplace=True)\r\n",
        "    df_benz.sort_index(ascending=True, inplace=True)\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(df_benz.shape)\r\n",
        "\r\n",
        "    # drop duplicates\r\n",
        "    df_benz.drop_duplicates(subset=\"title\", inplace=True)\r\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(df_benz.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dq2UTnqrJAw"
      },
      "source": [
        "Az adathalmaz felbontása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSkAzvfVrWGJ"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Drop the dates\r\n",
        "    df_benz_label_title = pd.DataFrame()\r\n",
        "    df_benz_label_title[\"label\"] = df_benz[\"label\"]\r\n",
        "    df_benz_label_title[\"title\"] = df_benz[\"title\"]\r\n",
        "    # Reset the index\r\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\r\n",
        "    print(\"New dataset without the dates\")\r\n",
        "    print(df_benz_label_title.head())\r\n",
        "    print(len(df_benz_label_title))\r\n",
        "\r\n",
        "    # Do the shuffle\r\n",
        "    for i in range(SHUFFLE_CYCLE):\r\n",
        "      df_benz_label_title = shuffle(df_benz_label_title, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    print(\"\\n\\nAfter shuffle\")\r\n",
        "    print(df_benz_label_title.head())    \r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    INPUT_SIZE = len(df_benz_label_title)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    train = df_benz_label_title[:TRAIN_SIZE] \r\n",
        "    test = df_benz_label_title[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"\\n\\nAfter split\")\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_benz_label_title)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_benz_label_title)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQA38WaDJWul"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGrMzP5r88E"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO8dmAdhr-28"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "    model_type_values = []\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    # Create model type values\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    print(\"\\n\\n\")\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCTns-8JsVZp"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIGQBKMgsYID"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIsCcJzgsa5W"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}