{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogReg_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ygDjIPBVTLl"
      },
      "source": [
        "# **Stock market news feed semantic analysis** *(Baseline LogReg)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kLvL-AJVg3C"
      },
      "source": [
        "Ebben a notebookban az eddigi általam kibányászott, megszerzett adathalmazokat fogom a hagyományos bag of words és logistic regression módszerrel megvizsgálni. Ezek után n-gram modelleket is ki fogok próbálni. Az általa használt források és referenciák az eredményekhez:\r\n",
        "\r\n",
        "\r\n",
        "*   https://colab.research.google.com/drive/1QPrBkh-KwX6qcUtiNWKp9rJoneBfGEVh#scrollTo=bQUJwMjYYN4- *(saját munka - átdolgozott)*\r\n",
        "*   https://colab.research.google.com/drive/1MdpXGCj2fb3g1BI_XfF54OWLkYQCZBBy#scrollTo=LndWT2Kn-UMK *(saját baseline munka)*\r\n",
        "*   https://www.kaggle.com/ndrewgele/omg-nlp-with-the-djia-and-reddit#Basic-Model-Training-and-Testing\r\n",
        "*   https://www.kaggle.com/lseiyjg/use-news-to-predict-stock-markets\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvCnWXPfXgZi"
      },
      "source": [
        "A használt adathalmazok alapján külön fejezeteket készítek és mindenhol jelzem a forrását és a megszerzésének a módját, ha saját bányászás eredménye."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsAeJYBEXzW_"
      },
      "source": [
        "## **A projekt előkészítése**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuHwIVcIX2Zp"
      },
      "source": [
        "A Drive csatlakoztatása a szükséges fájlok későbbi betöltésére. A betöltés közvetlen a használat előtt fogom megtenni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvU8LOxdKH5-",
        "outputId": "bc19727b-0eb3-4d22-f65c-1020b458080b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOuSSouYIb7"
      },
      "source": [
        "A szükséges könyvtárak betöltése a projekthez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgUBe7_VYLjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3327b3-d35c-421b-bc88-9ad6ff91fdb4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pandas_datareader as web\r\n",
        "from numpy.random import MT19937\r\n",
        "from numpy.random import RandomState, SeedSequence\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize  \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import accuracy_score \r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiFx9dUgBsN"
      },
      "source": [
        "A projektben használt makrók definiálása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bULVBJPegGcc"
      },
      "source": [
        "# Shuffle cycle number for the dataframe\r\n",
        "SHUFFLE_CYCLE = 500\r\n",
        "\r\n",
        "# Which dataset will be used\r\n",
        "DATASET = 2"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vfWZE7fZzcF"
      },
      "source": [
        "A reprodukálhatóság miatt definiálok egy seed-et a véletlen szám generátorhoz, amit a továbbiakban használni fogok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEjH22olZ1L_"
      },
      "source": [
        "# Random seed\r\n",
        "RANDOM_SEED = 1234\r\n",
        "\r\n",
        "# Numpy random seed\r\n",
        "NP_SEED = 1234\r\n",
        "\r\n",
        "# Max iteration for training\r\n",
        "MAX_ITER = 100000\r\n",
        "\r\n",
        "# Train size\r\n",
        "TRAIN_SPLIT = 0.85\r\n",
        "\r\n",
        "# Test size\r\n",
        "TEST_SPLIT = 0.15"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw003siyimeD"
      },
      "source": [
        "np.random.seed(NP_SEED)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoE6btiUYg-a"
      },
      "source": [
        "## **KAG_REDDIT_WRLD_DJIA_DF (1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2jP3czjYt6c"
      },
      "source": [
        "Ez az adathalmaz a top25 hírt tartalmazza a Reddit World News kategóriából 2008.08.08-2016.07.01 időtartamban. Ez nem általam gyűjtött adathalmaz, a forrása:\r\n",
        "Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved 2021.02.19. from https://www.kaggle.com/aaron7sun/stocknews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_wdhA1ZSrt"
      },
      "source": [
        "Az adathalmaz betöltése a csatlakoztatott Drive-omból."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw00CWOBZbeF"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[0].find(\"Combined_News_DJIA.csv\") != -1"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_uQPC2Mt6F"
      },
      "source": [
        "Az eredmények elmentésére és indexelésére az alábbi két tömböt fogom hasnzálni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ria16HC2MyZa"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    model_type = [\"Bag of words\", \"1,2 n-gram\", \"2,2 n-gram\", \r\n",
        "                  \"1,3 n-gram\", \"2,3 n-gram\", \"3,3 n-gram\"]\r\n",
        "\r\n",
        "    result = []              "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z73YJGnjYxAz"
      },
      "source": [
        "Makró definiálás."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lMyUJerYzAg"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Number of merged news into one string\r\n",
        "    ROWS = 2"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1DAhyIob0Bm"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KSUSX1b4-z"
      },
      "source": [
        "Az adathalmaz betöltése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPmTqk3Gb2Fo"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the dataset \r\n",
        "    df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\r\n",
        "\r\n",
        "    # Show the dataframe\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdvD8SIQcPB6"
      },
      "source": [
        "Érdekességképpen a következőkben megvizsgálom, hogy az adathalmaz címkéi megfelelőek. A forrás szerint a címke 1, ha nőtt vagy azonos maradt az érték azon a napon, illetve 0, ha csökkent. (Adj Close adott napi értéke az előző napihoz viszonyítva)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj26KF2ncgji"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")\r\n",
        "    \r\n",
        "    # Show the stock data\r\n",
        "    print(df_stock.head())"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9UbGo05jACh"
      },
      "source": [
        "Az dátumok formátumát egységesre hozom az összehasonlítás érdekében."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6UkTz6Vg23F"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    temp_day = []\r\n",
        "\r\n",
        "    for day in range(len(df_stock)):\r\n",
        "        temp_day.append(df_stock.index[day].date())\r\n",
        "\r\n",
        "    df_stock.index = temp_day\r\n",
        "\r\n",
        "    # Show the stock data\r\n",
        "    print(df_stock.head())"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWWYe1PpdCK3"
      },
      "source": [
        "Először a dátumok ellenőzöm, hogy megegyeznek-e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77SDqWWGdF31"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    difference = []\r\n",
        "\r\n",
        "    if len(df_combined) == len(df_stock):\r\n",
        "        print(\"The lengths are the same!\")\r\n",
        "\r\n",
        "    for day in range(max(len(df_combined), len(df_stock))):\r\n",
        "        if str(df_combined.index[day]) != str(df_stock.index[day]):\r\n",
        "            print(\"There is difference at: \" + str(day) + \" index\")\r\n",
        "            print(\"News: \" + str(df_combined.index[day]) + \"\\tStock: \" + str(df_stock.index[day]))\r\n",
        "            difference.append(day)\r\n",
        "\r\n",
        "    if len(difference) is 0:\r\n",
        "        print(\"The dates matched!\")"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJogdlwsjJm3"
      },
      "source": [
        "A labelek ellenőrzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyySRYjNjLwt"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    difference = []\r\n",
        "\r\n",
        "    for day in range(len(df_stock)):\r\n",
        "        # label should be 1 -> rise\r\n",
        "        if int(df_stock[\"Adj Close\"][day]) >= int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "            if df_combined[\"Label\"][day] != 1:\r\n",
        "                difference.append(str(df_stock.index[day]))\r\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        if int(df_stock[\"Adj Close\"][day]) < int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "            if df_combined[\"Label\"][day] != 0:\r\n",
        "                difference.append(str(df_stock.index[day]))\r\n",
        "                print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "                print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\r\n",
        "\r\n",
        "    print(\"All differences: \" + str(len(difference)))      "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOI1OZJPU6Wh"
      },
      "source": [
        "Látható, hogy rossz a label pár helyen. Egy kis kutakodás után megtaláltam, hogy maga az árfolyam lekérdezésük volt hibás pár nap esetében, ezért ezeket javítom, majd elmentem a drive-omon a javítottat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHvqT2FRtBkW"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # correct the wrong labels\r\n",
        "    for row in difference:\r\n",
        "        if df_combined.loc[row, \"Label\"] == 0:\r\n",
        "            df_combined.loc[row, \"Label\"] = 1\r\n",
        "        else:\r\n",
        "            df_combined.loc[row, \"Label\"] = 0\r\n",
        "\r\n",
        "    # check them\r\n",
        "    for row in difference:\r\n",
        "        print(str(row) + \"\\t\\t\" + str(df_combined.loc[row, \"Label\"]))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTIqNqucuiBO"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # save to drive\r\n",
        "    df_combined.to_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv')\r\n",
        "\r\n",
        "    # Show the dataset\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAhg7d5Xj2kt"
      },
      "source": [
        "A következőkben az esetleges adat nélküli napokat, illetve cellákat keresem meg és helyettesítem őket egy üres sztringgel. Ez a későbbi szövegfeldolgozás hibamentességéhez szükséges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrFufwo6j5_H"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Find the cells with NaN and after the rows for them\r\n",
        "    is_NaN = df_combined.isnull()\r\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "    # Replace them\r\n",
        "    df_combined = df_combined.replace(np.nan, \" \")\r\n",
        "\r\n",
        "    # Check the process\r\n",
        "    is_NaN = df_combined.isnull()\r\n",
        "    row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "    rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "    assert len(rows_with_NaN) is 0"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxCOud0ki5M-"
      },
      "source": [
        "Ezek után az egy naphoz tartozó híreket közös sztringekbe fűzöm. Az egy sztringbe tartozó hírek számát makróval definiálom:\r\n",
        "\r\n",
        "\r\n",
        "*   ROWS - egymásba fűzött hírek száma\r\n",
        "\r\n",
        "Itt megtalálható már az első előkészítő algoritmusom, méghozzá a sztringek elején található b karakter eltávolítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqv3bDcAi6Bf"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Get column names\r\n",
        "    combined_column_names = []\r\n",
        "    for column in df_combined.columns:\r\n",
        "      combined_column_names.append(column)\r\n",
        "\r\n",
        "    # 2D array creation for the news based on macros\r\n",
        "    COLUMNS = len(df_combined)\r\n",
        "    news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \r\n",
        "\r\n",
        "    # Show the column names\r\n",
        "    print(\"Column names of the dataset:\") \r\n",
        "    print(combined_column_names)\r\n",
        "\r\n",
        "    # Merge the news\r\n",
        "    for row in range(len(df_combined)):\r\n",
        "      for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "        temp = \"\"\r\n",
        "        news = \"\"\r\n",
        "        for word in range(ROWS):\r\n",
        "          news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\r\n",
        "          # Remove the b character at the begining of the string\r\n",
        "          if news[0] is \"b\":\r\n",
        "            news = \" \" + news[1:]\r\n",
        "          temp = temp + news\r\n",
        "        news_sum[column][row] = temp\r\n",
        "\r\n",
        "    # Show the first day second package of the news\r\n",
        "    print(\"\\nThe first day second package of the news:\")\r\n",
        "    print(news_sum[1][0])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvOqG6vckIcF"
      },
      "source": [
        "Ezek után a korábbi oszlopokat(Top1, Top2...) kicserélem a csoportosításnak megfelelő számú oszlopokra és nevekre (News_1, News_2...), majd feltöltöm őket az összevont hírcsomagokkal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xljXPUwmkKrZ"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Drop the old columns\r\n",
        "    for column in range(len(combined_column_names) - 1):\r\n",
        "      df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\r\n",
        "\r\n",
        "    # Create the new columns with the merged news\r\n",
        "    for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "      colum_name = \"News_\" + str(column + 1)\r\n",
        "      df_combined[colum_name] = news_sum[column]\r\n",
        "\r\n",
        "    # Show the DataFrame\r\n",
        "    print(df_combined.head())"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gmqmlpkqhm"
      },
      "source": [
        "Egy új dataframebe újracsoportosítom a hír blokkokat a címkéjükkel, már a dátumok nélkül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoeoCzPekrN2"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # The label column \r\n",
        "    LABEL_COLUMN = 0\r\n",
        "\r\n",
        "    news_sum = []\r\n",
        "    label_sum = []\r\n",
        "\r\n",
        "    # Get the column names\r\n",
        "    combined_column_names = []\r\n",
        "    for column in df_combined.columns:\r\n",
        "      combined_column_names.append(column)\r\n",
        "\r\n",
        "    # Write out the column names \r\n",
        "    print(combined_column_names)\r\n",
        "    print(\"\\n\")\r\n",
        "\r\n",
        "    # Connect the merged news with the labels\r\n",
        "    for column in range(len(df_combined)):\r\n",
        "      for row in range(len(combined_column_names) - 1):\r\n",
        "        news_sum.append(df_combined[combined_column_names[row + 1]][column])\r\n",
        "        label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\r\n",
        "\r\n",
        "    # Create the new DataFrame\r\n",
        "    df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\r\n",
        "    df_sum_news_labels[\"News\"] = news_sum\r\n",
        "\r\n",
        "    # Show it\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYQSgwmslqkO"
      },
      "source": [
        "Először a szövegek előfeldolgozásával kezdem: írásjelek eltávolítása, számok eltávolítása, felesleges szóközök eltávolítása, aztán minden szót kis kezdőbetűjü szóvá konvertálom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJT565FZlsgZ"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if word not in string.punctuation:\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_sum_news_labels[\"News\"] = temp_news\r\n",
        "\r\n",
        "    # Show it\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCvCsvel989"
      },
      "source": [
        "A következőkben az úgy nevezett töltelék szavakat (stop words) fogom eltávolítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3WjnHkAmA4e"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_sum_news_labels[\"News\"] = filtered_sentence\r\n",
        "\r\n",
        "    # Show the DataFrame\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1DHwHi3mbDn"
      },
      "source": [
        "Az adathalmazban lévő nulla hosszú sztring csomagok megkeresése és a hozzájuk tartozó cellák törlése következik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqfwAe1mbqM"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "\r\n",
        "    print(null_indexes)\r\n",
        "\r\n",
        "    for row in null_indexes:\r\n",
        "      df_sum_news_labels = df_sum_news_labels.drop(row)\r\n",
        "\r\n",
        "    news_sum = df_sum_news_labels[\"News\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "      \r\n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UW_rMpOmlSH"
      },
      "source": [
        "Az adathalmaz véletlenszerű sorbarendezése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdW_tECmlxs"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Do the shuffle\r\n",
        "    for i in range(SHUFFLE_CYCLE):\r\n",
        "      df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_sum_news_labels.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    print(df_sum_news_labels.head())"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9cUBq_WnJbH"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és validáló/tesztelő adathalmazokra, majd a szétbontás ellenőrzése mérettel és első elem kiíratásával."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Mg2z7LnKH1"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    train = df_sum_news_labels[:TRAIN_SIZE] \r\n",
        "    test = df_sum_news_labels[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_sum_news_labels)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s03PFsFQsgpS"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(train.tail(1))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au3VLLVzsh0U"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(test.head(1))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRgA6S5muPbE"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsDa53wcxRdG"
      },
      "source": [
        "Először a tanító adathalmaz híreit fűzöm össze egy tömbbe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RhOIEt3wmHn"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    train_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83MuhLlOxYK5"
      },
      "source": [
        "Ezek után vektorizálom őket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3hIvbQxgr6"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_vectorizer = CountVectorizer()\r\n",
        "    bow_train = bow_vectorizer.fit_transform(train_headlines)\r\n",
        "    print(bow_train.shape)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExOpaN_Ix3r_"
      },
      "source": [
        "Egy logistic regression modellt fogok erre a tanító halmazra betanítani."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D36NSmICyOCC"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    bow_model = bow_model.fit(bow_train, train[\"Label\"])"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EYr5_yygsr"
      },
      "source": [
        "A teszt adathalmaz előkészítése, majd becslés a modell segítségével a következő lépés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkxsor9uygSa"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    bow_test = bow_vectorizer.transform(test_headlines)\r\n",
        "    bow_predictions = bow_model.predict(bow_test)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFBYXRil2nS2"
      },
      "source": [
        "Az eredmények megjelenítése egy táblázatban."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfgw4P9-2rkL"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    pd.crosstab(test[\"Label\"], bow_predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9uWaAGm3E_J"
      },
      "source": [
        "A pontossága a modellnek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHxtBIe23HqK"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print (classification_report(test[\"Label\"], bow_predictions))\r\n",
        "    print (accuracy_score(test[\"Label\"], bow_predictions))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], bow_predictions))"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQZQqPar4Im8"
      },
      "source": [
        "A következőkben a top 10 legbefolyásolóbb sztringet jelenítem meg mind pozítiv és mind negatív irányba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oveqbdwt4Q-9"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    bow_words = bow_vectorizer.get_feature_names()\r\n",
        "    bow_coeffs = bow_model.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : bow_words, \r\n",
        "                            'Coefficient' : bow_coeffs})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKPketqV4uVw"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytoEWvZ_48_x"
      },
      "source": [
        "### 2-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_11F44gu5TdJ"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1reDgLAU5jQd"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_12 = CountVectorizer(ngram_range=(1,2))\r\n",
        "    train_vectorizer_12 = gram_vectorizer_12.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_12.shape)\r\n",
        "\r\n",
        "    gram_model_12 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_12 = gram_model_12.fit(train_vectorizer_12, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_12 = gram_vectorizer_12.transform(test_headlines)\r\n",
        "    gram_predictions_12 = gram_model_12.predict(gram_test_12)\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_12))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_12))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_12))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_JxbXWM7wss"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_12 = gram_vectorizer_12.get_feature_names()\r\n",
        "    gram_coeffs_12 = gram_model_12.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_12, \r\n",
        "                            'Coefficient' : gram_coeffs_12})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g96l9WK57xMH"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFrdv3Um9CoD"
      },
      "source": [
        "Másodjára a (2,2) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IdZdzJl9Ftm"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_22 = CountVectorizer(ngram_range=(2,2))\r\n",
        "    train_vectorizer_22 = gram_vectorizer_22.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_22.shape)\r\n",
        "\r\n",
        "    gram_model_22 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_22 = gram_model_22.fit(train_vectorizer_22, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_22 = gram_vectorizer_22.transform(test_headlines)\r\n",
        "    gram_predictions_22 = gram_model_22.predict(gram_test_22)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_22, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_22))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_22))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_22))"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snpLViV79GlO"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_22 = gram_vectorizer_22.get_feature_names()\r\n",
        "    gram_coeffs_22 = gram_model_22.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_22, \r\n",
        "                            'Coefficient' : gram_coeffs_22})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiSp21nl9Gai"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUxZOqjo5Eg7"
      },
      "source": [
        "### 3-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX7vV_715ckj"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (1,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRNHw3tAMSjp"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_13 = CountVectorizer(ngram_range=(1,3))\r\n",
        "    train_vectorizer_13 = gram_vectorizer_13.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_13.shape)\r\n",
        "\r\n",
        "    gram_model_13 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_13 = gram_model_13.fit(train_vectorizer_13, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_13 = gram_vectorizer_13.transform(test_headlines)\r\n",
        "    gram_predictions_13 = gram_model_13.predict(gram_test_13)\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_13))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_13))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_13))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG-xubCYMiFF"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_13 = gram_vectorizer_13.get_feature_names()\r\n",
        "    gram_coeffs_13 = gram_model_13.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_13, \r\n",
        "                            'Coefficient' : gram_coeffs_13})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DequuzvFMmH3"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHmKu3djMS14"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (2,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etgz8IAjMUE3"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_23 = CountVectorizer(ngram_range=(2,3))\r\n",
        "    train_vectorizer_23 = gram_vectorizer_23.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_23.shape)\r\n",
        "\r\n",
        "    gram_model_23 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_23 = gram_model_23.fit(train_vectorizer_23, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_23 = gram_vectorizer_23.transform(test_headlines)\r\n",
        "    gram_predictions_23 = gram_model_23.predict(gram_test_23)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_23, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_23))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_23))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_23))"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVN0l54FMi1-"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_23 = gram_vectorizer_23.get_feature_names()\r\n",
        "    gram_coeffs_23 = gram_model_23.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_23, \r\n",
        "                            'Coefficient' : gram_coeffs_23})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIO9FkYuMnNb"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftajhXMMUUs"
      },
      "source": [
        "Hasonlóan az eddigiekhez vektorizálom a tanító adathalmazom, logistic regression modellt illesztek rá, becslést hajtok végre majd kiértékelem az eredményeket. Először a (3,3) n-gram modellel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T-9458xMaJq"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_vectorizer_33 = CountVectorizer(ngram_range=(3,3))\r\n",
        "    train_vectorizer_33 = gram_vectorizer_33.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(train_vectorizer_33.shape)\r\n",
        "\r\n",
        "    gram_model_33 = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    gram_model_33 = gram_model_33.fit(train_vectorizer_33, train[\"Label\"])\r\n",
        "\r\n",
        "    gram_test_33 = gram_vectorizer_33.transform(test_headlines)\r\n",
        "    gram_predictions_33 = gram_model_33.predict(gram_test_33)\r\n",
        "\r\n",
        "    pd.crosstab(test[\"Label\"], gram_predictions_33, rownames=[\"Actual\"], colnames=[\"Predicted\"])\r\n",
        "\r\n",
        "    print (classification_report(test[\"Label\"], gram_predictions_33))\r\n",
        "    print (accuracy_score(test[\"Label\"], gram_predictions_33))\r\n",
        "\r\n",
        "    result.append(accuracy_score(test[\"Label\"], gram_predictions_33))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7_f6ay_MjhC"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    gram_words_33 = gram_vectorizer_33.get_feature_names()\r\n",
        "    gram_coeffs_33 = gram_model_33.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : gram_words_33, \r\n",
        "                            'Coefficient' : gram_coeffs_33})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNFCAZ2pMpCY"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpZIe-RTmk_"
      },
      "source": [
        "### 4-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y75LfFJBTtrz"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PabhGwFWUZrZ"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    for n in range(1,5):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",4 gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,4))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",4 n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkP2pSAV6Qm"
      },
      "source": [
        "### 5-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Y4HliaV-rg"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8hdr8A-WBRq"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    MODEL_TYPE = 5\r\n",
        "\r\n",
        "    for n in range(1,MODEL_TYPE+1):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgRyRExGWvFN"
      },
      "source": [
        "### 6-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv2plcaDWy8V"
      },
      "source": [
        "Ebben a fejezetben már egy ciklusban vizsgálom meg a bizonyos modelleket és mentem le az eredményeiket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-9kyXKFW1KI"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    MODEL_TYPE = 6\r\n",
        "\r\n",
        "    for n in range(1,MODEL_TYPE+1):\r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "              + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "        _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "        _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "        print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "        _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "        _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "        _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "        _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "        print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "        result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh7BcurhRaGe"
      },
      "source": [
        "### Eredmények összegzése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr2NAOefRc1j"
      },
      "source": [
        "Az eredmények kiíratása, a legjobbat kiemelve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPdalGBRgeL"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    best_model = 0\r\n",
        "\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model:\r\n",
        "            best_model = result[model]\r\n",
        "            best_model_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_index]))"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnuJNnfVYu04"
      },
      "source": [
        "### ROWS makró optimalizálás"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3gtt_TY80k"
      },
      "source": [
        "Ebben a fejezetben a különböző ROWS értékekre (mennyi napi hírt fűzünk egybe) futtatom végig egy automatizált bag of words -> 6,6 gram modell tanítást és becslést és állapítom meg, hogy melyik a legpontosabb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUJCC_EiZo63"
      },
      "source": [
        "A tesztelendő paraméterek megadása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KUt-wbZZO1i"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    # Number of merged news into one string: 1...12, 25 \r\n",
        "    rows_values = []\r\n",
        "    for value in range(1,13):\r\n",
        "        rows_values.append(value)\r\n",
        "\r\n",
        "    rows_values.append(25)\r\n",
        "\r\n",
        "    print(rows_values)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHIHq8mVcNvZ"
      },
      "source": [
        "A modell típusok összegyűjtése az automatizált tanításhoz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U4xwuVXcRsp"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    model_type_values = []\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    print(model_type_values)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9UswHzer9O"
      },
      "source": [
        "A paraméterhez tartozó eredmények tárolására létrehozom az alábbi tömböket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRcPzJ-1exEu"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    rows_summary_value = []\r\n",
        "    rows_summary_accuraccy = []"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0_5e2otZsMM"
      },
      "source": [
        "Automatizált tanítás és mentések."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypJXKkcqGPBZ"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def preprocess():\r\n",
        "        df_combined = pd.read_csv('drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv', \r\n",
        "                                index_col = \"Date\")\r\n",
        "\r\n",
        "        # Find the cells with NaN and after the rows for them\r\n",
        "        is_NaN = df_combined.isnull()\r\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "        # Replace them\r\n",
        "        df_combined = df_combined.replace(np.nan, \" \")\r\n",
        "\r\n",
        "        # Check the process\r\n",
        "        is_NaN = df_combined.isnull()\r\n",
        "        row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "        rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "        assert len(rows_with_NaN) is 0\r\n",
        "\r\n",
        "        # Get column names\r\n",
        "        combined_column_names = []\r\n",
        "        for column in df_combined.columns:\r\n",
        "          combined_column_names.append(column)\r\n",
        "\r\n",
        "        # 2D array creation for the news based on macros\r\n",
        "        COLUMNS = len(df_combined)\r\n",
        "        news_sum = []\r\n",
        "        news_sum = [[0 for i in range(COLUMNS)] for j in range(int((len(combined_column_names) - 1) / ROWS))]  \r\n",
        "\r\n",
        "        # Merge the news\r\n",
        "        for row in range(len(df_combined)):\r\n",
        "          for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "            temp = \"\"\r\n",
        "            news = \"\"\r\n",
        "            for word in range(ROWS):\r\n",
        "              news = df_combined[combined_column_names[(column * ROWS) + (word + 1)]][row]\r\n",
        "              # Remove the b character at the begining of the string\r\n",
        "              if news[0] is \"b\":\r\n",
        "                news = \" \" + news[1:]\r\n",
        "              temp = temp + news\r\n",
        "            news_sum[column][row] = temp\r\n",
        "\r\n",
        "        # Drop the old columns\r\n",
        "        for column in range(len(combined_column_names) - 1):\r\n",
        "          df_combined.drop(combined_column_names[column + 1], axis = 1, inplace = True)\r\n",
        "\r\n",
        "        # Create the new columns with the merged news\r\n",
        "        for column in range(int((len(combined_column_names) - 1) / ROWS)):\r\n",
        "          colum_name = \"News_\" + str(column + 1)\r\n",
        "          df_combined[colum_name] = news_sum[column]          \r\n",
        "\r\n",
        "        # The label column \r\n",
        "        LABEL_COLUMN = 0\r\n",
        "\r\n",
        "        news_sum = []\r\n",
        "        label_sum = []\r\n",
        "\r\n",
        "        # Get the column names\r\n",
        "        combined_column_names = []\r\n",
        "        for column in df_combined.columns:\r\n",
        "          combined_column_names.append(column)\r\n",
        "\r\n",
        "        # Connect the merged news with the labels\r\n",
        "        for column in range(len(df_combined)):\r\n",
        "          for row in range(len(combined_column_names) - 1):\r\n",
        "            news_sum.append(df_combined[combined_column_names[row + 1]][column])\r\n",
        "            label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\r\n",
        "\r\n",
        "        # Create the new DataFrame\r\n",
        "        df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\r\n",
        "        df_sum_news_labels[\"News\"] = news_sum\r\n",
        "\r\n",
        "        # Removing punctuations\r\n",
        "        temp_news = []\r\n",
        "        for line in news_sum:\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if word not in string.punctuation:\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          temp_news.append(temp_attach)\r\n",
        "\r\n",
        "        news_sum = temp_news\r\n",
        "        temp_news = []\r\n",
        "\r\n",
        "        # Remove numbers\r\n",
        "        for line in news_sum:\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if not word.isdigit():\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          temp_news.append(temp_attach)\r\n",
        "\r\n",
        "        # Remove space\r\n",
        "        for line in range(len(temp_news)):    \r\n",
        "          temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "        # Converting headlines to lower case\r\n",
        "        for line in range(len(temp_news)): \r\n",
        "            temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "        # Update the data frame\r\n",
        "        df_sum_news_labels[\"News\"] = temp_news\r\n",
        "\r\n",
        "        # Load the stop words\r\n",
        "        stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "        filtered_sentence = []\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "\r\n",
        "        # Remove stop words\r\n",
        "        for line in news_sum:\r\n",
        "          word_tokens = word_tokenize(line)\r\n",
        "          temp_attach = \"\"\r\n",
        "          for word in word_tokens:\r\n",
        "            temp = \" \"\r\n",
        "            if not word in stop_words:\r\n",
        "              temp = temp + word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "          filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "        # Remove space\r\n",
        "        for line in range(len(filtered_sentence)):    \r\n",
        "          filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "        # Update the data frame\r\n",
        "        df_sum_news_labels[\"News\"] = filtered_sentence\r\n",
        "\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "        null_indexes = []\r\n",
        "        index = 0\r\n",
        "\r\n",
        "        for line in news_sum:\r\n",
        "          if line is \"\":\r\n",
        "            null_indexes.append(index)\r\n",
        "          index = index + 1\r\n",
        "\r\n",
        "        for row in null_indexes:\r\n",
        "          df_sum_news_labels = df_sum_news_labels.drop(row)\r\n",
        "\r\n",
        "        news_sum = df_sum_news_labels[\"News\"]\r\n",
        "        null_indexes = []\r\n",
        "        index = 0\r\n",
        "\r\n",
        "        for line in news_sum:\r\n",
        "          if line is \"\":\r\n",
        "            null_indexes.append(index)\r\n",
        "          index = index + 1\r\n",
        "          \r\n",
        "        assert len(null_indexes) is 0\r\n",
        "\r\n",
        "        # Do the shuffle\r\n",
        "        for i in range(SHUFFLE_CYCLE):\r\n",
        "          df_sum_news_labels = shuffle(df_sum_news_labels, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "        # Reset the index\r\n",
        "        df_sum_news_labels.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "        return df_sum_news_labels"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zgMLtiLx6e"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def split_to_train():\r\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "\r\n",
        "        # Split the dataset\r\n",
        "        train = df_sum_news_labels[:TRAIN_SIZE] \r\n",
        "\r\n",
        "        return train"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1lgt7dMBol"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    def split_to_test():\r\n",
        "        INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "        TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "\r\n",
        "        # Split the dataset\r\n",
        "        test = df_sum_news_labels[TRAIN_SIZE:]\r\n",
        "\r\n",
        "        return test"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq0Oo4iHZufG"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    for ROWS in rows_values:\r\n",
        "      \r\n",
        "        print(\"--------------------------------------------\\n\\nStart of the ROWS = \" \r\n",
        "          + str(ROWS) + \" sequence\\n\\n--------------------------------------------\\n\")\r\n",
        "        \r\n",
        "        model_type = []\r\n",
        "        result = []\r\n",
        "\r\n",
        "        df_sum_news_labels = preprocess()\r\n",
        "        train = split_to_train()\r\n",
        "        test = split_to_test()\r\n",
        "\r\n",
        "        # check\r\n",
        "        split_sum = len(train) + len(test)\r\n",
        "        sum = len(df_sum_news_labels)\r\n",
        "        assert split_sum == sum    \r\n",
        "\r\n",
        "        train_headlines = []\r\n",
        "        test_headlines = []\r\n",
        "\r\n",
        "        for row in range(0, len(train.index)):\r\n",
        "            train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "        for row in range(0,len(test.index)):\r\n",
        "            test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "        # show the first\r\n",
        "        print(train_headlines[0])\r\n",
        "\r\n",
        "        for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "            for n in range(1,MODEL_TYPE+1):\r\n",
        "                print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                      + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "                _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "                _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "                print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "                _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "                _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "                _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "                _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "                print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "                model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "                result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "        rows_summary_value.append(ROWS)\r\n",
        "\r\n",
        "        # save the best\r\n",
        "        best_model_rows = 0\r\n",
        "\r\n",
        "        for model in range(len(model_type)):\r\n",
        "            if result[model] > best_model_rows:\r\n",
        "                best_model_rows = result[model]\r\n",
        "\r\n",
        "        rows_summary_accuraccy.append(best_model_rows)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgc2E1npZvJC"
      },
      "source": [
        "Kiértékelés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUCZnMNfebo"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    best_model_rows = 0\r\n",
        "\r\n",
        "    for model in range(len(rows_summary_value)):\r\n",
        "        print(str(rows_summary_value[model]) + \":\\t\\t\\t\\t\\t\" \r\n",
        "              + str(rows_summary_accuraccy[model]))\r\n",
        "\r\n",
        "        if rows_summary_accuraccy[model] > best_model_rows:\r\n",
        "            best_model_rows = rows_summary_accuraccy[model]\r\n",
        "            best_model_rows_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest row value:\\n\" \r\n",
        "          + str(rows_summary_value[best_model_rows_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(rows_summary_accuraccy[best_model_rows_index]))"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-GyfELNcMas"
      },
      "source": [
        "A legjobb ROWS eredményeinek megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnMyo9Ljca4I"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\r\n",
        "\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "\r\n",
        "    df_sum_news_labels = preprocess()\r\n",
        "    train = split_to_train()\r\n",
        "    test = split_to_test()\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum    \r\n",
        "\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5fXYYfZm_7W"
      },
      "source": [
        "A legjobbhoz tartozó korrelációs tényezők megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk-qyHFInIOl"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    ROWS = int(rows_summary_value[best_model_rows_index])\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    df_sum_news_labels = preprocess()\r\n",
        "    train = split_to_train()\r\n",
        "    test = split_to_test()\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_sum_news_labels)\r\n",
        "    assert split_sum == sum    \r\n",
        "\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"Label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"Label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "    result.append(accuracy_score(test[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6Q8qicpSAP"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkcfT8TFpTlq"
      },
      "source": [
        "if DATASET == 1:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKZQkIdFAXfG"
      },
      "source": [
        "## **ECO_BSN_DF, ECO_FNC_DF, ECO_US_DF 2008-2016 (2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyZGBhb0CcgL"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\r\n",
        "\r\n",
        "Ezeket az adathalmazokat én magam gyűjtöttem az alábbi oldalakról:\r\n",
        "\r\n",
        "\r\n",
        "*   https://www.economist.com/business/ \r\n",
        "*   https://www.economist.com/finance-and-economics/ \r\n",
        "*   https://www.economist.com/united-states/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt8lzj6Qs9_e"
      },
      "source": [
        "### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fORGByWVtCtj"
      },
      "source": [
        "Először betöltöm külön-külön az adathalmazokat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVNdyg2RtCR-"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_BSN_DF.csv\" \"ECO_BSN_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_FNC_DF.csv\" \"ECO_FNC_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Economist/ECO_US_DF.csv\" \"ECO_US_DF.csv\"\r\n",
        "\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[0].find(\"ECO_FNC_DF.csv\") != -1\r\n",
        "    assert read[0].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \r\n",
        "    assert read[1].find(\"ECO_BSN_DF.csv\") != -1\r\n",
        "    assert read[1].find(\"ECO_US_DF.csv\") != -1\r\n",
        "\r\n",
        "    # Load the datasets \r\n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\r\n",
        "    df_bsn = pd.read_csv('ECO_BSN_DF.csv', index_col = \"date\")\r\n",
        "    df_fnc = pd.read_csv('ECO_FNC_DF.csv', index_col = \"date\")\r\n",
        "    df_us = pd.read_csv('ECO_US_DF.csv', index_col = \"date\")\r\n",
        "\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkYz3ExjupOh"
      },
      "source": [
        "Az adathalmazok megvizsgálása az elemein keresztül."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNBo9X2RuoaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660b3d24-8ceb-4d8c-8026-b1d803c526ec"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Show the dataframe\r\n",
        "    print(\"Reddit\")\r\n",
        "    print(df_reddit.head())\r\n",
        "    print(\"\\n\\nBSN ECO\")\r\n",
        "    print(df_bsn.head())\r\n",
        "    print(\"\\n\\nFNC ECO\")\r\n",
        "    print(df_fnc.head())\r\n",
        "    print(\"\\n\\nUS ECO\")\r\n",
        "    print(df_us.head())"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reddit\n",
            "            Label  ...                                              Top25\n",
            "Date               ...                                                   \n",
            "2008-08-08      0  ...           b\"No Help for Mexico's Kidnapping Surge\"\n",
            "2008-08-11      1  ...  b\"So this is what it's come to: trading sex fo...\n",
            "2008-08-12      0  ...  b\"BBC NEWS | Asia-Pacific | Extinction 'by man...\n",
            "2008-08-13      0  ...  b'2006: Nobel laureate Aleksander Solzhenitsyn...\n",
            "2008-08-14      1  ...  b'Philippines : Peace Advocate say Muslims nee...\n",
            "\n",
            "[5 rows x 26 columns]\n",
            "\n",
            "\n",
            "BSN ECO\n",
            "                                                        title\n",
            "date                                                         \n",
            "2021/02/13  Unions take on Amazon and Alphabet. Big tech w...\n",
            "2021/02/13      Narendra Modi promises to privatise Air India\n",
            "2021/02/13                             Diary of a plague year\n",
            "2021/02/13  For Deutschland AG, Brexit goes from bad to wurst\n",
            "2021/02/10  The cult of an Elon Musk or a Jack Ma has its ...\n",
            "\n",
            "\n",
            "FNC ECO\n",
            "                                                        title\n",
            "date                                                         \n",
            "2021/02/08  For the first time in a year, oil prices top $...\n",
            "2021/02/06  Chinese investors’ access to foreign assets ex...\n",
            "2021/02/06        High-frequency traders are in the spotlight\n",
            "2021/02/06  A new epoch for retail investors is just begin...\n",
            "2021/02/06                           How WallStreetBets works\n",
            "\n",
            "\n",
            "US ECO\n",
            "                                                        title\n",
            "date                                                         \n",
            "2021/02/27  Smuggled into the covid-relief bill is an over...\n",
            "2021/02/27  Covid-19 has boosted the campaign against exam...\n",
            "2021/02/27                    Time to reopen the school gates\n",
            "2021/02/27  Donald Trump thrived by painting Democrats as ...\n",
            "2021/02/27  Now America has passed 500,000 deaths, what next?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf587n4txlu2"
      },
      "source": [
        "Azon elemek megkeresése az ECO adathalmazból ami beleesik a vizsgált időintervallumba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm4PdT-TxyV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08946f10-d4c3-4679-f477-f5cbff3531a1"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_bsn_inspect = df_bsn[df_bsn.index < '2016/07/02']\r\n",
        "    df_bsn_inspect = df_bsn_inspect[df_bsn_inspect.index > '2008/08/07']\r\n",
        "    df_bsn_inspect = df_bsn_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_fnc_inspect = df_fnc[df_fnc.index < '2016/07/02']\r\n",
        "    df_fnc_inspect = df_fnc_inspect[df_fnc_inspect.index > '2008/08/07']\r\n",
        "    df_fnc_inspect = df_fnc_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_us_inspect = df_us[df_us.index < '2016/07/02']\r\n",
        "    df_us_inspect = df_us_inspect[df_us_inspect.index > '2008/08/07']\r\n",
        "    df_us_inspect = df_us_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    print(\"BSN ECO\")\r\n",
        "    print(df_bsn_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_bsn_inspect.tail(2))\r\n",
        "    print(df_bsn_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nFNC ECO\")\r\n",
        "    print(df_fnc_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_fnc_inspect.tail(2))\r\n",
        "    print(df_fnc_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nUS ECO\")\r\n",
        "    print(df_us_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_us_inspect.tail(2))\r\n",
        "    print(df_us_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_bsn_inspect) + len(df_fnc_inspect) + len(df_us_inspect)))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BSN ECO\n",
            "                         title\n",
            "date                          \n",
            "2016/06/30  Squeezing the tube\n",
            "2016/06/30  From clout to rout\n",
            "...\n",
            "                                    title\n",
            "date                                     \n",
            "2008/08/14                 Going for goal\n",
            "2008/08/12  Come fly the fee-filled skies\n",
            "(3014, 1)\n",
            "\n",
            "\n",
            "FNC ECO\n",
            "                              title\n",
            "date                               \n",
            "2016/06/30  Prophets and profiteers\n",
            "2016/06/30   The consensus crumbles\n",
            "...\n",
            "                              title\n",
            "date                               \n",
            "2008/08/14           Home economics\n",
            "2008/08/10  The bear and the donkey\n",
            "(3269, 1)\n",
            "\n",
            "\n",
            "US ECO\n",
            "                          title\n",
            "date                           \n",
            "2016/06/30     Exodus postponed\n",
            "2016/06/23  Gloria in expansion\n",
            "...\n",
            "                                title\n",
            "date                                 \n",
            "2008/08/14      The next Billy Graham\n",
            "2008/08/09  Home thoughts from abroad\n",
            "(3066, 1)\n",
            "\n",
            "\n",
            "Summary length:\t\t9349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCBuxng0j-S"
      },
      "source": [
        "Az Economist oldalról származó adathalmazok összefűzése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDOluCHa1ASA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3db829-a5b0-443c-ceac-4055347cf235"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_eco_all = pd.concat([df_bsn_inspect, df_fnc_inspect, df_us_inspect])\r\n",
        "\r\n",
        "    df_eco_all = df_eco_all.drop_duplicates()\r\n",
        "\r\n",
        "    df_eco_all.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    print(\"ECO MERGED\")\r\n",
        "    print(df_eco_all.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_eco_all.tail(2))\r\n",
        "    print(df_eco_all.shape)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ECO MERGED\n",
            "                                title\n",
            "date                                 \n",
            "2008/08/09  Home thoughts from abroad\n",
            "2008/08/10    The bear and the donkey\n",
            "...\n",
            "                         title\n",
            "date                          \n",
            "2016/06/30    Exodus postponed\n",
            "2016/06/30  Squeezing the tube\n",
            "(9183, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9cTQSlx263t"
      },
      "source": [
        "Egy naphoz tartozó azonos hírek vizsgálata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpmSZfz2-0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14d7995-368f-471d-b7c1-9e44847f5442"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Groupby by date\r\n",
        "    dates = df_eco_all.groupby(\"date\")\r\n",
        "\r\n",
        "    # Summary statistic\r\n",
        "    print(\"Max:\")\r\n",
        "    print(dates.describe().max())\r\n",
        "    print(\"\\n\\nMin:\")\r\n",
        "    print(dates.describe().min())"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max:\n",
            "title  count             67\n",
            "       unique            67\n",
            "       top       iRational?\n",
            "       freq               1\n",
            "dtype: object\n",
            "\n",
            "\n",
            "Min:\n",
            "title  count                       1\n",
            "       unique                      1\n",
            "       top       \"Rewarding failure”\n",
            "       freq                        1\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5DuBpOl4MWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d9e3fe-cefe-43b2-d86d-c21970f9f66c"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    dates_count = [] # for count\r\n",
        "    dates_dates = [] # for indexing\r\n",
        "    df_dates = dates.describe()\r\n",
        "\r\n",
        "    for row in range(len(df_dates)):\r\n",
        "        dates_count.append(len(dates.get_group(df_dates.index[row])))\r\n",
        "        dates_dates.append(dates.get_group(df_dates.index[row]).index[0])\r\n",
        "\r\n",
        "    df_group_dates = pd.DataFrame()\r\n",
        "    df_group_dates[\"date\"] = dates_dates\r\n",
        "    df_group_dates[\"count\"] = dates_count\r\n",
        "    df_group_dates.set_index(\"date\", inplace=True)\r\n",
        "    df_group_dates.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    print(df_group_dates.head())"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            count\n",
            "date             \n",
            "2008/08/09      1\n",
            "2008/08/10      1\n",
            "2008/08/12      1\n",
            "2008/08/14     19\n",
            "2008/08/15      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDcXZfr9CS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5daa2782-c7b1-4d68-86ca-12e76dba6cd5"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Groupby by date\r\n",
        "    counts = df_group_dates.groupby(\"count\")\r\n",
        "\r\n",
        "    keys = list(counts.groups.keys())\r\n",
        "\r\n",
        "    sum = 0\r\n",
        "\r\n",
        "    for key in keys:\r\n",
        "      sum = sum + key * len(counts.get_group(key))\r\n",
        "      print(\"Count: \" + str(key) + \"\\t\\t\" + str(len(counts.get_group(key))))\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary:\\t\\t\" + str(sum))  "
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count: 1\t\t319\n",
            "Count: 2\t\t125\n",
            "Count: 3\t\t45\n",
            "Count: 4\t\t15\n",
            "Count: 5\t\t8\n",
            "Count: 6\t\t10\n",
            "Count: 7\t\t14\n",
            "Count: 8\t\t12\n",
            "Count: 9\t\t12\n",
            "Count: 10\t\t12\n",
            "Count: 11\t\t18\n",
            "Count: 12\t\t17\n",
            "Count: 13\t\t12\n",
            "Count: 14\t\t13\n",
            "Count: 15\t\t16\n",
            "Count: 16\t\t13\n",
            "Count: 17\t\t23\n",
            "Count: 18\t\t28\n",
            "Count: 19\t\t24\n",
            "Count: 20\t\t42\n",
            "Count: 21\t\t40\n",
            "Count: 22\t\t46\n",
            "Count: 23\t\t38\n",
            "Count: 24\t\t36\n",
            "Count: 25\t\t19\n",
            "Count: 26\t\t7\n",
            "Count: 28\t\t2\n",
            "Count: 32\t\t1\n",
            "Count: 34\t\t1\n",
            "Count: 36\t\t1\n",
            "Count: 46\t\t1\n",
            "Count: 67\t\t1\n",
            "\n",
            "\n",
            "Summary:\t\t9183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1JU31uKqLnw"
      },
      "source": [
        "Az összefűzött hírekhez a címkék generálása a részvény árfolyama alapján."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O6BT2vQKnFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c63033-9d4b-43a0-dbbb-0fcb036f9f71"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    days = []\r\n",
        "    stock_days = []\r\n",
        "    wrong_days = []\r\n",
        "\r\n",
        "    # Create dates and remove duplicates\r\n",
        "    for day in range(len(df_eco_all.index)):\r\n",
        "        if day == 0:\r\n",
        "            days.append(str(df_eco_all.index[day]))\r\n",
        "        elif df_eco_all.index[day] != days[len(days) - 1]:\r\n",
        "            days.append(str(df_eco_all.index[day]))\r\n",
        "\r\n",
        "    # Drop not needed days\r\n",
        "    for day in range(len(df_stock.index)):\r\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\r\n",
        "\r\n",
        "    # Remove not relevant date\r\n",
        "    good_days = []\r\n",
        "    for day in days:\r\n",
        "        try:\r\n",
        "            if stock_days.index(day):\r\n",
        "                good_days.append(str(day))\r\n",
        "        except:\r\n",
        "            wrong_days.append(str(day))\r\n",
        "\r\n",
        "    print(\"All days:\\t\\t\" + str(len(days)))\r\n",
        "    print(\"Good days:\\t\\t\" + str(len(good_days)))\r\n",
        "    print(\"Wrong days:\\t\\t\" + str(len(wrong_days)))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All days:\t\t971\n",
            "Good days:\t\t666\n",
            "Wrong days:\t\t305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBpmP0WArssx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8605f2-b428-46c0-e4cd-550fd268841e"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    label_eco = []\r\n",
        "    date_label_eco =[]\r\n",
        "    title_label_eco = []\r\n",
        "\r\n",
        "    for day in range(len(good_days)):\r\n",
        "        if day == 0:\r\n",
        "            title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "            label_eco.append(0)\r\n",
        "            date_label_eco.append(good_days[day])      \r\n",
        "        # label should be 1 -> rise\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\r\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\r\n",
        "                    title_label_eco.append(row)\r\n",
        "                    label_eco.append(1)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "                    label_eco.append(1)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_eco_all[\"title\"][good_days[day]], str) is False:\r\n",
        "                for row in df_eco_all[\"title\"][good_days[day]]:\r\n",
        "                    title_label_eco.append(row)\r\n",
        "                    label_eco.append(0)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_eco.append(df_eco_all[\"title\"][good_days[day]])\r\n",
        "                    label_eco.append(0)\r\n",
        "                    date_label_eco.append(good_days[day])\r\n",
        "\r\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_eco)))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News with labels length:\t\t5069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ec36DKBX9F"
      },
      "source": [
        "A címkékkel rendelkező, használható adatokból egy új adathalmaz létrehozása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEXYSpdbBVk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe29252-389a-46a7-d035-2b1c68e7eb77"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    df_eco = pd.DataFrame()\r\n",
        "    df_eco[\"date\"] = date_label_eco\r\n",
        "    df_eco[\"label\"] = label_eco\r\n",
        "    df_eco[\"title\"] = title_label_eco\r\n",
        "    df_eco.set_index(\"date\", inplace=True)\r\n",
        "    df_eco.sort_index(ascending=True, inplace=True)\r\n",
        "    print(df_eco.head())\r\n",
        "    print(len(df_eco))\r\n",
        "\r\n",
        "    # drop duplicates\r\n",
        "    df_eco.drop_duplicates(subset=\"title\", inplace=True)\r\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\r\n",
        "    print(df_eco.head())\r\n",
        "    print(len(df_eco))    "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            label                          title\n",
            "date                                            \n",
            "2008/08/12      0  Come fly the fee-filled skies\n",
            "2008/08/14      1              Kicked in the ARS\n",
            "2008/08/14      1          The next Billy Graham\n",
            "2008/08/14      1     Another inconvenient truth\n",
            "2008/08/14      1                 Phantom menace\n",
            "5069\n",
            "\n",
            "\n",
            " ----- Drop duplicate title -----\n",
            "\n",
            "            label                          title\n",
            "date                                            \n",
            "2008/08/12      0  Come fly the fee-filled skies\n",
            "2008/08/14      1              Kicked in the ARS\n",
            "2008/08/14      1          The next Billy Graham\n",
            "2008/08/14      1     Another inconvenient truth\n",
            "2008/08/14      1                 Phantom menace\n",
            "5069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREcqTfFmUwJ"
      },
      "source": [
        "### Adathalmazok előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa5nBTd8OFpB"
      },
      "source": [
        "Az adathalmaz megtisztítása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy7GYSLxOIrX",
        "outputId": "b85aa48f-9c6d-4b95-afbf-6001e8cec1b6"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      try:\r\n",
        "          for word in line:\r\n",
        "            temp = \" \"\r\n",
        "            if word not in string.punctuation:\r\n",
        "              temp = word\r\n",
        "            temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      except:\r\n",
        "          temp = \" \"\r\n",
        "          temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_eco[\"title\"] = temp_news\r\n",
        "\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_eco[\"title\"] = filtered_sentence\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_eco.reset_index(inplace=True)\r\n",
        "\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "\r\n",
        "    print(null_indexes)\r\n",
        "\r\n",
        "    for row in range(len(null_indexes)):\r\n",
        "      df_eco = df_eco.drop(df_eco.index[null_indexes[row] - row])\r\n",
        "\r\n",
        "    news_sum = df_eco[\"title\"]\r\n",
        "    null_indexes = []\r\n",
        "    index = 0\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      if line is \"\":\r\n",
        "        null_indexes.append(index)\r\n",
        "      index = index + 1\r\n",
        "      \r\n",
        "    assert len(null_indexes) is 0"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[79, 188, 722, 1417, 1983, 2716, 3105, 3214, 3855, 4094, 4335, 4950]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_b24C3cmZIQ"
      },
      "source": [
        "Az adathalmaz szétbontása tanító és tesztelő adathalmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0BpPPB_mYAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ca2bf5-e194-4423-8971-66379801365a"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    # Drop the dates\r\n",
        "    df_eco_label_title = pd.DataFrame()\r\n",
        "    df_eco_label_title[\"label\"] = df_eco[\"label\"]\r\n",
        "    df_eco_label_title[\"title\"] = df_eco[\"title\"]\r\n",
        "    print(\"New dataset without the dates\")\r\n",
        "    print(df_eco_label_title.head())\r\n",
        "    print(len(df_eco_label_title))\r\n",
        "\r\n",
        "    # Do the shuffle\r\n",
        "    for i in range(SHUFFLE_CYCLE):\r\n",
        "      df_eco_label_title = shuffle(df_eco_label_title, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_eco_label_title.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    print(\"\\n\\nAfter shuffle\")\r\n",
        "    print(df_eco_label_title.head())    \r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    INPUT_SIZE = len(df_eco_label_title)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    train = df_eco_label_title[:TRAIN_SIZE] \r\n",
        "    test = df_eco_label_title[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"\\n\\nAfter split\")\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_eco_label_title)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_eco_label_title)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New dataset without the dates\n",
            "   label                       title\n",
            "0      0   come fly fee filled skies\n",
            "1      1                  kicked ars\n",
            "2      1           next billy graham\n",
            "3      1  another inconvenient truth\n",
            "4      1              phantom menace\n",
            "5057\n",
            "\n",
            "\n",
            "After shuffle\n",
            "   label                         title\n",
            "0      0                   close power\n",
            "1      0                 russian bears\n",
            "2      1                    money shot\n",
            "3      1                 stuck neutral\n",
            "4      1  one listens jürgen grossmann\n",
            "\n",
            "\n",
            "After split\n",
            "Train data set length: 4298\n",
            "Test data set length: 759\n",
            "Split summa: 5057\n",
            "Dataset summa before split: 5057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmLc118-ozZo"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTDyz6azo5H7"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCwyjWHNo3Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5d7fd6-b19b-429c-9128-e64144df8c71"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "    model_type_values = []\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    # Create model type values\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    print(\"\\n\\n\")\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,1 gram model\n",
            "\n",
            "The shape is: (4298, 4955)\n",
            "\n",
            "0.5362318840579711\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,2 gram model\n",
            "\n",
            "The shape is: (4298, 10406)\n",
            "\n",
            "0.5362318840579711\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,2 gram model\n",
            "\n",
            "The shape is: (4298, 5451)\n",
            "\n",
            "0.5349143610013175\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,3 gram model\n",
            "\n",
            "The shape is: (4298, 12212)\n",
            "\n",
            "0.544137022397892\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,3 gram model\n",
            "\n",
            "The shape is: (4298, 7257)\n",
            "\n",
            "0.5349143610013175\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,3 gram model\n",
            "\n",
            "The shape is: (4298, 1806)\n",
            "\n",
            "0.538866930171278\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,4 gram model\n",
            "\n",
            "The shape is: (4298, 12793)\n",
            "\n",
            "0.544137022397892\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,4 gram model\n",
            "\n",
            "The shape is: (4298, 7838)\n",
            "\n",
            "0.5349143610013175\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,4 gram model\n",
            "\n",
            "The shape is: (4298, 2387)\n",
            "\n",
            "0.538866930171278\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,4 gram model\n",
            "\n",
            "The shape is: (4298, 581)\n",
            "\n",
            "0.5375494071146245\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,5 gram model\n",
            "\n",
            "The shape is: (4298, 13023)\n",
            "\n",
            "0.544137022397892\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,5 gram model\n",
            "\n",
            "The shape is: (4298, 8068)\n",
            "\n",
            "0.5349143610013175\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,5 gram model\n",
            "\n",
            "The shape is: (4298, 2617)\n",
            "\n",
            "0.538866930171278\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,5 gram model\n",
            "\n",
            "The shape is: (4298, 811)\n",
            "\n",
            "0.5375494071146245\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,5 gram model\n",
            "\n",
            "The shape is: (4298, 230)\n",
            "\n",
            "0.5375494071146245\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 1,6 gram model\n",
            "\n",
            "The shape is: (4298, 13132)\n",
            "\n",
            "0.544137022397892\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 2,6 gram model\n",
            "\n",
            "The shape is: (4298, 8177)\n",
            "\n",
            "0.5349143610013175\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 3,6 gram model\n",
            "\n",
            "The shape is: (4298, 2726)\n",
            "\n",
            "0.538866930171278\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 4,6 gram model\n",
            "\n",
            "The shape is: (4298, 920)\n",
            "\n",
            "0.5375494071146245\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 5,6 gram model\n",
            "\n",
            "The shape is: (4298, 339)\n",
            "\n",
            "0.5375494071146245\n",
            "--------------------------------------------\n",
            "\n",
            "Start of the 6,6 gram model\n",
            "\n",
            "The shape is: (4298, 109)\n",
            "\n",
            "0.5375494071146245\n",
            "\n",
            "\n",
            "\n",
            "1,1 n-gram:\t\t\t\t\t0.5362318840579711\n",
            "1,2 n-gram:\t\t\t\t\t0.5362318840579711\n",
            "2,2 n-gram:\t\t\t\t\t0.5349143610013175\n",
            "1,3 n-gram:\t\t\t\t\t0.544137022397892\n",
            "2,3 n-gram:\t\t\t\t\t0.5349143610013175\n",
            "3,3 n-gram:\t\t\t\t\t0.538866930171278\n",
            "1,4 n-gram:\t\t\t\t\t0.544137022397892\n",
            "2,4 n-gram:\t\t\t\t\t0.5349143610013175\n",
            "3,4 n-gram:\t\t\t\t\t0.538866930171278\n",
            "4,4 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "1,5 n-gram:\t\t\t\t\t0.544137022397892\n",
            "2,5 n-gram:\t\t\t\t\t0.5349143610013175\n",
            "3,5 n-gram:\t\t\t\t\t0.538866930171278\n",
            "4,5 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "5,5 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "1,6 n-gram:\t\t\t\t\t0.544137022397892\n",
            "2,6 n-gram:\t\t\t\t\t0.5349143610013175\n",
            "3,6 n-gram:\t\t\t\t\t0.538866930171278\n",
            "4,6 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "5,6 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "6,6 n-gram:\t\t\t\t\t0.5375494071146245\n",
            "--------------------------------------------\n",
            "Best model:\n",
            "1,3 n-gram\t\t\t\t\t0.544137022397892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9gbVqadzbUd",
        "outputId": "d9eef6ca-e130-4406-92fa-595c913e9d27"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "close power\n",
            "The shape is: (4298, 12212)\n",
            "\n",
            "0.544137022397892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU4nnwBzzmOF",
        "outputId": "dce8cfd8-1a18-41de-edcc-41eae15d87d5"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Word  Coefficient\n",
            "2751         debt     1.097224\n",
            "10891    thinking     1.040934\n",
            "7128       nation     0.967841\n",
            "8955         ride     0.953603\n",
            "10181       stand     0.936049\n",
            "9643         show     0.909019\n",
            "6433         make     0.902951\n",
            "222      american     0.885956\n",
            "11422  university     0.884117\n",
            "6106       lights     0.875793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXZ75tt2zobK",
        "outputId": "4af51368-a4a6-4109-ea1c-9467a65280e3"
      },
      "source": [
        "if DATASET == 2:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              Word  Coefficient\n",
            "1327         brink    -0.825932\n",
            "7503        office    -0.827585\n",
            "5744          kind    -0.835123\n",
            "170    alternative    -0.836957\n",
            "11393      unhappy    -0.837685\n",
            "9278         saved    -0.854158\n",
            "5639         judge    -0.882916\n",
            "11149      trickle    -0.895313\n",
            "9005          risk    -0.905173\n",
            "6154        liquid    -1.155973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wcHp91ptLF-"
      },
      "source": [
        "## **KAG_BENZ_ANALYST_DF, KAG_BENZ_PARTNER_DF 2008-2016**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG8XoC5ltwPe"
      },
      "source": [
        "Megvizsgálom a reddit-es világhírekkel megegyező intervallumon ezeket az összevont adathalmazokat, majd egyesítve és kombinálva a kettőt megvizsgálom, hogy javítja-e a pontosságot.\r\n",
        "\r\n",
        "Ezen adathalmazok forrása:\r\n",
        "\r\n",
        "*   https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPbi6d_zLCn"
      },
      "source": [
        "#### Adathalmazok betöltése"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pi6pacf80r5"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Copy the dataset to the local environment\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Reddit Top 25 DJIA/KAG_REDDIT_WRLD_DJIA_DF_corrected.csv\" \"KAG_REDDIT_WRLD_DJIA_DF.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_1.csv\" \"KAG_BENZ_ANALYST_DF_1.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_ANALYST_DF_2.csv\" \"KAG_BENZ_ANALYST_DF_2.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_1.csv\" \"KAG_BENZ_PARTNER_DF_1.csv\"\r\n",
        "    !cp \"/content/drive/MyDrive/Kaggle dataset/Benzinga news with ticker/KAG_BENZ_PARTNER_DF_2.csv\" \"KAG_BENZ_PARTNER_DF_2.csv\"\r\n",
        "\r\n",
        "\r\n",
        "    # Check the copy is succesfull -> good if no assertation error\r\n",
        "    read = !ls\r\n",
        "    assert read[1].find(\"KAG_BENZ_ANALYST_DF_1.csv\") != -1\r\n",
        "    assert read[2].find(\"KAG_REDDIT_WRLD_DJIA_DF.csv\") != -1    \r\n",
        "    assert read[2].find(\"KAG_BENZ_ANALYST_DF_2.csv\") != -1\r\n",
        "    assert read[0].find(\"KAG_BENZ_PARTNER_DF_1.csv\") != -1    \r\n",
        "    assert read[1].find(\"KAG_BENZ_PARTNER_DF_2.csv\") != -1\r\n",
        "\r\n",
        "    # Load the datasets \r\n",
        "    df_reddit = pd.read_csv('KAG_REDDIT_WRLD_DJIA_DF.csv', index_col = \"Date\")\r\n",
        "    df_benz_1 = pd.read_csv('KAG_BENZ_ANALYST_DF_1.csv', index_col = \"date\")\r\n",
        "    df_benz_2 = pd.read_csv('KAG_BENZ_ANALYST_DF_2.csv', index_col = \"date\")    \r\n",
        "    df_partner_1 = pd.read_csv('KAG_BENZ_PARTNER_DF_1.csv', index_col = \"date\")\r\n",
        "    df_partner_2 = pd.read_csv('KAG_BENZ_PARTNER_DF_2.csv', index_col = \"date\")\r\n",
        "\r\n",
        "    # Load the stock data\r\n",
        "    df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                              end=\"2016-07-01\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU771dZOCrfM"
      },
      "source": [
        "A szétbontott adathalmazok összefűzése, majd azok megjelenítése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1W5V63vCyA4"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Merge them\r\n",
        "    df_benz = pd.concat([df_benz_1, df_benz_2])\r\n",
        "    df_partner = pd.concat([df_partner_1, df_partner_2])\r\n",
        "\r\n",
        "    # Show the dataframe\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(len(df_benz))\r\n",
        "    print(\"\\n\\nPARTNER\")\r\n",
        "    print(df_partner.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_partner.tail())\r\n",
        "    print(len(df_partner))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxza21dEGyDD"
      },
      "source": [
        "A vizsgált időtartamba eső adatok kiszűrése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmab7DBG5cB"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz_inspect = df_benz[df_benz.index < '2016/07/02']\r\n",
        "    df_benz_inspect = df_benz_inspect[df_benz_inspect.index > '2008/08/07']\r\n",
        "    df_benz_inspect = df_benz_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    df_partner_inspect = df_partner[df_partner.index < '2016/07/02']\r\n",
        "    df_partner_inspect = df_partner_inspect[df_partner_inspect.index > '2008/08/07']\r\n",
        "    df_partner_inspect = df_partner_inspect.drop_duplicates()\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz_inspect.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz_inspect.tail())\r\n",
        "    print(df_benz_inspect.shape)\r\n",
        "\r\n",
        "    print(\"\\n\\nPARTNER\")\r\n",
        "    print(df_partner_inspect.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_partner_inspect.tail())\r\n",
        "    print(df_partner_inspect.shape)\r\n",
        "\r\n",
        "    df_benz = pd.concat([df_benz_inspect, df_partner_inspect])\r\n",
        "\r\n",
        "    print(\"\\n\\nSummary length:\\t\\t\" + str(len(df_benz_inspect) + len(df_partner_inspect)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UngRtIFVpi"
      },
      "source": [
        "Az adathalmazban található adatok kiszűrése a Dow Jonews Industrial Average alapján:\r\n",
        "\r\n",
        "\r\n",
        "*   Procter & Gamble, PG, 1932-05-26\r\n",
        "*   3M Company, MMM, 1976-08-09\r\n",
        "*   IBM, IBM, 1979-06-29\r\n",
        "*   Merck & Co., MRK, 1979-06-29\r\n",
        "*   American Express, AXP, 1982-08-30\r\n",
        "*   McDonald's, MCD, 1985-10-30\r\n",
        "*   Boeing, BA, 1987-03-12\r\n",
        "*   The Coca-Cola Company, KO, 1987-03-12\r\n",
        "*   Caterpillar Inc., CAT, 1991-05-06\r\n",
        "*   JPMorgan Chase, JPM, 1991-05-06\r\n",
        "*   The Walt Disney Company, DIS, 1991-05-06\r\n",
        "*   Johnson & Johnson, JNJ, 1997-03-17\r\n",
        "*   Walmart, WMT, 1997-03-17\r\n",
        "*   The Home Depot, HD, 1999-11-01\r\n",
        "*   Intel, INTC, 1999-11-01\r\n",
        "*   Microsoft, MSFT, 1999-11-01\r\n",
        "*   Verizon, VZ, 2004-04-08\r\n",
        "*   Chevron Corporation, CVX, 2008-02-19\r\n",
        "*   Cisco Systems, CSCO, 2009-06-08\r\n",
        "*   The Travelers Companies, \tTRV, 2009-06-08\r\n",
        "*   UnitedHealth Group, UNH, \t2012-09-24\r\n",
        "*   Goldman Sachs, GS, \t2013-09-20\t\r\n",
        "*   Nike, NKE, 2013-09-20\r\n",
        "*   Visa Inc., \tV, 2013-09-20\t\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaU5ro8Y1vl"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    tickers = []\r\n",
        "    unique_count = []\r\n",
        "\r\n",
        "    # The stock tickers which is needed\r\n",
        "    stock_ticker = [\"PG\", \"MMM\", \"IBM\", \"MRK\", \"AXP\", \"MCD\", \"BA\", \"KO\", \"CAT\", \"JPM\",\r\n",
        "                    \"DIS\", \"JNJ\", \"WMT\", \"HD\", \"INTC\", \"MSFT\", \"VZ\", \"CVX\", \"CSCO\",\r\n",
        "                    \"TRV\", \"UNH\", \"GS\", \"NKE\", \"V\"]\r\n",
        "\r\n",
        "    stocks_benz = df_benz.groupby(\"stock\")\r\n",
        "    stock_benz_df = stocks_benz.describe()\r\n",
        "\r\n",
        "    for stock in range(len(stock_benz_df.index)):\r\n",
        "        tickers.append(stock_benz_df.index[stock])\r\n",
        "\r\n",
        "    for stock in stock_ticker:\r\n",
        "        try:\r\n",
        "            unique_count.append(stock_benz_df.iloc[tickers.index(stock), :][1]) #unique\r\n",
        "        except:\r\n",
        "            unique_count.append(0)\r\n",
        "            print(str(stock) + \"\\tis not in list\")\r\n",
        "\r\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\r\n",
        "\r\n",
        "    sum_count = 0\r\n",
        "\r\n",
        "    for stock in range(len(stock_ticker)):\r\n",
        "        print(str(stock_ticker[stock]) + \"\\t\\t\\t\" + str(unique_count[stock]))\r\n",
        "        sum_count = sum_count + unique_count[stock]\r\n",
        "\r\n",
        "    print(\"\\n\\t---------------------------------------\\n\")\r\n",
        "    print(\"Summary of news with the tickers:\\t\" + str(sum_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okO_8su2ABZL"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz_filtered = pd.DataFrame()\r\n",
        "\r\n",
        "    for stock in stock_ticker:\r\n",
        "        df_temp = df_benz[(df_benz[\"stock\"]) == stock].drop_duplicates()\r\n",
        "        df_benz_filtered = pd.concat([df_benz_filtered, df_temp])\r\n",
        "\r\n",
        "    df_benz_filtered.sort_index(ascending=True, inplace=True)\r\n",
        "\r\n",
        "    df_benz = df_benz_filtered\r\n",
        "    df_benz.drop(\"stock\", axis = 1, inplace = True)\r\n",
        "    df_benz.drop_duplicates(inplace=True)\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail(2))\r\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_fkeQjJPtw"
      },
      "source": [
        "### A szöveg előkészítése"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVB_ZTTqKTYR"
      },
      "source": [
        "A szöveg előfeldolgozása következik, mint az írásjelek eltűvolítása, a számok eltávolítása, felesleges szóközöktől való megtisztítás, minden szó kisbetűs szóra cserélése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_03-r1XOKWsY"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Removing punctuations\r\n",
        "    temp_news = []\r\n",
        "    news_sum = df_benz[\"headline\"]\r\n",
        "\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if word not in string.punctuation:\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    news_sum = temp_news\r\n",
        "    temp_news = []\r\n",
        "\r\n",
        "    # Remove numbers\r\n",
        "    for line in news_sum:\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in line:\r\n",
        "        temp = \" \"\r\n",
        "        if not word.isdigit():\r\n",
        "          temp = word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      temp_news.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(temp_news)):    \r\n",
        "      temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "    # Converting headlines to lower case\r\n",
        "    for line in range(len(temp_news)): \r\n",
        "        temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_benz[\"headline\"] = temp_news\r\n",
        "\r\n",
        "    # Load the stop words\r\n",
        "    stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "    filtered_sentence = []\r\n",
        "    news_sum = df_benz[\"headline\"]\r\n",
        "\r\n",
        "    # Remove stop words\r\n",
        "    for line in news_sum:\r\n",
        "      word_tokens = word_tokenize(line)\r\n",
        "      temp_attach = \"\"\r\n",
        "      for word in word_tokens:\r\n",
        "        temp = \" \"\r\n",
        "        if not word in stop_words:\r\n",
        "          temp = temp + word\r\n",
        "        temp_attach = temp_attach + \"\".join(temp)\r\n",
        "      filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "    # Remove space\r\n",
        "    for line in range(len(filtered_sentence)):    \r\n",
        "      filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "    # Update the data frame\r\n",
        "    df_benz[\"headline\"] = filtered_sentence\r\n",
        "\r\n",
        "    print(\"BENZ\")\r\n",
        "    print(df_benz.head(2))\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail(2))\r\n",
        "    print(df_benz.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cxxhSMNJSP-"
      },
      "source": [
        "### Címke létrehozása, adathalmaz felbontása"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bgzTSWYLV1r"
      },
      "source": [
        "A következőkben a címkék generálása és az adathalmaz felbontása történik tanító és validáló halmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDFCry1TLbYn"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    days = []\r\n",
        "    stock_days = []\r\n",
        "    wrong_days = []\r\n",
        "\r\n",
        "    # Create dates and remove duplicates\r\n",
        "    for day in range(len(df_benz.index)):\r\n",
        "        temp = str(df_benz.index[day])[0:10].replace(\"-\",\"/\")\r\n",
        "        if day == 0:\r\n",
        "            days.append(temp)\r\n",
        "        elif df_benz.index[day] != df_benz.index[day - 1]:\r\n",
        "            days.append(temp)\r\n",
        "\r\n",
        "    # Update the dataframe date column\r\n",
        "    df_benz.reset_index(inplace=True)\r\n",
        "    temp_days = df_benz[\"date\"]\r\n",
        "    days_to_update = []\r\n",
        "    for date in range(len(temp_days)):\r\n",
        "        temp = str(temp_days[date])[0:10].replace(\"-\",\"/\")\r\n",
        "        days_to_update.append(temp)\r\n",
        "\r\n",
        "    df_benz[\"date\"] = days_to_update\r\n",
        "    df_benz.set_index(\"date\", inplace=True, drop=True)    \r\n",
        "\r\n",
        "    # Drop not needed days\r\n",
        "    for day in range(len(df_stock.index)):\r\n",
        "        stock_days.append(str(df_stock.index[day])[0:10].replace(\"-\",\"/\"))\r\n",
        "\r\n",
        "    # Remove not relevant date\r\n",
        "    good_days = []\r\n",
        "    for day in days:\r\n",
        "        try:\r\n",
        "            if stock_days.index(day):\r\n",
        "                good_days.append(str(day))\r\n",
        "        except:\r\n",
        "            wrong_days.append(str(day))\r\n",
        "\r\n",
        "    print(\"All days:\\t\\t\\t\\t\" + str(len(days)))\r\n",
        "    print(\"Good days:\\t\\t\\t\\t\" + str(len(good_days)))\r\n",
        "    print(\"Wrong days:\\t\\t\\t\\t\" + str(len(wrong_days)))\r\n",
        "\r\n",
        "    label_benz = []\r\n",
        "    date_label_benz =[]\r\n",
        "    title_label_benz = []\r\n",
        "\r\n",
        "    for day in range(len(good_days)):\r\n",
        "        if day == 0:\r\n",
        "            title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "            label_benz.append(0)\r\n",
        "            date_label_benz.append(good_days[day])      \r\n",
        "        # label should be 1 -> rise\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) >= int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\r\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\r\n",
        "                    title_label_benz.append(row)\r\n",
        "                    label_benz.append(1)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "                    label_benz.append(1)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "\r\n",
        "        # label should be 0 -> fall\r\n",
        "        elif int(df_stock[\"Adj Close\"][stock_days.index(good_days[day])]) < int(df_stock[\"Adj Close\"][stock_days.index(good_days[day]) - 1]):   \r\n",
        "            if isinstance(df_benz[\"headline\"][good_days[day]], str) is False:\r\n",
        "                for row in df_benz[\"headline\"][good_days[day]]:\r\n",
        "                    title_label_benz.append(row)\r\n",
        "                    label_benz.append(0)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "            else:\r\n",
        "                    title_label_benz.append(df_benz[\"headline\"][good_days[day]])\r\n",
        "                    label_benz.append(0)\r\n",
        "                    date_label_benz.append(good_days[day])\r\n",
        "\r\n",
        "    print(\"News with labels length:\\t\\t\" + str(len(label_benz)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ8BBw1Zok7W"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    df_benz = pd.DataFrame()\r\n",
        "    df_benz[\"date\"] = date_label_benz\r\n",
        "    df_benz[\"label\"] = label_benz\r\n",
        "    df_benz[\"title\"] = title_label_benz\r\n",
        "    df_benz.set_index(\"date\", inplace=True)\r\n",
        "    df_benz.sort_index(ascending=True, inplace=True)\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(df_benz.shape)\r\n",
        "\r\n",
        "    # drop duplicates\r\n",
        "    df_benz.drop_duplicates(subset=\"title\", inplace=True)\r\n",
        "    print(\"\\n\\n ----- Drop duplicate title -----\\n\")\r\n",
        "    print(df_benz.head())\r\n",
        "    print(\"...\")\r\n",
        "    print(df_benz.tail())\r\n",
        "    print(df_benz.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dq2UTnqrJAw"
      },
      "source": [
        "Az adathalmaz felbontása."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSkAzvfVrWGJ"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    # Drop the dates\r\n",
        "    df_benz_label_title = pd.DataFrame()\r\n",
        "    df_benz_label_title[\"label\"] = df_benz[\"label\"]\r\n",
        "    df_benz_label_title[\"title\"] = df_benz[\"title\"]\r\n",
        "    # Reset the index\r\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\r\n",
        "    print(\"New dataset without the dates\")\r\n",
        "    print(df_benz_label_title.head())\r\n",
        "    print(len(df_benz_label_title))\r\n",
        "\r\n",
        "    # Do the shuffle\r\n",
        "    for i in range(SHUFFLE_CYCLE):\r\n",
        "      df_benz_label_title = shuffle(df_benz_label_title, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "    # Reset the index\r\n",
        "    df_benz_label_title.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "    # Show the data frame\r\n",
        "    print(\"\\n\\nAfter shuffle\")\r\n",
        "    print(df_benz_label_title.head())    \r\n",
        "\r\n",
        "    # Split the dataset\r\n",
        "    INPUT_SIZE = len(df_benz_label_title)\r\n",
        "    TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "    TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "    train = df_benz_label_title[:TRAIN_SIZE] \r\n",
        "    test = df_benz_label_title[TRAIN_SIZE:]\r\n",
        "\r\n",
        "    # Print out the length\r\n",
        "    print(\"\\n\\nAfter split\")\r\n",
        "    print(\"Train data set length: \" + str(len(train)))\r\n",
        "    print(\"Test data set length: \" + str(len(test)))\r\n",
        "    print(\"Split summa: \" + str(len(train) + len(test)))\r\n",
        "    print(\"Dataset summa before split: \" + str(len(df_benz_label_title)))\r\n",
        "\r\n",
        "    # check\r\n",
        "    split_sum = len(train) + len(test)\r\n",
        "    sum = len(df_benz_label_title)\r\n",
        "    assert split_sum == sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQA38WaDJWul"
      },
      "source": [
        "### n-gram modell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNGrMzP5r88E"
      },
      "source": [
        "Automatikus tanítás és eredmények megjelenítése a legmagasabb korrelációs tényezőjű szavakkal együtt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO8dmAdhr-28"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    model_type = []\r\n",
        "    result = []\r\n",
        "    model_type_values = []\r\n",
        "    train_headlines = []\r\n",
        "    test_headlines = []\r\n",
        "\r\n",
        "    # Create model type values\r\n",
        "    for value in range(1,7):\r\n",
        "        model_type_values.append(value)\r\n",
        "\r\n",
        "    for row in range(0, len(train.index)):\r\n",
        "        train_headlines.append(train.iloc[row, 1])\r\n",
        "\r\n",
        "    for row in range(0,len(test.index)):\r\n",
        "        test_headlines.append(test.iloc[row, 1])\r\n",
        "\r\n",
        "\r\n",
        "    for MODEL_TYPE in model_type_values:\r\n",
        "\r\n",
        "        for n in range(1,MODEL_TYPE+1):\r\n",
        "            print(\"--------------------------------------------\\n\\nStart of the \" \r\n",
        "                  + str(n) + \",\" + str(MODEL_TYPE) + \" gram model\\n\")\r\n",
        "\r\n",
        "            _gram_vectorizer_ = CountVectorizer(ngram_range=(n,MODEL_TYPE))\r\n",
        "            _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "            print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "            _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "            _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "            _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "            _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "            print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "            model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "            result.append(accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    best_model_gram = 0\r\n",
        "\r\n",
        "    print(\"\\n\\n\")\r\n",
        "    for model in range(len(model_type)):\r\n",
        "        print(str(model_type[model]) + \":\\t\\t\\t\\t\\t\" + str(result[model]))\r\n",
        "\r\n",
        "        if result[model] > best_model_gram:\r\n",
        "            best_model_gram = result[model]\r\n",
        "            best_model_gram_index = model\r\n",
        "\r\n",
        "    print(\"--------------------------------------------\\nBest model:\\n\" \r\n",
        "          + str(model_type[best_model_gram_index]) + \"\\t\\t\\t\\t\\t\" + \r\n",
        "          str(result[best_model_gram_index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCTns-8JsVZp"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    MODEL_TYPE = str(model_type[best_model_gram_index])\r\n",
        "\r\n",
        "    # show the first\r\n",
        "    print(train_headlines[0])\r\n",
        "\r\n",
        "    _gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "    _train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "    print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "    _gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "    _gram_model_ = _gram_model_.fit(_train_vectorizer_, train[\"label\"])\r\n",
        "\r\n",
        "    _gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "    _gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "    print (accuracy_score(test[\"label\"], _gram_predictions_))\r\n",
        "\r\n",
        "    model_type.append(str(n) + \",\" + str(MODEL_TYPE) + \" n-gram\")\r\n",
        "    result.append(accuracy_score(test[\"label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIGQBKMgsYID"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    _gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "    _gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "    coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                            'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "    coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "    print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIsCcJzgsa5W"
      },
      "source": [
        "if DATASET == 3:\r\n",
        "    print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}