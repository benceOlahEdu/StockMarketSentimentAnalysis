{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news+numerical.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1F1SzLTr5naX",
        "l29F9PKM6Nd6",
        "3GivJd0P6REl",
        "kUHFe6KX6VAd",
        "z-qGOxjyBX-y",
        "34wgxeDrBZ64",
        "7MZ3w8hQBbmM",
        "4_5FEqa0BeMN",
        "NXqMz0ELBf5r"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b31c8bdb3e9c4c729041747760361eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_710bb8189865444a93fdde5bc060a641",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8616b0eb9c84466ba40fe36d6290b82",
              "IPY_MODEL_cc16afea2bba473da26b0beeec05d0c0",
              "IPY_MODEL_da95b20fb9654df9b6007e0295cb3cff"
            ]
          }
        },
        "710bb8189865444a93fdde5bc060a641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8616b0eb9c84466ba40fe36d6290b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be516f3eed9249c7942f68a4d4041b45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eee901ad23f249de81be20b38874d3ee"
          }
        },
        "cc16afea2bba473da26b0beeec05d0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9faa0d8bb43d4a15b923ca3adb5bdb88",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27955de2797f43a39a75e415e2ac5b19"
          }
        },
        "da95b20fb9654df9b6007e0295cb3cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1aa030c956794a9e980674b3f54630f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 821kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e7f71a46d564c8c8568020f9400abed"
          }
        },
        "be516f3eed9249c7942f68a4d4041b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eee901ad23f249de81be20b38874d3ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9faa0d8bb43d4a15b923ca3adb5bdb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27955de2797f43a39a75e415e2ac5b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aa030c956794a9e980674b3f54630f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e7f71a46d564c8c8568020f9400abed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68d94616056541098426622b9ed4a99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e80d8332a22c4578bbd6fe815d65b77c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_38f43111c3ce4f16bc7ebf75518d7e10",
              "IPY_MODEL_c976ece2e42b4c0e8bccae3703e62d00",
              "IPY_MODEL_aa2c904fed6d46a1918f211f8ac5f599"
            ]
          }
        },
        "e80d8332a22c4578bbd6fe815d65b77c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38f43111c3ce4f16bc7ebf75518d7e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f8b7231f72f42458d92e4b0a429bade",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e8480d86d984be3b1a0b4efe39ccfb4"
          }
        },
        "c976ece2e42b4c0e8bccae3703e62d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b40917a9e7e04321b91ce4760b344534",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_213fa74ced40400897782d6716462b1d"
          }
        },
        "aa2c904fed6d46a1918f211f8ac5f599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2af0776dbc8d44088919422c2f76fa33",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 419B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_115ef3125ff74c69a4a790550d22af6b"
          }
        },
        "3f8b7231f72f42458d92e4b0a429bade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e8480d86d984be3b1a0b4efe39ccfb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b40917a9e7e04321b91ce4760b344534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "213fa74ced40400897782d6716462b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2af0776dbc8d44088919422c2f76fa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "115ef3125ff74c69a4a790550d22af6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6c625097f6c4994a18aabc6d285c218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d84d6e1ca094443803fbb8f26044058",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_43199e1cba054b9fbc81563fc6b30ebc",
              "IPY_MODEL_616e5cf137c94edd8a973a70b1cf9cf0",
              "IPY_MODEL_0415b3bdf64f4526ad4cee9c8ad6cba0"
            ]
          }
        },
        "5d84d6e1ca094443803fbb8f26044058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43199e1cba054b9fbc81563fc6b30ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31736b29450041388063f6f147327aa7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6bd04aca8fa1405d81919de3a4201ecb"
          }
        },
        "616e5cf137c94edd8a973a70b1cf9cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4fa14b26aec849e78a921eaf6aefd65e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1b5f44868954c8c862a81f52715c2ae"
          }
        },
        "0415b3bdf64f4526ad4cee9c8ad6cba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f49b7a11ff2e4bc995e89bbf7bb1d16d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 455k/455k [00:00&lt;00:00, 904kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_366c3b7d407c4084a8657c2a9ddd0f45"
          }
        },
        "31736b29450041388063f6f147327aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6bd04aca8fa1405d81919de3a4201ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4fa14b26aec849e78a921eaf6aefd65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1b5f44868954c8c862a81f52715c2ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f49b7a11ff2e4bc995e89bbf7bb1d16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "366c3b7d407c4084a8657c2a9ddd0f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F1SzLTr5naX"
      },
      "source": [
        "# Imports and connect to drive, reproduce settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnWCVLcY1XpE",
        "outputId": "66e9d871-fa09-4e37-d99e-3ec5cbe96997"
      },
      "source": [
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.utils.data as data_utils\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import numpy\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7Vy8pRY6CGP",
        "outputId": "b6883f02-71f9-410a-c0b4-1b787c114162"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZrj_XS7KTyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89c2343-c709-4dbf-8b06-f10745fff4c8"
      },
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade pandas-datareader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (1.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1ok2zkGogBo",
        "outputId": "a0ed2e45-bba3-43c1-e8f3-1b5d5319b914"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkR5YQWoYeZw",
        "outputId": "508326ef-1d0e-4d0e-b4be-b20c1bd67677"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ0pWMZg9YuI"
      },
      "source": [
        "# reproduce\n",
        "seed = 1\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxNWRPybrZhy"
      },
      "source": [
        "# todo define device here and remove the device def from after\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPejX7cj4zcG",
        "outputId": "434cbc7e-a4c3-4901-fbc4-6b5e3bbe9d18"
      },
      "source": [
        "pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Using cached vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHDLjZt947WY"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l29F9PKM6Nd6"
      },
      "source": [
        "# Prepare the numerical stock data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JH4yGWm6Fja"
      },
      "source": [
        "# Copy the dataset to the local environment\n",
        "#!cp \"/content/drive/MyDrive/upload_DJIA_table.csv\" \"DJIA.csv\"\n",
        "\n",
        "# Load the stock data\n",
        "#change back to kaggle dataset, however I know it has some problem in it\n",
        "#df_stock = pd.read_csv('DJIA.csv', index_col = \"Date\")\n",
        "\n",
        "# Load the stock data\n",
        "df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \n",
        "                          end=\"2016-07-01\")\n",
        "df_stock.sort_index(axis=0,ascending=True,inplace=True)\n",
        "\n",
        "#check if close and adj close is ever different\n",
        "diff_index = []\n",
        "for row in range(len(df_stock)):\n",
        "    if df_stock[\"Close\"][row] != df_stock[\"Adj Close\"][row]:\n",
        "        diff_index.append(row)\n",
        "\n",
        "if len(diff_index) > 1:\n",
        "  raise ValueError(\"DIFFERENT\")\n",
        "\n",
        "# Calculate moving average for 7, 14, 21 days\n",
        "ma_day = [7, 14, 21]\n",
        "\n",
        "for ma in ma_day:\n",
        "  column_name = f\"MA for {ma} days\"\n",
        "  df_stock[column_name] = df_stock[\"Adj Close\"].rolling(ma).mean()  \n",
        "\n",
        "#calculate the diff volume for the before day\n",
        "diff_volume = []\n",
        "for row in range(len(df_stock)):\n",
        "    if row == 0:\n",
        "        diff_volume.append(0)\n",
        "    else:\n",
        "        diff_volume.append(df_stock[\"Volume\"][row] - df_stock[\"Volume\"][row - 1])\n",
        "\n",
        "df_stock[\"Volume diff\"] = diff_volume\n",
        "\n",
        "for ma in ma_day:\n",
        "  column_name = f\"Volume diff MA for {ma} days\"\n",
        "  df_stock[column_name] = df_stock[\"Volume diff\"].rolling(ma).mean()\n",
        "  break # only use 7 MA\n",
        "\n",
        "#H-L with MA\n",
        "diff_H_L = []\n",
        "for row in range(len(df_stock)):\n",
        "    diff_H_L.append(df_stock[\"High\"][row] - df_stock[\"Low\"][row])\n",
        "\n",
        "df_stock[\"High-Low\"] = diff_H_L\n",
        "\n",
        "#O-C with MA\n",
        "diff_O_C = []\n",
        "for row in range(len(df_stock)):\n",
        "    diff_O_C.append(df_stock[\"Open\"][row] - df_stock[\"Close\"][row])\n",
        "\n",
        "df_stock[\"Open-Close\"] = diff_O_C\n",
        "\n",
        "#close standard deviation\n",
        "df_stock[\"Close 7day deviation\"] = df_stock[\"Close\"].rolling(7).std()\n",
        "\n",
        "#create input and output data\n",
        "input_df = pd.DataFrame()\n",
        "output_df = pd.DataFrame()\n",
        "\n",
        "#input features:\n",
        "#   Open\n",
        "#   High\n",
        "#   Low\n",
        "#   Close\n",
        "#   Volume\n",
        "#   High - Low\n",
        "#   Open - Close\n",
        "#   7 days MA close\n",
        "#   14 days MA close\n",
        "#   21 days MA close\n",
        "#   7 days MA volume\n",
        "#   7 days std \n",
        "input_df[\"Open\"] = df_stock[\"Open\"][21:]\n",
        "input_df[\"High\"] = df_stock[\"Open\"][21:]\n",
        "input_df[\"Low\"] = df_stock[\"Low\"][21:]\n",
        "input_df[\"Close\"] = df_stock[\"Close\"][21:]\n",
        "input_df[\"Volume\"] = df_stock[\"Volume\"][21:]\n",
        "input_df[\"High-Low\"] = df_stock[\"High-Low\"][21:]\n",
        "input_df[\"Open-Close\"] = df_stock[\"Open-Close\"][21:]\n",
        "input_df[\"7d MA close\"] = df_stock[\"MA for 7 days\"][21:]\n",
        "input_df[\"14d MA close\"] = df_stock[\"MA for 14 days\"][21:]\n",
        "input_df[\"21d MA close\"] = df_stock[\"MA for 21 days\"][21:]\n",
        "input_df[\"7d MA volume\"] = df_stock[\"Volume diff MA for 7 days\"][21:]\n",
        "input_df[\"14d std\"] = df_stock[\"Close 7day deviation\"][21:]\n",
        "\n",
        "#output:\n",
        "#   close for next day\n",
        "#input start from 24. day and last to last before one,\n",
        "#output start from 25. day and last to the last\n",
        "#will drop the rows after std for get the same values\n",
        "output_df[\"Next Close\"] = df_stock[\"Close\"][21:]\n",
        "\n",
        "# std the input and output\n",
        "input_std = (input_df - input_df.mean())/input_df.std()\n",
        "\n",
        "output_std = (output_df - output_df.mean())/output_df.std()\n",
        "\n",
        "# drop the input last row\n",
        "input_std.drop(input_std.tail(1).index,inplace=True)\n",
        "\n",
        "# drop the output first row\n",
        "output_std.drop(output_std.head(1).index,inplace=True)\n",
        "###################################### from here\n",
        "\n",
        "data_num = input_std\n",
        "data_num[\"Next Close\"] = output_std[\"Next Close\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GivJd0P6REl"
      },
      "source": [
        "# Prepare the stock news information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPXlGqJb6Uj3"
      },
      "source": [
        "# Copy the dataset to the local environment\n",
        "!cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\n",
        "\n",
        "# Load the dataset \n",
        "df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\n",
        "\n",
        "# Find the cells with NaN and after the rows for them\n",
        "is_NaN = df_combined.isnull()\n",
        "row_has_NaN = is_NaN.any(axis = 1)\n",
        "rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "# Replace them\n",
        "df_combined = df_combined.replace(np.nan, \" \")\n",
        "\n",
        "# Check the process\n",
        "is_NaN = df_combined.isnull()\n",
        "row_has_NaN = is_NaN.any(axis = 1)\n",
        "rows_with_NaN = df_combined[row_has_NaN]\n",
        "\n",
        "if len(rows_with_NaN) != 0:\n",
        "    raise ValueError(\"There is NaN in news dataset!\")\n",
        "\n",
        "# Drop the Label and change it with trend target\n",
        "df_combined.drop([\"Label\"], axis=1,inplace=True)\n",
        "\n",
        "# Get column names\n",
        "combined_column_names = []\n",
        "for column in df_combined.columns:\n",
        "  combined_column_names.append(column)\n",
        "\n",
        "# 2D array creation for the news based on macros\n",
        "COLUMNS = len(df_combined)\n",
        "ROWS = 25\n",
        "news_sum = [[0 for i in range(COLUMNS)] for j in range(int(len(combined_column_names) / ROWS))]\n",
        "\n",
        "# Merge the news\n",
        "for row in range(len(df_combined)):\n",
        "  for column in range(int(len(combined_column_names) / ROWS)):\n",
        "    temp = \"\"\n",
        "    news = \"\"\n",
        "    for word in range(ROWS):\n",
        "      news = df_combined[combined_column_names[(column * ROWS) + word]][row]\n",
        "      # Remove the b character at the begining of the string\n",
        "      if news[0] is \"b\":\n",
        "        news = \" \" + news[1:]\n",
        "      temp = temp + news\n",
        "    news_sum[column][row] = temp\n",
        "\n",
        "# Removing punctuations and numbers\n",
        "# Also removes the stop words\n",
        "# using tokenizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for line_i in range(len(news_sum)):\n",
        "  for row_i in range(len(news_sum[line_i])):\n",
        "    word_tokens = word_tokenize(news_sum[line_i][row_i])\n",
        "    temp_attach = \"\"\n",
        "    for word in word_tokens:\n",
        "      temp = \"\"\n",
        "      word = word.translate(str.maketrans('', '', string.punctuation)) # remove punctations\n",
        "      if word.isdigit() is False: # remove number\n",
        "        if word not in stop_words: # remove stop words\n",
        "          temp = word.lower() # converting lower case\n",
        "      temp_attach = temp_attach + \" \" + temp # add the word\n",
        "    temp_attach = \" \".join(temp_attach.split()) # remove space\n",
        "    news_sum[line_i][row_i] = temp_attach\n",
        "\n",
        "# Drop the old columns\n",
        "for column in range(len(combined_column_names)):\n",
        "  df_combined.drop(combined_column_names[column], axis = 1, inplace = True)\n",
        "\n",
        "# Create the new columns with the merged news\n",
        "for column in range(int(len(combined_column_names) / ROWS)):\n",
        "  colum_name = \"News_\" + str(column + 1)\n",
        "  df_combined[colum_name] = news_sum[column]      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUHFe6KX6VAd"
      },
      "source": [
        "# Prepare the complex dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjpYafeqJtKG"
      },
      "source": [
        "# join the two dataset and create the trend target for the news dataset\n",
        "data_num.index = data_num.index.strftime('%Y-%m-%d')\n",
        "complex_dataset = data_num.join(df_combined)\n",
        "\n",
        "#generate the trend target (shows the next day)\n",
        "#1 -> rise, otherwise 0\n",
        "#the model is the today's news how to change the next day trend !\n",
        "trend_target = []\n",
        "for element in range(len(complex_dataset)):\n",
        "  if complex_dataset[\"Close\"][element] > complex_dataset[\"Next Close\"][element]:\n",
        "      trend_target.append(0)\n",
        "  else:\n",
        "      trend_target.append(1)  \n",
        "\n",
        "complex_dataset[\"Trend target\"] = trend_target\n",
        "\n",
        "# Check the process\n",
        "is_NaN = complex_dataset.isnull()\n",
        "row_has_NaN = is_NaN.any(axis = 1)\n",
        "rows_with_NaN = complex_dataset[row_has_NaN]\n",
        "\n",
        "if len(rows_with_NaN) != 0:\n",
        "    raise ValueError(\"There is NaN in news dataset!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ5aQlYZ6gR7"
      },
      "source": [
        "# output for the complex model is the next close\n",
        "#split to 80% train+valid and 20% test\n",
        "train_valid, test = np.split(complex_dataset, [int(.8*len(complex_dataset))])\n",
        "\n",
        "#shuffle and split 60% 20% the train and valid\n",
        "#80% is train+valid -> 75% of train+valid is 60% train from the complete\n",
        "train, valid = np.split(train_valid, [int(.75*len(train_valid))])\n",
        "\n",
        "numeric_columns = []\n",
        "for i in range(complex_dataset.columns.get_loc(\"Next Close\")+1):\n",
        "  numeric_columns.append(i)\n",
        "train_num = train.iloc[:, numeric_columns]\n",
        "valid_num = valid.iloc[:, numeric_columns]\n",
        "test_num = test.iloc[:, numeric_columns]\n",
        "\n",
        "news_columns = []\n",
        "for i in range(complex_dataset.columns.get_loc(\"Next Close\") + 1,len(complex_dataset.columns)):\n",
        "  news_columns.append(i)\n",
        "train_news = train.iloc[:, news_columns]\n",
        "valid_news = valid.iloc[:, news_columns]\n",
        "test_news = test.iloc[:, news_columns]\n",
        "\n",
        "train_Y_num = torch.tensor(train_num[\"Next Close\"].values.astype(np.float32))\n",
        "train_Y_num = train_Y_num.view(-1,1)\n",
        "train_X_num = torch.tensor(train_num.drop([\"Next Close\"],axis=1).values.astype(np.float32))\n",
        "\n",
        "valid_Y_num = torch.tensor(valid_num[\"Next Close\"].values.astype(np.float32))\n",
        "valid_Y_num = valid_Y_num.view(-1,1)\n",
        "valid_X_num = torch.tensor(valid_num.drop(\"Next Close\",axis=1).values.astype(np.float32))\n",
        "\n",
        "test_Y_num = torch.tensor(test_num[\"Next Close\"].values.astype(np.float32))\n",
        "test_Y_num = test_Y_num.view(-1,1)\n",
        "test_X_num = torch.tensor(test_num.drop(\"Next Close\",axis=1).values.astype(np.float32))\n",
        "\n",
        "train_Y_news = torch.tensor(train_news[\"Trend target\"].values.astype(np.float32))\n",
        "train_Y_news = train_Y_news.view(-1,1)\n",
        "train_X_news_raw = train_news.drop(\"Trend target\",axis=1)\n",
        "\n",
        "valid_Y_news = torch.tensor(valid_news[\"Trend target\"].values.astype(np.float32))\n",
        "valid_Y_news = valid_Y_news.view(-1,1)\n",
        "valid_X_news_raw = valid_news.drop(\"Trend target\",axis=1)\n",
        "\n",
        "test_Y_news = torch.tensor(test_news[\"Trend target\"].values.astype(np.float32))\n",
        "test_Y_news = test_Y_news.view(-1,1)\n",
        "test_X_news_raw = test_news.drop(\"Trend target\",axis=1)\n",
        "\n",
        "train_Y_all = torch.tensor(train[\"Next Close\"].values.astype(np.float32))\n",
        "train_Y_all = train_Y_all.view(-1,1)\n",
        "train_X_all_raw = train.drop([\"Next Close\",\"Trend target\"],axis=1)\n",
        "\n",
        "valid_Y_all = torch.tensor(valid[\"Next Close\"].values.astype(np.float32))\n",
        "valid_Y_all = valid_Y_all.view(-1,1)\n",
        "valid_X_all_raw = valid.drop([\"Next Close\",\"Trend target\"],axis=1)\n",
        "\n",
        "test_Y_all = torch.tensor(test[\"Next Close\"].values.astype(np.float32))\n",
        "test_Y_all = test_Y_all.view(-1,1)\n",
        "test_X_all_raw = test.drop([\"Next Close\",\"Trend target\"],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M86D0NN4Vfe7"
      },
      "source": [
        "def preprocess_raw_text(tokenizer):\n",
        "    global vocab\n",
        "    if tokenizer == \"simple\":\n",
        "        def yield_tokens(data_iter):\n",
        "            for text in data_iter:\n",
        "                yield word_tokenize(text)\n",
        "\n",
        "        min_freq = 30\n",
        "        special_tokens = [\"<unk>\", \"<pad>\"]        \n",
        "\n",
        "        vocab = build_vocab_from_iterator(yield_tokens(train_X_news_raw[\"News_1\"]),\n",
        "                                          min_freq = min_freq,\n",
        "                                          specials=special_tokens)\n",
        "\n",
        "        global pretrained_embedding\n",
        "        # uncomment if no saved pretrained\n",
        "        #pretrained_vectors = torchtext.vocab.GloVe(name='6B', dim=50)\n",
        "        #pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocab.get_itos())\n",
        "        # save out the pretrained embedding\n",
        "        #torch.save(pretrained_embedding, \"drive/MyDrive/complex/pretrained_embeddings.pt\")\n",
        "\n",
        "        # load the pretrained embeddings\n",
        "        pretrained_embedding = torch.load(\"drive/MyDrive/complex/pretrained_embeddings.pt\")\n",
        "        # can model.embedding.weight.data = pretrained_embedding\n",
        "\n",
        "        # set deafult vocab token\n",
        "        vocab.set_default_index(vocab.lookup_indices([\"<unk>\"])[0])\n",
        "\n",
        "        # setting padding id\n",
        "        pad_id = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "\n",
        "        text_pipeline = lambda x: vocab(word_tokenize(x))\n",
        "\n",
        "    elif tokenizer == \"bert\":\n",
        "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "      max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "      def text_pipeline(text):\n",
        "          tokens = tokenizer.tokenize(text)\n",
        "          if len(tokens) > max_input_length-2:\n",
        "              print(f\"Text is bigger than the max length: {len(tokens)}\")\n",
        "          tokens = tokens[:max_input_length-2]\n",
        "          ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "          return ids     \n",
        "\n",
        "      pad_id = tokenizer.pad_token_id \n",
        "\n",
        "      longest_new = max_input_length-2\n",
        "\n",
        "    elif tokenizer == \"VADER\":\n",
        "      analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "      longest_new = 1\n",
        "\n",
        "      # output is raw VADER sentiment\n",
        "      def text_pipeline(text):\n",
        "        vs = analyzer.polarity_scores(text)\n",
        "        retVal = []\n",
        "        retVal.append(vs.get(\"compound\"))\n",
        "        return retVal       \n",
        "\n",
        "      pad_id = 0 \n",
        "\n",
        "    elif tokenizer == \"logreg\": # create vocab with every word, then after batch reconvert it \n",
        "        def yield_tokens(data_iter):\n",
        "            for text in data_iter:\n",
        "                yield word_tokenize(text)\n",
        "\n",
        "        min_freq = 0\n",
        "        special_tokens = [\"<unk>\", \"<pad>\"]        \n",
        "\n",
        "        vocab = build_vocab_from_iterator(yield_tokens(train_X_news_raw[\"News_1\"]),\n",
        "                                          min_freq = min_freq,\n",
        "                                          specials=special_tokens)\n",
        "\n",
        "        # set deafult vocab token\n",
        "        vocab.set_default_index(vocab.lookup_indices([\"<unk>\"])[0])\n",
        "\n",
        "        # setting padding id\n",
        "        pad_id = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "\n",
        "        text_pipeline = lambda x: vocab(word_tokenize(x)) \n",
        "\n",
        "        bow_vectorizer = CountVectorizer()\n",
        "        bow_train = bow_vectorizer.fit_transform(train_X_news_raw[\"News_1\"])     \n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"No suitable tokenizer given!\")     \n",
        "\n",
        "    # tokenize all of the news and create the tokens from vocab\n",
        "    train_news = []\n",
        "    valid_news = []\n",
        "    test_news = []\n",
        "    for new in train_X_news_raw[\"News_1\"]:\n",
        "        processed_text = text_pipeline(new)\n",
        "        train_news.append(processed_text)\n",
        "    for new in valid_X_news_raw[\"News_1\"]:\n",
        "        processed_text = text_pipeline(new)\n",
        "        valid_news.append(processed_text)\n",
        "    for new in test_X_news_raw[\"News_1\"]:\n",
        "        processed_text = text_pipeline(new)\n",
        "        test_news.append(processed_text)   \n",
        "\n",
        "    if tokenizer == \"simple\" or tokenizer == \"logreg\":\n",
        "        # search for the longest\n",
        "        longest_new = 0\n",
        "        for new in train_news:\n",
        "            if len(new) > longest_new:\n",
        "              longest_new = len(new)\n",
        "        for new in valid_news:\n",
        "            if len(new) > longest_new:\n",
        "              longest_new = len(new)\n",
        "        for new in test_news:\n",
        "            if len(new) > longest_new:\n",
        "              longest_new = len(new)       \n",
        "\n",
        "    # pad the news tokens to the same length\n",
        "    # do the pad\n",
        "    train_news_temp = []\n",
        "    valid_news_temp = []\n",
        "    test_news_temp = []\n",
        "    for new in train_news:\n",
        "        train_news_temp.append(\n",
        "            numpy.pad(new,\n",
        "                      (0,\n",
        "                      longest_new-len(new)),\n",
        "                      mode=\"constant\",\n",
        "                      constant_values=pad_id))\n",
        "    for new in valid_news:\n",
        "        valid_news_temp.append(\n",
        "            numpy.pad(new,\n",
        "                      (0,\n",
        "                      longest_new-len(new)),\n",
        "                      mode=\"constant\",\n",
        "                      constant_values=pad_id))\n",
        "    for new in test_news:\n",
        "        test_news_temp.append(\n",
        "            numpy.pad(new,\n",
        "                      (0,\n",
        "                      longest_new-len(new)),\n",
        "                      mode=\"constant\",\n",
        "                      constant_values=pad_id))\n",
        "        \n",
        "    if tokenizer == \"VADER\":\n",
        "      train_X_news = torch.FloatTensor(train_news_temp)\n",
        "      valid_X_news = torch.FloatTensor(valid_news_temp)\n",
        "      test_X_news = torch.FloatTensor(test_news_temp)  \n",
        "    else:  \n",
        "      train_X_news = torch.LongTensor(train_news_temp)\n",
        "      valid_X_news = torch.LongTensor(valid_news_temp)\n",
        "      test_X_news = torch.LongTensor(test_news_temp)    \n",
        "\n",
        "    return train_X_news, valid_X_news, test_X_news"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBP1G88yTlK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "b31c8bdb3e9c4c729041747760361eeb",
            "710bb8189865444a93fdde5bc060a641",
            "f8616b0eb9c84466ba40fe36d6290b82",
            "cc16afea2bba473da26b0beeec05d0c0",
            "da95b20fb9654df9b6007e0295cb3cff",
            "be516f3eed9249c7942f68a4d4041b45",
            "eee901ad23f249de81be20b38874d3ee",
            "9faa0d8bb43d4a15b923ca3adb5bdb88",
            "27955de2797f43a39a75e415e2ac5b19",
            "1aa030c956794a9e980674b3f54630f5",
            "9e7f71a46d564c8c8568020f9400abed",
            "68d94616056541098426622b9ed4a99c",
            "e80d8332a22c4578bbd6fe815d65b77c",
            "38f43111c3ce4f16bc7ebf75518d7e10",
            "c976ece2e42b4c0e8bccae3703e62d00",
            "aa2c904fed6d46a1918f211f8ac5f599",
            "3f8b7231f72f42458d92e4b0a429bade",
            "7e8480d86d984be3b1a0b4efe39ccfb4",
            "b40917a9e7e04321b91ce4760b344534",
            "213fa74ced40400897782d6716462b1d",
            "2af0776dbc8d44088919422c2f76fa33",
            "115ef3125ff74c69a4a790550d22af6b",
            "f6c625097f6c4994a18aabc6d285c218",
            "5d84d6e1ca094443803fbb8f26044058",
            "43199e1cba054b9fbc81563fc6b30ebc",
            "616e5cf137c94edd8a973a70b1cf9cf0",
            "0415b3bdf64f4526ad4cee9c8ad6cba0",
            "31736b29450041388063f6f147327aa7",
            "6bd04aca8fa1405d81919de3a4201ecb",
            "4fa14b26aec849e78a921eaf6aefd65e",
            "a1b5f44868954c8c862a81f52715c2ae",
            "f49b7a11ff2e4bc995e89bbf7bb1d16d",
            "366c3b7d407c4084a8657c2a9ddd0f45"
          ]
        },
        "outputId": "98bb9d51-aa1c-45cd-f72d-3811c0f01924"
      },
      "source": [
        "# preprocess raw text\n",
        "train_X_news, valid_X_news, test_X_news = preprocess_raw_text(\"bert\")\n",
        "\n",
        "# create tensor datasets and loaders\n",
        "# only numerical data\n",
        "train_num_tensor = data_utils.TensorDataset(train_X_num, train_Y_num)\n",
        "valid_num_tensor = data_utils.TensorDataset(valid_X_num, valid_Y_num)\n",
        "test_num_tensor = data_utils.TensorDataset(test_X_num, test_Y_num)\n",
        "\n",
        "train_num_loader = data_utils.DataLoader(dataset = train_num_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "valid_num_loader = data_utils.DataLoader(dataset = valid_num_tensor, \n",
        "                              batch_size = 32, shuffle = False)  \n",
        "test_num_loader = data_utils.DataLoader(dataset = test_num_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "# news and labels data only\n",
        "train_news_tensor = data_utils.TensorDataset(train_X_news, train_Y_news)\n",
        "valid_news_tensor = data_utils.TensorDataset(valid_X_news, valid_Y_news)\n",
        "test_news_tensor = data_utils.TensorDataset(test_X_news, test_Y_news)\n",
        "\n",
        "train_news_loader = data_utils.DataLoader(dataset = train_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "valid_news_loader = data_utils.DataLoader(dataset = valid_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)  \n",
        "test_news_loader = data_utils.DataLoader(dataset = test_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "# all data (news + numerical)\n",
        "# concat the numeric and news tensor dataset\n",
        "train_all_tensor = data_utils.TensorDataset(train_X_num, train_X_news,  train_Y_num)\n",
        "valid_all_tensor = data_utils.TensorDataset(valid_X_num, valid_X_news,  valid_Y_num)\n",
        "test_all_tensor = data_utils.TensorDataset(test_X_num, test_X_news,  test_Y_num)\n",
        "\n",
        "train_all_loader = data_utils.DataLoader(dataset = train_all_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "valid_all_loader = data_utils.DataLoader(dataset = valid_all_tensor, \n",
        "                              batch_size = 32, shuffle = False)  \n",
        "test_all_loader = data_utils.DataLoader(dataset = test_all_tensor, \n",
        "                              batch_size = 32, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b31c8bdb3e9c4c729041747760361eeb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68d94616056541098426622b9ed4a99c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6c625097f6c4994a18aabc6d285c218",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text is bigger than the max length: 519\n",
            "Text is bigger than the max length: 522\n",
            "Text is bigger than the max length: 529\n",
            "Text is bigger than the max length: 562\n",
            "Text is bigger than the max length: 514\n",
            "Text is bigger than the max length: 584\n",
            "Text is bigger than the max length: 526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhHjCaOa6oz1"
      },
      "source": [
        "# Set up the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-qGOxjyBX-y"
      },
      "source": [
        "## Numerical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hnAd57F6rv_"
      },
      "source": [
        "# set up a numerical model\n",
        "# linear 108\n",
        "# LR start/end 0.0001, batch 32, dropout 0, layer d 11, activition none, score 70.63 were \n",
        "# but needed retrain for not shuffled dataset -> to be coherent with news dataset\n",
        "class Linear_108_num(nn.Module):\n",
        "    def __init__(self): # \n",
        "        super(Linear_108_num, self).__init__()\n",
        "\n",
        "        self.fc = nn.ModuleList()\n",
        "        self.fc.append(nn.Linear(12, 64))\n",
        "        self.fc.append(nn.Linear(64, 128))\n",
        "        self.fc.append(nn.Linear(128, 256))\n",
        "        self.fc.append(nn.Linear(256, 512))\n",
        "        self.fc.append(nn.Linear(512, 1024))  \n",
        "        self.fc.append(nn.Linear(1024, 512))               \n",
        "        self.fc.append(nn.Linear(512, 256))\n",
        "        self.fc.append(nn.Linear(256, 128))\n",
        "        self.fc.append(nn.Linear(128, 64))\n",
        "        self.fc.append(nn.Linear(64, 16))\n",
        "        self.fc.append(nn.Linear(16, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for fc in self.fc:\n",
        "            x = fc(x)\n",
        "        return x        \n",
        "\n",
        "# LSTM 92 -> these names not OK, they werent suitable for the new dataset split...\n",
        "# batch size 32, LR start and end 0.001 -> no dynamic, dropout 0.1, architect 223,\n",
        "# activition function none, bidirect false; score 83.5 and acc was 48.35\n",
        "class LSTM_92_num(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM_92_num, self).__init__()\n",
        "  \n",
        "        self.LSTM = nn.LSTM(input_size=12,\n",
        "                                hidden_size=256,\n",
        "                                num_layers=2,\n",
        "                                bidirectional=True,\n",
        "                                dropout=0.1)\n",
        "        self.fc1 = nn.Linear(2*256,64)\n",
        "        self.fc2 = nn.Linear(64,8)   \n",
        "        self.fc3 = nn.Linear(8,1)   \n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)    \n",
        "        self.batch_size = 32                                                                                                                                          \n",
        "\n",
        "    def forward(self, x):      \n",
        "        try:\n",
        "          x_reshaped = torch.reshape(x,(-1,32,12))\n",
        "        except:\n",
        "          x_reshaped = torch.reshape(x,(-1,1,12))\n",
        "\n",
        "        output, (hidden, cell) = self.LSTM(x_reshaped)\n",
        "\n",
        "        x = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# LSTM 36\n",
        "# batch size 32, LR start and end 0.001 -> no dynamic, dropout 0, architect 321,\n",
        "# activition function none, bidirect false; score 117.546 and acc was 52.42\n",
        "class LSTM_36_num(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM_36_num, self).__init__()\n",
        "  \n",
        "        self.LSTM = nn.LSTM(input_size=12,\n",
        "                                hidden_size=128,\n",
        "                                num_layers=2,\n",
        "                                bidirectional=True,\n",
        "                                dropout=0.1)\n",
        "        self.fc1 = nn.Linear(2*128,16)\n",
        "        self.fc2 = nn.Linear(16,1)   \n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)    \n",
        "        self.batch_size = 32                                                                                                                                          \n",
        "\n",
        "    def forward(self, x):      \n",
        "        try:\n",
        "          x_reshaped = torch.reshape(x,(-1,32,12))\n",
        "        except:\n",
        "          x_reshaped = torch.reshape(x,(-1,1,12))\n",
        "\n",
        "        output, (hidden, cell) = self.LSTM(x_reshaped)\n",
        "\n",
        "        x = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34wgxeDrBZ64"
      },
      "source": [
        "## News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyW4LGCtBj-O"
      },
      "source": [
        "# set up a NLP model\n",
        "# LSTM\n",
        "# merged news into 1, embedding dims 300, hidden size 32, layer 2, 3 numerical after\n",
        "# biderictional yes, dropout 0.4, ReLU act, sigmoid activiton at the end\n",
        "class LSTM_news(nn.Module):\n",
        "    def __init__(self, vocab_size, pad_idx):        \n",
        "        super(LSTM_news, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, 50, padding_idx = pad_idx)  \n",
        "\n",
        "        self.rnn = nn.LSTM(50, #input size\n",
        "                          32, #features in hidden state\n",
        "                          num_layers=2, \n",
        "                          bidirectional=True, \n",
        "                          dropout=0.4)        \n",
        "       \n",
        "        self.fc1 = nn.Linear(2*32, 32)\n",
        "        self.fc2 = nn.Linear(32, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        self.activition = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        try:\n",
        "          text = torch.reshape(text,(464,32))\n",
        "        except:\n",
        "          text = torch.reshape(text,(464,1))\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        #this is for Bidirectional only!\n",
        "        hidden = self.dropout(self.activition(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        x = self.activition(self.fc1(hidden))\n",
        "        x = self.activition(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        sig = nn.Sigmoid()\n",
        "        x = sig(x)  \n",
        "\n",
        "        return x\n",
        "\n",
        "# Bert\n",
        "class BERT_news(nn.Module):\n",
        "    def __init__(self):        \n",
        "        super(BERT_news, self).__init__()\n",
        "        \n",
        "        self.bert = bert.to(device)\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                          hidden_size=32,\n",
        "                          num_layers=2,\n",
        "                          bidirectional=False,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0.3)\n",
        "        \n",
        "        self.out = nn.Linear(32, 1)\n",
        "        \n",
        "        self.activition = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def forward(self, text):                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        hidden = self.activition(self.dropout(hidden[-1,:,:]))\n",
        "        \n",
        "        sig = nn.Sigmoid()\n",
        "        output = sig(self.out(hidden))   \n",
        "        return output       \n",
        "\n",
        "# VADER\n",
        "class VADER():\n",
        "    def __init__(self):        \n",
        "        super(VADER, self).__init__()\n",
        "        \n",
        "    def forward(self, text):                        \n",
        "        sig = nn.Sigmoid()\n",
        "        output = sig(-1*text)\n",
        "        return output\n",
        "\n",
        "# logreg\n",
        "class LogReg():\n",
        "    def __init__(self):        \n",
        "        super(LogReg, self).__init__()\n",
        "\n",
        "    def convertIndicesToToken(self, news, train):\n",
        "      if train == True:\n",
        "        tokens = []\n",
        "        for new in news:\n",
        "          token_row = \"\"\n",
        "          for indice in new:\n",
        "              token = vocab.lookup_token(indice)\n",
        "              if token != \"<unk>\" and token != \"<pad>\":\n",
        "                  token_row += token + \" \"\n",
        "          tokens.append(token_row)   \n",
        "      else:\n",
        "        tokens_tmp = \"\"\n",
        "        for indice in news:\n",
        "            token = vocab.lookup_token(indice)\n",
        "            if token != \"<unk>\" and token != \"<pad>\":\n",
        "                tokens_tmp += token + \" \"\n",
        "        tokens = []\n",
        "        tokens.append(tokens_tmp)\n",
        "\n",
        "      return tokens        \n",
        "\n",
        "    def train(self, train_x, train_y):\n",
        "        # reconvert indices to tokens\n",
        "        train_x = self.convertIndicesToToken(train_x, train=True)\n",
        "\n",
        "        # fit vectorizer\n",
        "        bow_vectorizer = CountVectorizer()\n",
        "        bow_vectorizer.fit(train_x)\n",
        "\n",
        "        # fit model\n",
        "        bow_train = bow_vectorizer.transform(train_x)\n",
        "        bow_model = LogisticRegression(random_state=seed, max_iter=100000,solver='lbfgs')\n",
        "        bow_model = bow_model.fit(bow_train, torch.reshape(train_y,(-1,)))\n",
        "\n",
        "        self.bow_vectorizer = bow_vectorizer\n",
        "        self.bow_model = bow_model\n",
        "\n",
        "        print(\"Train done\")\n",
        "\n",
        "    def forward(self, text):      \n",
        "        # reconvert indices to tokens\n",
        "        try:\n",
        "          text = self.convertIndicesToToken(text, train=False)\n",
        "        except:\n",
        "          text = self.convertIndicesToToken(text, train=True)\n",
        "\n",
        "        bow_test = self.bow_vectorizer.transform(text)\n",
        "        bow_prediction = self.bow_model.predict_proba(bow_test)\n",
        "        prob_out = []\n",
        "        for predict_prob in bow_prediction:\n",
        "            prob_out.append(predict_prob[1])\n",
        "\n",
        "        return torch.reshape(torch.FloatTensor(prob_out),(len(prob_out),1))                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MZ3w8hQBbmM"
      },
      "source": [
        "## Complex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if3czYWMBkOp"
      },
      "source": [
        "# complex model 1\n",
        "# 4 layer linear multi layer after\n",
        "class Complex_1(nn.Module):\n",
        "    def __init__(self, model_num, model_news):  \n",
        "        super(Complex_1, self).__init__()\n",
        "\n",
        "        self.model_num = model_num\n",
        "        self.model_news = model_news\n",
        "\n",
        "        # create the NN after the two outputs\n",
        "        # input -> trend probability from news and std. close prediction\n",
        "        # output -> close prediction\n",
        "        self.fc1 = nn.Linear(2,8)\n",
        "        self.fc2 = nn.Linear(8,32)\n",
        "        self.fc3 = nn.Linear(32,16)\n",
        "        self.fc4 = nn.Linear(16,1)\n",
        "\n",
        "    def forward(self, x_num, x_news):\n",
        "        global y_num,y_news\n",
        "        y_num = self.model_num(x_num)\n",
        "        try:\n",
        "          y_news = self.model_news(x_news)\n",
        "        except:\n",
        "          y_news = self.model_news.forward(x_news)\n",
        "\n",
        "        x = torch.cat((y_num, y_news), dim=1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)        \n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# complex model 2\n",
        "# 6 layer linear multi layer after\n",
        "class Complex_2(nn.Module):\n",
        "    def __init__(self, model_num, model_news):  \n",
        "        super(Complex_2, self).__init__()\n",
        "\n",
        "        self.model_num = model_num\n",
        "        self.model_news = model_news\n",
        "\n",
        "        # create the NN after the two outputs\n",
        "        # input -> trend probability from news and std. close prediction\n",
        "        # output -> close prediction\n",
        "        self.fc1 = nn.Linear(2,8)\n",
        "        self.fc2 = nn.Linear(8,16)\n",
        "        self.fc3 = nn.Linear(16,64)\n",
        "        self.fc4 = nn.Linear(64,32)\n",
        "        self.fc5 = nn.Linear(32,16)\n",
        "        self.fc6 = nn.Linear(16,1)\n",
        "\n",
        "    def forward(self, x_num, x_news):\n",
        "        y_num = self.model_num(x_num)\n",
        "        try:\n",
        "          y_news = self.model_news(x_news)\n",
        "        except:\n",
        "          y_news = self.model_news.forward(x_news)\n",
        "\n",
        "        x = torch.cat((y_num, y_news), dim=1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)        \n",
        "        x = self.fc4(x)\n",
        "        x = self.fc5(x)        \n",
        "        x = self.fc6(x)        \n",
        "\n",
        "        return x      \n",
        "\n",
        "# complex model 3\n",
        "# 8 layer linear multi layer after\n",
        "class Complex_3(nn.Module):\n",
        "    def __init__(self, model_num, model_news):  \n",
        "        super(Complex_3, self).__init__()\n",
        "\n",
        "        self.model_num = model_num\n",
        "        self.model_news = model_news\n",
        "\n",
        "\n",
        "        # create the NN after the two outputs\n",
        "        # input -> trend probability from news and std. close prediction\n",
        "        # output -> close prediction\n",
        "        self.fc1 = nn.Linear(2,8)\n",
        "        self.fc2 = nn.Linear(8,32)\n",
        "        self.fc3 = nn.Linear(32,64)\n",
        "        self.fc4 = nn.Linear(64,256)\n",
        "        self.fc5 = nn.Linear(256,128)\n",
        "        self.fc6 = nn.Linear(128,64)\n",
        "        self.fc7 = nn.Linear(64,16)\n",
        "        self.fc8 = nn.Linear(16,1)  \n",
        "\n",
        "        self.act = nn.ReLU()      \n",
        "\n",
        "    def forward(self, x_num, x_news):\n",
        "        y_num = self.model_num(x_num)\n",
        "        try:\n",
        "          y_news = self.model_news(x_news)\n",
        "        except:\n",
        "          y_news = self.model_news.forward(x_news)\n",
        "\n",
        "        x = torch.cat((y_num, y_news), dim=1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)        \n",
        "        x = self.act(self.fc4(x))\n",
        "        x = self.act(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        x = self.fc7(x)        \n",
        "        x = self.fc8(x)        \n",
        "\n",
        "        return x                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaYiC-cD6sSK"
      },
      "source": [
        "# Train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj5mi7tbX7zM"
      },
      "source": [
        "#save the time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_0VN8S3LgwM"
      },
      "source": [
        "def calc_score(prediction,real):\n",
        "  prediction = torch.FloatTensor(prediction)\n",
        "  real = torch.FloatTensor(real)\n",
        "  score = 0\n",
        "  loss = criterion(prediction,real)\n",
        "  score = loss.item()\n",
        "  score = score / len(prediction)\n",
        "  return score   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Mn4coQP4DI"
      },
      "source": [
        "def save_the_log(path,name):\n",
        "  # save the log\n",
        "  with open(str(path)+\"/\"+str(name)+\"_log.csv\", \"w\") as f:\n",
        "    s = \"\"      \n",
        "    s += 'Model;Epoch;Accuraccy;Batch size;LR start;LR end;Dropout;Layer deepness;Activition function;Score\\n'\n",
        "    for i in range(len(model_name)):\n",
        "      s += f'{model_name[i]};{e_res[i]};{acc_res[i]}%;{batch_s_res[i]};{optimizer_start_res[i]};{optimizer_end_res[i]};{dropout_res[i]};{layer_dep_res[i]};{act_res[i]};{score_res[i]}\\n'\n",
        "    \n",
        "    f.write(s)\n",
        "\n",
        "  plt_teach_res[-1].savefig(str(path)+\"/\"+str(name)+\"_log_teach_\" + str(model_name[-1]) + \"_\" + str(e_res[-1]) + \".png\")\n",
        "  try:\n",
        "    plt_pred_res[-1].savefig(str(path)+\"/\"+str(name)+\"_log_pred_\" + str(model_name[-1]) + \"_\" + str(e_res[-1]) + \".png\")     \n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaPK_A4hxZms"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_5FEqa0BeMN"
      },
      "source": [
        "## Numerical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TELIPYsBk10"
      },
      "source": [
        "def run_model_num(batch_size_in, model_in, optimizer_in, criterion_in, model_name_in):\n",
        "  # check if needed padding:\n",
        "  if len(train_X_num) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(train_X_num) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(train_X_num)),0)\n",
        "    padded_train_X_num = pad(train_X_num)\n",
        "    padded_train_Y_num = pad(train_Y_num)\n",
        "    train_num_tensor = data_utils.TensorDataset(padded_train_X_num, padded_train_Y_num)\n",
        "    train_num_loader = data_utils.DataLoader(dataset = train_num_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "  if len(valid_X_num) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(valid_X_num) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(valid_X_num)),0)\n",
        "    padded_valid_X_num = pad(valid_X_num)\n",
        "    padded_valid_Y_num = pad(valid_Y_num)\n",
        "    valid_num_tensor = data_utils.TensorDataset(padded_valid_X_num, padded_valid_Y_num)\n",
        "    valid_num_loader = data_utils.DataLoader(dataset = valid_num_tensor, \n",
        "                              batch_size = 32, shuffle = False)                                 \n",
        "  \n",
        "  model = model_in\n",
        "\n",
        "  optimizer = optimizer_in\n",
        "\n",
        "  # Set the device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  criterion = criterion_in\n",
        "\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  # Training with Validation\n",
        "  min_valid_loss = np.inf\n",
        "  epoch_num = 1000\n",
        "\n",
        "  #store the losses\n",
        "  train_loss_array = []\n",
        "  valid_loss_array = []\n",
        "  epoch = 0\n",
        "\n",
        "  real_test_array = []\n",
        "  real_trend_array = test[\"Trend target\"]  \n",
        "  for y in test_Y_num:\n",
        "    real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "  early_stop = False\n",
        "  for e in range(epoch_num):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss = 0.0\n",
        "      model.train()\n",
        "      for x, y in train_num_loader:\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "            \n",
        "          # Clear the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Forward Pass\n",
        "          pred_y = model(x)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate gradients \n",
        "          loss.backward()\n",
        "          # Update Weights\n",
        "          optimizer.step()\n",
        "          # Calculate Loss\n",
        "          train_loss += loss.item() * x.size(0)\n",
        "\n",
        "      train_loss = train_loss / len(train_num_loader.sampler)\n",
        "      train_loss_array.append(train_loss) \n",
        "\n",
        "      valid_loss = 0.0\n",
        "      model.eval()     # Optional when not using Model Specific layer\n",
        "      for x, y in valid_num_loader:\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "            \n",
        "          # Forward Pass\n",
        "          pred_y = model(x)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate Loss\n",
        "          valid_loss += loss.item() * x.size(0)\n",
        "    \n",
        "      valid_loss = valid_loss / len(valid_num_loader.sampler)\n",
        "      valid_loss_array.append(valid_loss)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      \n",
        "      print(f'Epoch {e+1} \\t\\t Epoch time: {epoch_mins}m {epoch_secs}s\\n\\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
        "        \n",
        "      if min_valid_loss > valid_loss:\n",
        "          print(f'\\t\\t Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "                            \n",
        "          # Saving State Dict\n",
        "          torch.save(model.state_dict(), 'drive/MyDrive/complex/numerical_models/best_model_' + str(model_name_in) + '.pt')\n",
        "          epoch = e\n",
        "\n",
        "          # early stop\n",
        "          if (valid_loss < 0.05 and train_loss < 0.05) and (min_valid_loss - valid_loss) < 0.00005:\n",
        "              #early_stop = True\n",
        "              pass\n",
        "\n",
        "          min_valid_loss = valid_loss\n",
        "        \n",
        "      if  early_stop == True and e > 50:\n",
        "          print(f'\\tEarly stop of the training')\n",
        "          break \n",
        "\n",
        "  # Visualize the training\n",
        "  f1 = plt.figure(figsize=(16,8))\n",
        "  plt.title('Train and validation loss')\n",
        "  plt.plot(train_loss_array, color = \"green\", label = \"Train loss\")\n",
        "  plt.plot(valid_loss_array, color = \"blue\", label = \"Valid loss\")\n",
        "  plt.xlabel('Epoch',fontsize=18)\n",
        "  plt.ylabel('Loss',fontsize=18)\n",
        "  plt.legend(fontsize=18)\n",
        "  plt_teach = f1\n",
        "  plt.close()\n",
        "\n",
        "  model.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_' + str(model_name_in) + '.pt'))\n",
        "\n",
        "  predict_test_array = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for x in test_X_num:\n",
        "          model.eval()\n",
        "          predict = model(x.reshape(1,-1).to(device))        \n",
        "          predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "  # calculate score\n",
        "  score = calc_score(predict_test_array,real_test_array)    \n",
        "  print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "  #check the trend -> create an array with the real and with the predicted trend\n",
        "  #if the current value is bigger than before -> 1\n",
        "  #otherwise (same or smaller) -> 0\n",
        "  predicted_trend_array = []\n",
        "  for element in range(len(predict_test_array)):\n",
        "      real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "      if real_today_close > predict_test_array[element].values:\n",
        "          predicted_trend_array.append(0)\n",
        "      else:\n",
        "          predicted_trend_array.append(1)\n",
        "\n",
        "  #check the number of differences\n",
        "  trend_diff_array = []\n",
        "  for element in range(len(real_trend_array)):\n",
        "      if real_trend_array[element] != predicted_trend_array[element]:\n",
        "        trend_diff_array.append(element)\n",
        "\n",
        "  #percentage of good predict\n",
        "  acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "  print(f\"Accuracy: {acc}\\n\")  \n",
        "  #visualize it\n",
        "  f2 = plt.figure(figsize=(24,12))\n",
        "  plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "  plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "          label = \"Real\")\n",
        "  plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "          label = \"Predicted\")\n",
        "  plt.xlabel(\"Date\",fontsize = 18)\n",
        "  plt.legend(fontsize = 18)\n",
        "  f2.set_size_inches(12,6)\n",
        "  plt_best = f2\n",
        "  plt.close()\n",
        "\n",
        "  # return the results for compare:\n",
        "  #   epoch\n",
        "  #   trend result (best)\n",
        "  #   diagram (best)\n",
        "  #   score\n",
        "  return epoch, score, acc, plt_teach, plt_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kou7-D_02UvQ",
        "outputId": "62b6da8a-cc78-4487-f7c7-880c5a6953fd"
      },
      "source": [
        "model = []\n",
        "model.append(LSTM_92_num())\n",
        "\n",
        "for i in range(len(model)):\n",
        "    model_name = [\"LSTM_92_num\"]\n",
        "    optimizer = optim.Adam(model[i].parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_num(batch_size_in=32,model_in=model[i],\n",
        "          optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"0.1\")\n",
        "    layer_dep_res.append(\"4\")\n",
        "    act_res.append(\"None\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/numerical_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.4904472491628415 \t\t Validation Loss: 1.4334435371252208\n",
            "\t\t Validation Loss Decreased(inf--->1.433444) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.417036327621522 \t\t Validation Loss: 1.3912824300619273\n",
            "\t\t Validation Loss Decreased(1.433444--->1.391282) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.29295839901661147 \t\t Validation Loss: 1.1803890054042523\n",
            "\t\t Validation Loss Decreased(1.391282--->1.180389) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.12919648620299995 \t\t Validation Loss: 0.897739419570336\n",
            "\t\t Validation Loss Decreased(1.180389--->0.897739) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.060284077158399126 \t\t Validation Loss: 0.7773278745321127\n",
            "\t\t Validation Loss Decreased(0.897739--->0.777328) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.04634876920514413 \t\t Validation Loss: 0.6216224546615894\n",
            "\t\t Validation Loss Decreased(0.777328--->0.621622) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.03140907659119851 \t\t Validation Loss: 0.5124614651386554\n",
            "\t\t Validation Loss Decreased(0.621622--->0.512461) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.02165203747289205 \t\t Validation Loss: 0.39608873885411483\n",
            "\t\t Validation Loss Decreased(0.512461--->0.396089) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.01575935137697269 \t\t Validation Loss: 0.3156984712068851\n",
            "\t\t Validation Loss Decreased(0.396089--->0.315698) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.011910958247057893 \t\t Validation Loss: 0.24458774121908042\n",
            "\t\t Validation Loss Decreased(0.315698--->0.244588) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.009920463821775204 \t\t Validation Loss: 0.19327858892770913\n",
            "\t\t Validation Loss Decreased(0.244588--->0.193279) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.007976541616576346 \t\t Validation Loss: 0.15103106573224068\n",
            "\t\t Validation Loss Decreased(0.193279--->0.151031) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.006739048457175896 \t\t Validation Loss: 0.119976156606124\n",
            "\t\t Validation Loss Decreased(0.151031--->0.119976) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.006121670830415914 \t\t Validation Loss: 0.09674481488764286\n",
            "\t\t Validation Loss Decreased(0.119976--->0.096745) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.005135892173023643 \t\t Validation Loss: 0.07775926919510731\n",
            "\t\t Validation Loss Decreased(0.096745--->0.077759) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00463886746689929 \t\t Validation Loss: 0.0634344989577165\n",
            "\t\t Validation Loss Decreased(0.077759--->0.063434) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.004121096777993985 \t\t Validation Loss: 0.05271886783437087\n",
            "\t\t Validation Loss Decreased(0.063434--->0.052719) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003958908057570256 \t\t Validation Loss: 0.04451502494227428\n",
            "\t\t Validation Loss Decreased(0.052719--->0.044515) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003810453200335237 \t\t Validation Loss: 0.03942632438758245\n",
            "\t\t Validation Loss Decreased(0.044515--->0.039426) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003697141237254884 \t\t Validation Loss: 0.034310101746366575\n",
            "\t\t Validation Loss Decreased(0.039426--->0.034310) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0034202221667746432 \t\t Validation Loss: 0.031005531621093933\n",
            "\t\t Validation Loss Decreased(0.034310--->0.031006) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0035946474936035637 \t\t Validation Loss: 0.028658109788711254\n",
            "\t\t Validation Loss Decreased(0.031006--->0.028658) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0035306423103336143 \t\t Validation Loss: 0.02660702713407003\n",
            "\t\t Validation Loss Decreased(0.028658--->0.026607) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0033485007683142416 \t\t Validation Loss: 0.025534196828420345\n",
            "\t\t Validation Loss Decreased(0.026607--->0.025534) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0034554040333180614 \t\t Validation Loss: 0.02399228635029151\n",
            "\t\t Validation Loss Decreased(0.025534--->0.023992) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003319285257807555 \t\t Validation Loss: 0.02322709384875802\n",
            "\t\t Validation Loss Decreased(0.023992--->0.023227) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032843350892181734 \t\t Validation Loss: 0.022198965020764332\n",
            "\t\t Validation Loss Decreased(0.023227--->0.022199) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032440216950694652 \t\t Validation Loss: 0.021395007721506633\n",
            "\t\t Validation Loss Decreased(0.022199--->0.021395) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032256431452854463 \t\t Validation Loss: 0.021197829753733598\n",
            "\t\t Validation Loss Decreased(0.021395--->0.021198) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032148328059151566 \t\t Validation Loss: 0.020779727385021173\n",
            "\t\t Validation Loss Decreased(0.021198--->0.020780) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0033284925998855585 \t\t Validation Loss: 0.019956015336972017\n",
            "\t\t Validation Loss Decreased(0.020780--->0.019956) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0033033356696644143 \t\t Validation Loss: 0.01963239019879928\n",
            "\t\t Validation Loss Decreased(0.019956--->0.019632) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029001509097429947 \t\t Validation Loss: 0.01846172033737485\n",
            "\t\t Validation Loss Decreased(0.019632--->0.018462) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0031246405867293376 \t\t Validation Loss: 0.018197715282440186\n",
            "\t\t Validation Loss Decreased(0.018462--->0.018198) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003163598622572986 \t\t Validation Loss: 0.01607303018681705\n",
            "\t\t Validation Loss Decreased(0.018198--->0.016073) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003105941731986162 \t\t Validation Loss: 0.01652960554481699\n",
            "Epoch 37 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032337170198081513 \t\t Validation Loss: 0.01700422474039862\n",
            "Epoch 38 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032440390448853678 \t\t Validation Loss: 0.015567560644390492\n",
            "\t\t Validation Loss Decreased(0.016073--->0.015568) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0031662793866220258 \t\t Validation Loss: 0.015450605262930576\n",
            "\t\t Validation Loss Decreased(0.015568--->0.015451) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002984020686739854 \t\t Validation Loss: 0.015188813281173889\n",
            "\t\t Validation Loss Decreased(0.015451--->0.015189) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00320725778363192 \t\t Validation Loss: 0.015187824718081035\n",
            "\t\t Validation Loss Decreased(0.015189--->0.015188) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003122909490893771 \t\t Validation Loss: 0.013744558995732894\n",
            "\t\t Validation Loss Decreased(0.015188--->0.013745) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030261207264973907 \t\t Validation Loss: 0.012977851268190604\n",
            "\t\t Validation Loss Decreased(0.013745--->0.012978) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029101164443607165 \t\t Validation Loss: 0.012918005178037744\n",
            "\t\t Validation Loss Decreased(0.012978--->0.012918) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002997775881463108 \t\t Validation Loss: 0.012372584028456073\n",
            "\t\t Validation Loss Decreased(0.012918--->0.012373) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003087650990818401 \t\t Validation Loss: 0.01251371937374083\n",
            "Epoch 47 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003137674678991725 \t\t Validation Loss: 0.012906462634698702\n",
            "Epoch 48 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030698693967801897 \t\t Validation Loss: 0.01176428347109602\n",
            "\t\t Validation Loss Decreased(0.012373--->0.011764) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029486202582956066 \t\t Validation Loss: 0.010889341266682515\n",
            "\t\t Validation Loss Decreased(0.011764--->0.010889) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00303141976395194 \t\t Validation Loss: 0.010963901716212813\n",
            "Epoch 51 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002975249576470437 \t\t Validation Loss: 0.011081386984397586\n",
            "Epoch 52 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030535709315822838 \t\t Validation Loss: 0.011584666772530628\n",
            "Epoch 53 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032426635585284815 \t\t Validation Loss: 0.011230314616113901\n",
            "Epoch 54 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0031352439467367287 \t\t Validation Loss: 0.011706808420757834\n",
            "Epoch 55 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002841790732172494 \t\t Validation Loss: 0.010156594700394915\n",
            "\t\t Validation Loss Decreased(0.010889--->0.010157) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029294090282574697 \t\t Validation Loss: 0.009337952736621866\n",
            "\t\t Validation Loss Decreased(0.010157--->0.009338) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00277060891164945 \t\t Validation Loss: 0.008106155231451759\n",
            "\t\t Validation Loss Decreased(0.009338--->0.008106) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002905951425861064 \t\t Validation Loss: 0.008109639416663693\n",
            "Epoch 59 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002857011794804822 \t\t Validation Loss: 0.008618508662598638\n",
            "Epoch 60 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003132896968226477 \t\t Validation Loss: 0.008319797513719935\n",
            "Epoch 61 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003045472376224761 \t\t Validation Loss: 0.00890762536213375\n",
            "Epoch 62 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030637400152438597 \t\t Validation Loss: 0.008743506748802386\n",
            "Epoch 63 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002866806511175693 \t\t Validation Loss: 0.007769549104313438\n",
            "\t\t Validation Loss Decreased(0.008106--->0.007770) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002932931705554192 \t\t Validation Loss: 0.00816479201715153\n",
            "Epoch 65 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003071174168694966 \t\t Validation Loss: 0.007926667789713694\n",
            "Epoch 66 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030197611501805383 \t\t Validation Loss: 0.008514922022676239\n",
            "Epoch 67 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003015584474416903 \t\t Validation Loss: 0.006851747775307069\n",
            "\t\t Validation Loss Decreased(0.007770--->0.006852) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029979783591713655 \t\t Validation Loss: 0.00737737613515212\n",
            "Epoch 69 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029917136707808822 \t\t Validation Loss: 0.006956421537324786\n",
            "Epoch 70 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00281524847928636 \t\t Validation Loss: 0.007017206651373551\n",
            "Epoch 71 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002757984881345938 \t\t Validation Loss: 0.006387895188079431\n",
            "\t\t Validation Loss Decreased(0.006852--->0.006388) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002825390007913213 \t\t Validation Loss: 0.005971261586707372\n",
            "\t\t Validation Loss Decreased(0.006388--->0.005971) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002936749538167606 \t\t Validation Loss: 0.0063349839944679\n",
            "Epoch 74 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030289349984692258 \t\t Validation Loss: 0.006857757456600666\n",
            "Epoch 75 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028709425288881802 \t\t Validation Loss: 0.0060381632960902955\n",
            "Epoch 76 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002827950382787684 \t\t Validation Loss: 0.006146333968410125\n",
            "Epoch 77 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002779252389036874 \t\t Validation Loss: 0.005710003790087425\n",
            "\t\t Validation Loss Decreased(0.005971--->0.005710) \t Saving The Model\n",
            "Epoch 78 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027521523706791166 \t\t Validation Loss: 0.00486734257831883\n",
            "\t\t Validation Loss Decreased(0.005710--->0.004867) \t Saving The Model\n",
            "Epoch 79 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029553484393447333 \t\t Validation Loss: 0.00464493790283226\n",
            "\t\t Validation Loss Decreased(0.004867--->0.004645) \t Saving The Model\n",
            "Epoch 80 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002724940418989423 \t\t Validation Loss: 0.004398275014514534\n",
            "\t\t Validation Loss Decreased(0.004645--->0.004398) \t Saving The Model\n",
            "Epoch 81 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003118714193398183 \t\t Validation Loss: 0.0049468279511739426\n",
            "Epoch 82 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027799992287229442 \t\t Validation Loss: 0.004665659937577752\n",
            "Epoch 83 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002743338868714164 \t\t Validation Loss: 0.005333031500832966\n",
            "Epoch 84 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003026003884843777 \t\t Validation Loss: 0.004377085590161956\n",
            "\t\t Validation Loss Decreased(0.004398--->0.004377) \t Saving The Model\n",
            "Epoch 85 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028122202925874875 \t\t Validation Loss: 0.004012862739797968\n",
            "\t\t Validation Loss Decreased(0.004377--->0.004013) \t Saving The Model\n",
            "Epoch 86 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002851627129948119 \t\t Validation Loss: 0.0042901471740781115\n",
            "Epoch 87 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002997521436899095 \t\t Validation Loss: 0.003743617168556039\n",
            "\t\t Validation Loss Decreased(0.004013--->0.003744) \t Saving The Model\n",
            "Epoch 88 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030645790157487265 \t\t Validation Loss: 0.004433049274107011\n",
            "Epoch 89 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002779566319470571 \t\t Validation Loss: 0.004059309381633424\n",
            "Epoch 90 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029729381581214635 \t\t Validation Loss: 0.004846820742106781\n",
            "Epoch 91 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030407028263308915 \t\t Validation Loss: 0.004300518785245144\n",
            "Epoch 92 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029110961713249214 \t\t Validation Loss: 0.004853189851229007\n",
            "Epoch 93 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027466099827260886 \t\t Validation Loss: 0.004523573828359636\n",
            "Epoch 94 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003173750579810223 \t\t Validation Loss: 0.0043988950066984845\n",
            "Epoch 95 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00280003620915719 \t\t Validation Loss: 0.004779402883006976\n",
            "Epoch 96 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027769365138688904 \t\t Validation Loss: 0.0042543341942991204\n",
            "Epoch 97 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027153176708600004 \t\t Validation Loss: 0.003186432340254004\n",
            "\t\t Validation Loss Decreased(0.003744--->0.003186) \t Saving The Model\n",
            "Epoch 98 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027152068419004412 \t\t Validation Loss: 0.0031781231250184085\n",
            "\t\t Validation Loss Decreased(0.003186--->0.003178) \t Saving The Model\n",
            "Epoch 99 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002645862894447369 \t\t Validation Loss: 0.0027712733717635274\n",
            "\t\t Validation Loss Decreased(0.003178--->0.002771) \t Saving The Model\n",
            "Epoch 100 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026994506675888457 \t\t Validation Loss: 0.0026249344944237517\n",
            "\t\t Validation Loss Decreased(0.002771--->0.002625) \t Saving The Model\n",
            "Epoch 101 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026289948822326354 \t\t Validation Loss: 0.0027878778950812724\n",
            "Epoch 102 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002730533412997485 \t\t Validation Loss: 0.002192893414758146\n",
            "\t\t Validation Loss Decreased(0.002625--->0.002193) \t Saving The Model\n",
            "Epoch 103 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002863773058256688 \t\t Validation Loss: 0.002832075930200517\n",
            "Epoch 104 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002831367966152627 \t\t Validation Loss: 0.0023867783333676364\n",
            "Epoch 105 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027982962232813036 \t\t Validation Loss: 0.002545608628469591\n",
            "Epoch 106 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002781760701091608 \t\t Validation Loss: 0.0024045833852142096\n",
            "Epoch 107 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029152870002026494 \t\t Validation Loss: 0.0024646261545757833\n",
            "Epoch 108 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002709523186314146 \t\t Validation Loss: 0.002785325202589425\n",
            "Epoch 109 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029296935221331346 \t\t Validation Loss: 0.002663562772795558\n",
            "Epoch 110 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002800762771339332 \t\t Validation Loss: 0.00232559556248956\n",
            "Epoch 111 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002559910589715818 \t\t Validation Loss: 0.0020898192171723796\n",
            "\t\t Validation Loss Decreased(0.002193--->0.002090) \t Saving The Model\n",
            "Epoch 112 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002649975083988613 \t\t Validation Loss: 0.002004022647340137\n",
            "\t\t Validation Loss Decreased(0.002090--->0.002004) \t Saving The Model\n",
            "Epoch 113 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00277837977282124 \t\t Validation Loss: 0.0020147389732301235\n",
            "Epoch 114 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002702740494577164 \t\t Validation Loss: 0.0020291414201402892\n",
            "Epoch 115 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027287675939324137 \t\t Validation Loss: 0.0017546835050989802\n",
            "\t\t Validation Loss Decreased(0.002004--->0.001755) \t Saving The Model\n",
            "Epoch 116 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002683556739341568 \t\t Validation Loss: 0.0018777816305653406\n",
            "Epoch 117 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026312286641473905 \t\t Validation Loss: 0.0015712448870404982\n",
            "\t\t Validation Loss Decreased(0.001755--->0.001571) \t Saving The Model\n",
            "Epoch 118 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026166992721374373 \t\t Validation Loss: 0.0015984094665887265\n",
            "Epoch 119 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002724335337768787 \t\t Validation Loss: 0.0016982757504313039\n",
            "Epoch 120 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028015081338093593 \t\t Validation Loss: 0.0016694950709979122\n",
            "Epoch 121 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002670469294251824 \t\t Validation Loss: 0.0017980959069413634\n",
            "Epoch 122 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028277580701821558 \t\t Validation Loss: 0.0016896954039111733\n",
            "Epoch 123 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025400447602559988 \t\t Validation Loss: 0.001573236242760546\n",
            "Epoch 124 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002832728305932235 \t\t Validation Loss: 0.0018071530816646723\n",
            "Epoch 125 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026465593632688193 \t\t Validation Loss: 0.001711237530868787\n",
            "Epoch 126 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028793158676676655 \t\t Validation Loss: 0.0014420295975959073\n",
            "\t\t Validation Loss Decreased(0.001571--->0.001442) \t Saving The Model\n",
            "Epoch 127 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027721624701862804 \t\t Validation Loss: 0.0014089679148478003\n",
            "\t\t Validation Loss Decreased(0.001442--->0.001409) \t Saving The Model\n",
            "Epoch 128 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002437111663606924 \t\t Validation Loss: 0.0013636667818690722\n",
            "\t\t Validation Loss Decreased(0.001409--->0.001364) \t Saving The Model\n",
            "Epoch 129 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002578108020070185 \t\t Validation Loss: 0.0013720745919272304\n",
            "Epoch 130 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002661492338095716 \t\t Validation Loss: 0.0014472880732053174\n",
            "Epoch 131 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002747050060173245 \t\t Validation Loss: 0.0015106043921640287\n",
            "Epoch 132 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002698290388326387 \t\t Validation Loss: 0.001334509411558079\n",
            "\t\t Validation Loss Decreased(0.001364--->0.001335) \t Saving The Model\n",
            "Epoch 133 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00258762260027406 \t\t Validation Loss: 0.0013316828251565592\n",
            "\t\t Validation Loss Decreased(0.001335--->0.001332) \t Saving The Model\n",
            "Epoch 134 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026785943545464025 \t\t Validation Loss: 0.0013359716350141054\n",
            "Epoch 135 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00253648662112175 \t\t Validation Loss: 0.0013322786925038179\n",
            "Epoch 136 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002788670886484151 \t\t Validation Loss: 0.00135481124296068\n",
            "Epoch 137 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026525535740310677 \t\t Validation Loss: 0.001369994818770255\n",
            "Epoch 138 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002697416517499683 \t\t Validation Loss: 0.0013181652421525752\n",
            "\t\t Validation Loss Decreased(0.001332--->0.001318) \t Saving The Model\n",
            "Epoch 139 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002395436492578655 \t\t Validation Loss: 0.0013935832080628292\n",
            "Epoch 140 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002683492780134485 \t\t Validation Loss: 0.0014206646905782132\n",
            "Epoch 141 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029314052285248967 \t\t Validation Loss: 0.001457628855580249\n",
            "Epoch 142 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027454190327458688 \t\t Validation Loss: 0.0013025986845605075\n",
            "\t\t Validation Loss Decreased(0.001318--->0.001303) \t Saving The Model\n",
            "Epoch 143 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002736772838476542 \t\t Validation Loss: 0.0013035199819849087\n",
            "Epoch 144 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00253432580722949 \t\t Validation Loss: 0.0012910416945277785\n",
            "\t\t Validation Loss Decreased(0.001303--->0.001291) \t Saving The Model\n",
            "Epoch 145 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027410429257370933 \t\t Validation Loss: 0.0012962032163229126\n",
            "Epoch 146 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025656110751820175 \t\t Validation Loss: 0.0012976026583391314\n",
            "Epoch 147 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026256462474152243 \t\t Validation Loss: 0.0012954096623266546\n",
            "Epoch 148 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002686413227599366 \t\t Validation Loss: 0.0012944613005786848\n",
            "Epoch 149 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002659494865912239 \t\t Validation Loss: 0.0013262087125510264\n",
            "Epoch 150 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026844968371143615 \t\t Validation Loss: 0.0013190075987949967\n",
            "Epoch 151 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002508998545660713 \t\t Validation Loss: 0.0012675640822495692\n",
            "\t\t Validation Loss Decreased(0.001291--->0.001268) \t Saving The Model\n",
            "Epoch 152 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026342387891663994 \t\t Validation Loss: 0.0013151569202399026\n",
            "Epoch 153 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002484311263951649 \t\t Validation Loss: 0.0013603350022234595\n",
            "Epoch 154 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026556822216465465 \t\t Validation Loss: 0.001364305936686623\n",
            "Epoch 155 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002582579617533279 \t\t Validation Loss: 0.001432937791553111\n",
            "Epoch 156 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002700358590683774 \t\t Validation Loss: 0.0013729651971236588\n",
            "Epoch 157 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002626735223079654 \t\t Validation Loss: 0.0012992322131489904\n",
            "Epoch 158 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002548334762970036 \t\t Validation Loss: 0.0012594558065757155\n",
            "\t\t Validation Loss Decreased(0.001268--->0.001259) \t Saving The Model\n",
            "Epoch 159 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025575465893976994 \t\t Validation Loss: 0.0012750579715849687\n",
            "Epoch 160 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024893408959001505 \t\t Validation Loss: 0.0012614486913662404\n",
            "Epoch 161 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002553170339229542 \t\t Validation Loss: 0.001539233242510818\n",
            "Epoch 162 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025008649194009 \t\t Validation Loss: 0.0013030089086476858\n",
            "Epoch 163 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025357794719811128 \t\t Validation Loss: 0.0018837872888140667\n",
            "Epoch 164 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002407990084533152 \t\t Validation Loss: 0.0012910724986935607\n",
            "Epoch 165 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002645612315186677 \t\t Validation Loss: 0.0020428918919955883\n",
            "Epoch 166 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002524275323250205 \t\t Validation Loss: 0.0012621592278054988\n",
            "Epoch 167 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027939861953396954 \t\t Validation Loss: 0.002384754663440757\n",
            "Epoch 168 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023459619826737893 \t\t Validation Loss: 0.0012622642836784227\n",
            "Epoch 169 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002874862700008561 \t\t Validation Loss: 0.0021695621432557414\n",
            "Epoch 170 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002656725746566883 \t\t Validation Loss: 0.0014033071840038667\n",
            "Epoch 171 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028868600587687782 \t\t Validation Loss: 0.0022657097993382756\n",
            "Epoch 172 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025842803108782784 \t\t Validation Loss: 0.0017685688183589194\n",
            "Epoch 173 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025773013853845565 \t\t Validation Loss: 0.0016623660395942773\n",
            "Epoch 174 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025436822356454825 \t\t Validation Loss: 0.0012581447361019226\n",
            "\t\t Validation Loss Decreased(0.001259--->0.001258) \t Saving The Model\n",
            "Epoch 175 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027456443136985842 \t\t Validation Loss: 0.0015356789316193988\n",
            "Epoch 176 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025716678623261082 \t\t Validation Loss: 0.0014640707824862776\n",
            "Epoch 177 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002607450383948162 \t\t Validation Loss: 0.0016131551114645286\n",
            "Epoch 178 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024818154964344323 \t\t Validation Loss: 0.0012764134254002084\n",
            "Epoch 179 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026186013828673575 \t\t Validation Loss: 0.0020088950158634153\n",
            "Epoch 180 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024626509544464783 \t\t Validation Loss: 0.001503766234516381\n",
            "Epoch 181 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024879570819148038 \t\t Validation Loss: 0.0019467306421854748\n",
            "Epoch 182 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024695050559711417 \t\t Validation Loss: 0.0016794993011549546\n",
            "Epoch 183 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026102239514600384 \t\t Validation Loss: 0.002151550228098551\n",
            "Epoch 184 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026111224370479985 \t\t Validation Loss: 0.002069879679206329\n",
            "Epoch 185 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002538882498000119 \t\t Validation Loss: 0.002502466426589168\n",
            "Epoch 186 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025450236933318446 \t\t Validation Loss: 0.0020749135023484435\n",
            "Epoch 187 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002803754176302637 \t\t Validation Loss: 0.002561916158391306\n",
            "Epoch 188 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026942579676648855 \t\t Validation Loss: 0.002082818156430641\n",
            "Epoch 189 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026002903641956683 \t\t Validation Loss: 0.002223555121767836\n",
            "Epoch 190 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002477968221756856 \t\t Validation Loss: 0.0018732214364438103\n",
            "Epoch 191 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026955230608330786 \t\t Validation Loss: 0.0015765737536220024\n",
            "Epoch 192 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024283161423030637 \t\t Validation Loss: 0.0019540812690348294\n",
            "Epoch 193 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002337549300235067 \t\t Validation Loss: 0.002171066759798962\n",
            "Epoch 194 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002532960437993343 \t\t Validation Loss: 0.0017766268417919772\n",
            "Epoch 195 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024925776972825565 \t\t Validation Loss: 0.0019515814335766034\n",
            "Epoch 196 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025115530006587505 \t\t Validation Loss: 0.0020686755328474995\n",
            "Epoch 197 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024487445291408614 \t\t Validation Loss: 0.0018823927810164886\n",
            "Epoch 198 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024664588390560065 \t\t Validation Loss: 0.0020127240635562115\n",
            "Epoch 199 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025610983154635775 \t\t Validation Loss: 0.0016857816842205536\n",
            "Epoch 200 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002632711500376211 \t\t Validation Loss: 0.0021919481626425227\n",
            "Epoch 201 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024917506647447276 \t\t Validation Loss: 0.0018732076837645413\n",
            "Epoch 202 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002537263702047435 \t\t Validation Loss: 0.0021039184424667978\n",
            "Epoch 203 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026623163658596976 \t\t Validation Loss: 0.0015307182687907838\n",
            "Epoch 204 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023688700756473416 \t\t Validation Loss: 0.002092107808074126\n",
            "Epoch 205 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023598134898379242 \t\t Validation Loss: 0.001752832252979995\n",
            "Epoch 206 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00250070691627853 \t\t Validation Loss: 0.0024166797834018674\n",
            "Epoch 207 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002394340397488024 \t\t Validation Loss: 0.0013809511796213114\n",
            "Epoch 208 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002600892089823311 \t\t Validation Loss: 0.0033846267302019093\n",
            "Epoch 209 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023210389158889854 \t\t Validation Loss: 0.0013015404804788816\n",
            "Epoch 210 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026839647034648806 \t\t Validation Loss: 0.0037018521420227792\n",
            "Epoch 211 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025261874338989524 \t\t Validation Loss: 0.0012921303568873554\n",
            "Epoch 212 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030554298050324055 \t\t Validation Loss: 0.004083511867345526\n",
            "Epoch 213 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025112587420546726 \t\t Validation Loss: 0.0015343721472443296\n",
            "Epoch 214 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0034324848832487055 \t\t Validation Loss: 0.004276699440267224\n",
            "Epoch 215 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026676651071505368 \t\t Validation Loss: 0.0014631886823246111\n",
            "Epoch 216 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003496120364493313 \t\t Validation Loss: 0.006425252721573298\n",
            "Epoch 217 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027406591264455505 \t\t Validation Loss: 0.0019973801860872368\n",
            "Epoch 218 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003329063831778193 \t\t Validation Loss: 0.005573540102117336\n",
            "Epoch 219 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027599783261885516 \t\t Validation Loss: 0.002151590852568356\n",
            "Epoch 220 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0034840552048203913 \t\t Validation Loss: 0.006725058031196778\n",
            "Epoch 221 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0031968125069790796 \t\t Validation Loss: 0.0022214854028649055\n",
            "Epoch 222 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0034790319025617194 \t\t Validation Loss: 0.007959074788511945\n",
            "Epoch 223 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003040695540934197 \t\t Validation Loss: 0.0026910513137968686\n",
            "Epoch 224 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003630052092548057 \t\t Validation Loss: 0.006886896659405186\n",
            "Epoch 225 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002810941109232403 \t\t Validation Loss: 0.0025144397245290186\n",
            "Epoch 226 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003485658289424169 \t\t Validation Loss: 0.005456995971214313\n",
            "Epoch 227 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.003111856215557939 \t\t Validation Loss: 0.0027032885610914\n",
            "Epoch 228 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0032369440116265134 \t\t Validation Loss: 0.005832470774364013\n",
            "Epoch 229 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028554771529711986 \t\t Validation Loss: 0.0018524816259741783\n",
            "Epoch 230 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002898712961846409 \t\t Validation Loss: 0.004027148843026505\n",
            "Epoch 231 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002757285479330332 \t\t Validation Loss: 0.0014563894275432597\n",
            "Epoch 232 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025733918717722533 \t\t Validation Loss: 0.003695896486393534\n",
            "Epoch 233 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024778536863851587 \t\t Validation Loss: 0.0016283303311166281\n",
            "Epoch 234 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025920381178022238 \t\t Validation Loss: 0.0037038974875870804\n",
            "Epoch 235 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002360998303629458 \t\t Validation Loss: 0.0014460784463713376\n",
            "Epoch 236 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027373181357098794 \t\t Validation Loss: 0.0030164257757580625\n",
            "Epoch 237 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025672265595907497 \t\t Validation Loss: 0.0016629335407812435\n",
            "Epoch 238 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002744630675265767 \t\t Validation Loss: 0.003980303953330104\n",
            "Epoch 239 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002488917623948608 \t\t Validation Loss: 0.0019070925638796045\n",
            "Epoch 240 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027461274105401055 \t\t Validation Loss: 0.003777046025229188\n",
            "Epoch 241 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025147415817136295 \t\t Validation Loss: 0.0015761949112997032\n",
            "Epoch 242 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028585731885647655 \t\t Validation Loss: 0.0029856007671556794\n",
            "Epoch 243 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023726251312984606 \t\t Validation Loss: 0.0015412656805263115\n",
            "Epoch 244 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028189066703104087 \t\t Validation Loss: 0.0037814943084063437\n",
            "Epoch 245 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023900928806412864 \t\t Validation Loss: 0.0013800376618746668\n",
            "Epoch 246 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002608443733864791 \t\t Validation Loss: 0.0033071142567608217\n",
            "Epoch 247 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025258613514361553 \t\t Validation Loss: 0.0017633088416634845\n",
            "Epoch 248 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00258337621297058 \t\t Validation Loss: 0.0035153261672418853\n",
            "Epoch 249 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002488872199948575 \t\t Validation Loss: 0.0013487134464622403\n",
            "Epoch 250 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025811210963749202 \t\t Validation Loss: 0.0032871637869483004\n",
            "Epoch 251 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002494507159320386 \t\t Validation Loss: 0.0018988660214325557\n",
            "Epoch 252 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002742795096678861 \t\t Validation Loss: 0.0035407388170894524\n",
            "Epoch 253 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025600428265091534 \t\t Validation Loss: 0.0014859606341745418\n",
            "Epoch 254 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028308990900719386 \t\t Validation Loss: 0.003695299633993552\n",
            "Epoch 255 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002537487183282202 \t\t Validation Loss: 0.0014553672965401067\n",
            "Epoch 256 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026661166449303965 \t\t Validation Loss: 0.0037946384041928328\n",
            "Epoch 257 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022749665548798402 \t\t Validation Loss: 0.0014131576172076166\n",
            "Epoch 258 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002873523689403727 \t\t Validation Loss: 0.0035338317438100395\n",
            "Epoch 259 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024066635744167944 \t\t Validation Loss: 0.0015959701402327763\n",
            "Epoch 260 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027017368497704534 \t\t Validation Loss: 0.003546106378332927\n",
            "Epoch 261 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023696874158230384 \t\t Validation Loss: 0.0016817385868097728\n",
            "Epoch 262 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002738594327663147 \t\t Validation Loss: 0.004083198092233103\n",
            "Epoch 263 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023983674347274812 \t\t Validation Loss: 0.001428846892220183\n",
            "Epoch 264 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028819144535117557 \t\t Validation Loss: 0.004412198143724639\n",
            "Epoch 265 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024242733740499497 \t\t Validation Loss: 0.0013883090891445486\n",
            "Epoch 266 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00266174874992135 \t\t Validation Loss: 0.0043238401950265355\n",
            "Epoch 267 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002634308475535363 \t\t Validation Loss: 0.001551117398776114\n",
            "Epoch 268 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028156999276787347 \t\t Validation Loss: 0.004892379179811821\n",
            "Epoch 269 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024147558079503878 \t\t Validation Loss: 0.001652451205210617\n",
            "Epoch 270 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026812515586513925 \t\t Validation Loss: 0.004369900167848055\n",
            "Epoch 271 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024492105994549755 \t\t Validation Loss: 0.0017050371299354504\n",
            "Epoch 272 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028157180279015084 \t\t Validation Loss: 0.004188171005807817\n",
            "Epoch 273 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002603469987563487 \t\t Validation Loss: 0.0015739638877746004\n",
            "Epoch 274 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025901452518647186 \t\t Validation Loss: 0.004775062481013055\n",
            "Epoch 275 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023684057838132453 \t\t Validation Loss: 0.001749318057241348\n",
            "Epoch 276 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029995966233892016 \t\t Validation Loss: 0.0053312630894092414\n",
            "Epoch 277 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025800632067477782 \t\t Validation Loss: 0.0025883022821150147\n",
            "Epoch 278 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002896463905926794 \t\t Validation Loss: 0.004175903060688422\n",
            "Epoch 279 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026019158980783983 \t\t Validation Loss: 0.002259757835417986\n",
            "Epoch 280 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0029221903021459947 \t\t Validation Loss: 0.004825332479623075\n",
            "Epoch 281 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025687205598916153 \t\t Validation Loss: 0.0022286664640817503\n",
            "Epoch 282 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00259643023034457 \t\t Validation Loss: 0.004922656646858041\n",
            "Epoch 283 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024249890513995008 \t\t Validation Loss: 0.0018175891421448726\n",
            "Epoch 284 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028380848092933162 \t\t Validation Loss: 0.0037319599382149484\n",
            "Epoch 285 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024112658858349597 \t\t Validation Loss: 0.0013679683548756517\n",
            "Epoch 286 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026248488252718204 \t\t Validation Loss: 0.003623313861540877\n",
            "Epoch 287 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023093966065553598 \t\t Validation Loss: 0.0013305271030940975\n",
            "Epoch 288 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023892903094796616 \t\t Validation Loss: 0.0028644781559705734\n",
            "Epoch 289 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00227893065862559 \t\t Validation Loss: 0.0013399725161994307\n",
            "Epoch 290 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024821353365188916 \t\t Validation Loss: 0.003108541476719368\n",
            "Epoch 291 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022193645992366648 \t\t Validation Loss: 0.0014125358653613008\n",
            "Epoch 292 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024621573684271425 \t\t Validation Loss: 0.0027135967393405735\n",
            "Epoch 293 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002148711946918755 \t\t Validation Loss: 0.0013315514079295099\n",
            "Epoch 294 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026839642346572334 \t\t Validation Loss: 0.0026438022032380104\n",
            "Epoch 295 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022458656442452323 \t\t Validation Loss: 0.0013468269563208406\n",
            "Epoch 296 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002508123850487676 \t\t Validation Loss: 0.0029515496571548283\n",
            "Epoch 297 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002384052885620779 \t\t Validation Loss: 0.0013758582913746627\n",
            "Epoch 298 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025407251496044163 \t\t Validation Loss: 0.003562982355316098\n",
            "Epoch 299 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022365980454393335 \t\t Validation Loss: 0.0014318974169257742\n",
            "Epoch 300 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025895622769697897 \t\t Validation Loss: 0.00322732889953141\n",
            "Epoch 301 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002260738571577177 \t\t Validation Loss: 0.0015345604087297733\n",
            "Epoch 302 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028585735999513417 \t\t Validation Loss: 0.0031028870681229117\n",
            "Epoch 303 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002620133865511397 \t\t Validation Loss: 0.001364613818273378\n",
            "Epoch 304 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025730474562560383 \t\t Validation Loss: 0.003907279406960767\n",
            "Epoch 305 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025709282065666207 \t\t Validation Loss: 0.0013448180800948578\n",
            "Epoch 306 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002633478891299534 \t\t Validation Loss: 0.004338273742737679\n",
            "Epoch 307 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002317805933401089 \t\t Validation Loss: 0.0013572343763931154\n",
            "Epoch 308 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002538238576537854 \t\t Validation Loss: 0.003634953128102307\n",
            "Epoch 309 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024199098870913323 \t\t Validation Loss: 0.001496777569767661\n",
            "Epoch 310 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002830540285464628 \t\t Validation Loss: 0.0060484523061089795\n",
            "Epoch 311 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002596028328470483 \t\t Validation Loss: 0.0024040885753213214\n",
            "Epoch 312 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030880214762873948 \t\t Validation Loss: 0.005272549935258352\n",
            "Epoch 313 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025424688308801806 \t\t Validation Loss: 0.002071543435494487\n",
            "Epoch 314 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002931156799801298 \t\t Validation Loss: 0.006129905706844651\n",
            "Epoch 315 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026945494291548792 \t\t Validation Loss: 0.0018893101772006887\n",
            "Epoch 316 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0030710268954469546 \t\t Validation Loss: 0.0069326596836057994\n",
            "Epoch 317 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025234452190783783 \t\t Validation Loss: 0.0021705902092015515\n",
            "Epoch 318 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002971028322291032 \t\t Validation Loss: 0.005976143973664596\n",
            "Epoch 319 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025532379686656233 \t\t Validation Loss: 0.0020199562602031687\n",
            "Epoch 320 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00281901505848116 \t\t Validation Loss: 0.005290972749487712\n",
            "Epoch 321 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00261707285047836 \t\t Validation Loss: 0.002049844479188323\n",
            "Epoch 322 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027419524172925063 \t\t Validation Loss: 0.00601595792417916\n",
            "Epoch 323 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025531662867177983 \t\t Validation Loss: 0.002125760934387262\n",
            "Epoch 324 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002855551356726603 \t\t Validation Loss: 0.004902368886038088\n",
            "Epoch 325 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024863768328056745 \t\t Validation Loss: 0.001678612593633051\n",
            "Epoch 326 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028128622766785526 \t\t Validation Loss: 0.004257337073795497\n",
            "Epoch 327 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002484599718308026 \t\t Validation Loss: 0.00152459123189776\n",
            "Epoch 328 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002553919821071464 \t\t Validation Loss: 0.004192419278507049\n",
            "Epoch 329 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023499522468301694 \t\t Validation Loss: 0.0014801905535233135\n",
            "Epoch 330 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002550531658532752 \t\t Validation Loss: 0.004630066526050751\n",
            "Epoch 331 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024132913787144462 \t\t Validation Loss: 0.0015380241182776024\n",
            "Epoch 332 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026278097160487763 \t\t Validation Loss: 0.004564400028007535\n",
            "Epoch 333 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022771878536110997 \t\t Validation Loss: 0.001515806585442848\n",
            "Epoch 334 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002864366184882316 \t\t Validation Loss: 0.0046122647606982635\n",
            "Epoch 335 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002590904024907866 \t\t Validation Loss: 0.0014962381459414386\n",
            "Epoch 336 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002564844930217274 \t\t Validation Loss: 0.00517957267136528\n",
            "Epoch 337 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023838602670317364 \t\t Validation Loss: 0.0016412694150438677\n",
            "Epoch 338 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028593108149220212 \t\t Validation Loss: 0.00554502413321573\n",
            "Epoch 339 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023223615968851626 \t\t Validation Loss: 0.001557413273705886\n",
            "Epoch 340 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028782543471410267 \t\t Validation Loss: 0.004770586052193091\n",
            "Epoch 341 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024830792744241254 \t\t Validation Loss: 0.0015512519569780964\n",
            "Epoch 342 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026232730054510504 \t\t Validation Loss: 0.005025002200944493\n",
            "Epoch 343 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002520200689449101 \t\t Validation Loss: 0.0017811591969802976\n",
            "Epoch 344 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002546215207297103 \t\t Validation Loss: 0.00354857265483588\n",
            "Epoch 345 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002345333221947422 \t\t Validation Loss: 0.001417247267314591\n",
            "Epoch 346 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025090137362870314 \t\t Validation Loss: 0.0035439777420833707\n",
            "Epoch 347 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023805213890768385 \t\t Validation Loss: 0.0016059326187062722\n",
            "Epoch 348 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026008488517569226 \t\t Validation Loss: 0.004392961290879891\n",
            "Epoch 349 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022775773132043715 \t\t Validation Loss: 0.001395731089779964\n",
            "Epoch 350 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002531872269370266 \t\t Validation Loss: 0.005021877139090345\n",
            "Epoch 351 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022898065964565488 \t\t Validation Loss: 0.0014222607670280223\n",
            "Epoch 352 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002592892972028789 \t\t Validation Loss: 0.0048773027687835\n",
            "Epoch 353 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002363366512204143 \t\t Validation Loss: 0.001436696535585305\n",
            "Epoch 354 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026314330100069274 \t\t Validation Loss: 0.005128859596040387\n",
            "Epoch 355 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022156526668097925 \t\t Validation Loss: 0.0014820233829176198\n",
            "Epoch 356 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026911558926332037 \t\t Validation Loss: 0.005461015899737294\n",
            "Epoch 357 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002335855584730067 \t\t Validation Loss: 0.0014012963622987557\n",
            "Epoch 358 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002463339200255939 \t\t Validation Loss: 0.004451009552352703\n",
            "Epoch 359 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024783953889021396 \t\t Validation Loss: 0.0014934000523331074\n",
            "Epoch 360 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002630623946331341 \t\t Validation Loss: 0.005144087329077033\n",
            "Epoch 361 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024098980278280135 \t\t Validation Loss: 0.001396808168815019\n",
            "Epoch 362 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026773018856860093 \t\t Validation Loss: 0.0042578592144239405\n",
            "Epoch 363 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024134096661205025 \t\t Validation Loss: 0.001423219695705204\n",
            "Epoch 364 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002483730666131738 \t\t Validation Loss: 0.0034821575973182917\n",
            "Epoch 365 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023063139461383625 \t\t Validation Loss: 0.001673540252690705\n",
            "Epoch 366 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002598745155311819 \t\t Validation Loss: 0.004555486375465989\n",
            "Epoch 367 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002303785809655548 \t\t Validation Loss: 0.0014188805115946496\n",
            "Epoch 368 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024269967870094587 \t\t Validation Loss: 0.004000806903394942\n",
            "Epoch 369 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002160672126715449 \t\t Validation Loss: 0.0015810146623362715\n",
            "Epoch 370 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025925557892077376 \t\t Validation Loss: 0.003944013357305756\n",
            "Epoch 371 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023416874291551475 \t\t Validation Loss: 0.0015195292608740812\n",
            "Epoch 372 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024968921065305336 \t\t Validation Loss: 0.0044561756792693185\n",
            "Epoch 373 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021467969463383023 \t\t Validation Loss: 0.001396911386669112\n",
            "Epoch 374 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025949789867170054 \t\t Validation Loss: 0.004372446484362276\n",
            "Epoch 375 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022731425879346963 \t\t Validation Loss: 0.0014764301998254198\n",
            "Epoch 376 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00279686671234919 \t\t Validation Loss: 0.005055468457822616\n",
            "Epoch 377 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023255731621002022 \t\t Validation Loss: 0.0014360913639673246\n",
            "Epoch 378 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028295873639148637 \t\t Validation Loss: 0.00603217574266287\n",
            "Epoch 379 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023822029187293673 \t\t Validation Loss: 0.0014244079536113602\n",
            "Epoch 380 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025213921231821783 \t\t Validation Loss: 0.004324541525700345\n",
            "Epoch 381 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002334590705514357 \t\t Validation Loss: 0.0014925489130501563\n",
            "Epoch 382 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002706927912800598 \t\t Validation Loss: 0.005667034733610658\n",
            "Epoch 383 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002501510991400259 \t\t Validation Loss: 0.0015864490549294995\n",
            "Epoch 384 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002773930334385384 \t\t Validation Loss: 0.006801034126860591\n",
            "Epoch 385 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023780670662007825 \t\t Validation Loss: 0.0017068393152350416\n",
            "Epoch 386 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028371487105441455 \t\t Validation Loss: 0.006564838166993398\n",
            "Epoch 387 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026343921167977357 \t\t Validation Loss: 0.0017554327764978202\n",
            "Epoch 388 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002701875377710707 \t\t Validation Loss: 0.006254127732693003\n",
            "Epoch 389 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026276847080176544 \t\t Validation Loss: 0.002770644200679201\n",
            "Epoch 390 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002965605155426405 \t\t Validation Loss: 0.00714931280638736\n",
            "Epoch 391 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024876660459700064 \t\t Validation Loss: 0.002159150236716064\n",
            "Epoch 392 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0027743523509392667 \t\t Validation Loss: 0.006720586841066296\n",
            "Epoch 393 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002401299324717272 \t\t Validation Loss: 0.00220561045436905\n",
            "Epoch 394 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002874051939658317 \t\t Validation Loss: 0.005097111767659394\n",
            "Epoch 395 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024636983273651552 \t\t Validation Loss: 0.0014871009446393985\n",
            "Epoch 396 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025301987912531034 \t\t Validation Loss: 0.0056761674797878815\n",
            "Epoch 397 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002494684017791941 \t\t Validation Loss: 0.001892826439311298\n",
            "Epoch 398 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002584468626819954 \t\t Validation Loss: 0.0033094067251882874\n",
            "Epoch 399 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023216820712125786 \t\t Validation Loss: 0.0020343094097020533\n",
            "Epoch 400 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026207784382701925 \t\t Validation Loss: 0.00379161664750427\n",
            "Epoch 401 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002205901006803017 \t\t Validation Loss: 0.0015317231792813311\n",
            "Epoch 402 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002312291146105004 \t\t Validation Loss: 0.003585472823872876\n",
            "Epoch 403 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00234653424263957 \t\t Validation Loss: 0.0015592224528798116\n",
            "Epoch 404 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024015048386629772 \t\t Validation Loss: 0.00386770642720736\n",
            "Epoch 405 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023035980710709417 \t\t Validation Loss: 0.0014778765885589214\n",
            "Epoch 406 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024179914931615662 \t\t Validation Loss: 0.004078728218491261\n",
            "Epoch 407 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002124920872832976 \t\t Validation Loss: 0.0014555776052964996\n",
            "Epoch 408 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024008879113967554 \t\t Validation Loss: 0.002945653659900507\n",
            "Epoch 409 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021680375479312766 \t\t Validation Loss: 0.0015078390570124611\n",
            "Epoch 410 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023498099762946367 \t\t Validation Loss: 0.003409927236274458\n",
            "Epoch 411 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023461284518644615 \t\t Validation Loss: 0.0015968702332779334\n",
            "Epoch 412 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022103657475609734 \t\t Validation Loss: 0.0032615849542288254\n",
            "Epoch 413 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023628034993905473 \t\t Validation Loss: 0.0014831477008276405\n",
            "Epoch 414 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025407977966981866 \t\t Validation Loss: 0.0046980726049066735\n",
            "Epoch 415 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002169003851736921 \t\t Validation Loss: 0.0014389343481385508\n",
            "Epoch 416 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002463064200521724 \t\t Validation Loss: 0.004448656109161675\n",
            "Epoch 417 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022216150301156215 \t\t Validation Loss: 0.0015128038089292555\n",
            "Epoch 418 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024631566256347642 \t\t Validation Loss: 0.0038661454141569827\n",
            "Epoch 419 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002242910036993389 \t\t Validation Loss: 0.0015079481202589634\n",
            "Epoch 420 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023041014729470416 \t\t Validation Loss: 0.0036480787228076504\n",
            "Epoch 421 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022687738512743366 \t\t Validation Loss: 0.0015132586581435485\n",
            "Epoch 422 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023572621985049162 \t\t Validation Loss: 0.0041539889599124975\n",
            "Epoch 423 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022597692913149258 \t\t Validation Loss: 0.0015012380172265694\n",
            "Epoch 424 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025071266415604464 \t\t Validation Loss: 0.004243446433415206\n",
            "Epoch 425 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023487026825219997 \t\t Validation Loss: 0.0015615940568610453\n",
            "Epoch 426 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024489044798601018 \t\t Validation Loss: 0.003923333706692434\n",
            "Epoch 427 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022101636733733924 \t\t Validation Loss: 0.0015442213446546632\n",
            "Epoch 428 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002587038333798331 \t\t Validation Loss: 0.0048849198143356126\n",
            "Epoch 429 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025160540480519064 \t\t Validation Loss: 0.0016125886468216777\n",
            "Epoch 430 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026845242261785912 \t\t Validation Loss: 0.005128836689087061\n",
            "Epoch 431 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002289289004496626 \t\t Validation Loss: 0.001595305238599674\n",
            "Epoch 432 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002407294653223928 \t\t Validation Loss: 0.0038520037912978576\n",
            "Epoch 433 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022479597199078953 \t\t Validation Loss: 0.0015708443511707278\n",
            "Epoch 434 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025476949836907756 \t\t Validation Loss: 0.003985917132992584\n",
            "Epoch 435 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022684646081305234 \t\t Validation Loss: 0.0014805586878747607\n",
            "Epoch 436 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024523880856577307 \t\t Validation Loss: 0.004962373023422865\n",
            "Epoch 437 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002324561892089912 \t\t Validation Loss: 0.001591137489483047\n",
            "Epoch 438 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026112836581101088 \t\t Validation Loss: 0.004997965736457935\n",
            "Epoch 439 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023550025653094053 \t\t Validation Loss: 0.0016651159474769463\n",
            "Epoch 440 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002749701316871152 \t\t Validation Loss: 0.006266524746584205\n",
            "Epoch 441 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002553407027310616 \t\t Validation Loss: 0.0019507829565554857\n",
            "Epoch 442 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028813629319916502 \t\t Validation Loss: 0.007965306518599391\n",
            "Epoch 443 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002523663776259668 \t\t Validation Loss: 0.0023005793068128135\n",
            "Epoch 444 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0031327731133363136 \t\t Validation Loss: 0.007449040941607494\n",
            "Epoch 445 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025808321362411654 \t\t Validation Loss: 0.0021253976111228648\n",
            "Epoch 446 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028170144399376333 \t\t Validation Loss: 0.006914468947798014\n",
            "Epoch 447 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002780053629288198 \t\t Validation Loss: 0.0020177954037745413\n",
            "Epoch 448 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0028255037677071587 \t\t Validation Loss: 0.00634427467146172\n",
            "Epoch 449 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024141876178013313 \t\t Validation Loss: 0.001710030330846516\n",
            "Epoch 450 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002702438156120479 \t\t Validation Loss: 0.005109189669243419\n",
            "Epoch 451 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023968722891515573 \t\t Validation Loss: 0.0014922884292900562\n",
            "Epoch 452 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00245518414885737 \t\t Validation Loss: 0.00549155636690557\n",
            "Epoch 453 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023130198100556596 \t\t Validation Loss: 0.0015320473466999829\n",
            "Epoch 454 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024804457333064763 \t\t Validation Loss: 0.004578285143137551\n",
            "Epoch 455 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022379479235717773 \t\t Validation Loss: 0.0014931125020106824\n",
            "Epoch 456 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024260324363068147 \t\t Validation Loss: 0.004172401323627967\n",
            "Epoch 457 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023223783764233057 \t\t Validation Loss: 0.001528015550530444\n",
            "Epoch 458 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024706457412763927 \t\t Validation Loss: 0.0042839520252667945\n",
            "Epoch 459 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023577963772884293 \t\t Validation Loss: 0.0015170902031688737\n",
            "Epoch 460 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024492144199226656 \t\t Validation Loss: 0.0037429832871286916\n",
            "Epoch 461 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020858896691729693 \t\t Validation Loss: 0.0015174316190845834\n",
            "Epoch 462 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002426285825267031 \t\t Validation Loss: 0.004064043742031432\n",
            "Epoch 463 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002315678378894321 \t\t Validation Loss: 0.0016637840073179598\n",
            "Epoch 464 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022686373464435943 \t\t Validation Loss: 0.004576858383818314\n",
            "Epoch 465 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023180136938478696 \t\t Validation Loss: 0.0014900581042568844\n",
            "Epoch 466 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023693804223266605 \t\t Validation Loss: 0.0044482571180336754\n",
            "Epoch 467 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023421660849094594 \t\t Validation Loss: 0.0015488858760083811\n",
            "Epoch 468 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025905136237663494 \t\t Validation Loss: 0.00474774066466265\n",
            "Epoch 469 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002332092575867333 \t\t Validation Loss: 0.0017601666712345411\n",
            "Epoch 470 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002537486856060757 \t\t Validation Loss: 0.004720947490288661\n",
            "Epoch 471 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022327860351651907 \t\t Validation Loss: 0.0015551875134965836\n",
            "Epoch 472 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002765645209316013 \t\t Validation Loss: 0.006294397010396306\n",
            "Epoch 473 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002442087229290927 \t\t Validation Loss: 0.0015620641329755576\n",
            "Epoch 474 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025397545651759247 \t\t Validation Loss: 0.005771966471981544\n",
            "Epoch 475 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002159872904067507 \t\t Validation Loss: 0.0016495500861380536\n",
            "Epoch 476 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002484839390711607 \t\t Validation Loss: 0.004347629119785359\n",
            "Epoch 477 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002352113143314381 \t\t Validation Loss: 0.0016534910805953236\n",
            "Epoch 478 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024552868720068522 \t\t Validation Loss: 0.004501346642008195\n",
            "Epoch 479 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002301278236208836 \t\t Validation Loss: 0.001465280826848287\n",
            "Epoch 480 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023326030696389846 \t\t Validation Loss: 0.0036886001113229073\n",
            "Epoch 481 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022224655966400293 \t\t Validation Loss: 0.001508100202325015\n",
            "Epoch 482 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002429459010395599 \t\t Validation Loss: 0.004122321046172426\n",
            "Epoch 483 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002317565949507863 \t\t Validation Loss: 0.0015835714427969204\n",
            "Epoch 484 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025316990457311577 \t\t Validation Loss: 0.004483664175495505\n",
            "Epoch 485 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023413997181263324 \t\t Validation Loss: 0.0018292034868724071\n",
            "Epoch 486 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002601680822506849 \t\t Validation Loss: 0.004695692484696897\n",
            "Epoch 487 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002322674121682507 \t\t Validation Loss: 0.001719913579738484\n",
            "Epoch 488 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002678900639325179 \t\t Validation Loss: 0.005627925973385572\n",
            "Epoch 489 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024405209901365073 \t\t Validation Loss: 0.0020033102840758287\n",
            "Epoch 490 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025439231036658828 \t\t Validation Loss: 0.00522374829205756\n",
            "Epoch 491 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023030113708857143 \t\t Validation Loss: 0.002187285581245445\n",
            "Epoch 492 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026256543996341125 \t\t Validation Loss: 0.004522405555829979\n",
            "Epoch 493 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002287592122180236 \t\t Validation Loss: 0.0015653795114933299\n",
            "Epoch 494 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002444511277103998 \t\t Validation Loss: 0.004737040717512942\n",
            "Epoch 495 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022669010245039856 \t\t Validation Loss: 0.0015020044654822694\n",
            "Epoch 496 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025029480724746513 \t\t Validation Loss: 0.004333504672663717\n",
            "Epoch 497 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002117886539938784 \t\t Validation Loss: 0.0015557283566942294\n",
            "Epoch 498 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002400374658267341 \t\t Validation Loss: 0.004116916787237502\n",
            "Epoch 499 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002250106662285288 \t\t Validation Loss: 0.0015339980645176882\n",
            "Epoch 500 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024533850452359263 \t\t Validation Loss: 0.0035826545163917425\n",
            "Epoch 501 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002123180636821466 \t\t Validation Loss: 0.001499347281284057\n",
            "Epoch 502 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002316067212361943 \t\t Validation Loss: 0.0036302338345334507\n",
            "Epoch 503 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002209241586938702 \t\t Validation Loss: 0.0015336124745842356\n",
            "Epoch 504 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025339165491656073 \t\t Validation Loss: 0.004938818451661903\n",
            "Epoch 505 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023726131609716527 \t\t Validation Loss: 0.0015963857641998823\n",
            "Epoch 506 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002348258754716733 \t\t Validation Loss: 0.005173493758775294\n",
            "Epoch 507 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002262929262517876 \t\t Validation Loss: 0.0016825028713076161\n",
            "Epoch 508 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002438315437364115 \t\t Validation Loss: 0.0044050478967479784\n",
            "Epoch 509 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022714396016798107 \t\t Validation Loss: 0.0015727483036104017\n",
            "Epoch 510 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023231920896324555 \t\t Validation Loss: 0.003985769025838146\n",
            "Epoch 511 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023293631258294793 \t\t Validation Loss: 0.0015409202314913273\n",
            "Epoch 512 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00250862048690308 \t\t Validation Loss: 0.004726478409093733\n",
            "Epoch 513 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002400761279843848 \t\t Validation Loss: 0.0015180976703189886\n",
            "Epoch 514 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024674093607196432 \t\t Validation Loss: 0.006635966728656338\n",
            "Epoch 515 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002313918430937102 \t\t Validation Loss: 0.001718038742323048\n",
            "Epoch 516 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002540680388699764 \t\t Validation Loss: 0.005201152687032635\n",
            "Epoch 517 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00227070548666037 \t\t Validation Loss: 0.0018800277209195953\n",
            "Epoch 518 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025700137893845502 \t\t Validation Loss: 0.005180962956868685\n",
            "Epoch 519 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022972695296630263 \t\t Validation Loss: 0.0022483294417795082\n",
            "Epoch 520 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002642574446351343 \t\t Validation Loss: 0.0056928172528457185\n",
            "Epoch 521 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002462993924996841 \t\t Validation Loss: 0.0017969794616174812\n",
            "Epoch 522 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025609600785616283 \t\t Validation Loss: 0.005602944481114929\n",
            "Epoch 523 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023038457666926487 \t\t Validation Loss: 0.0015209399996540295\n",
            "Epoch 524 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002483988822614019 \t\t Validation Loss: 0.006556919113231393\n",
            "Epoch 525 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022543668274994234 \t\t Validation Loss: 0.0015102077562075395\n",
            "Epoch 526 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025885170286307365 \t\t Validation Loss: 0.0059749624118782\n",
            "Epoch 527 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024280268494417338 \t\t Validation Loss: 0.0016241496487054974\n",
            "Epoch 528 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002513939292937819 \t\t Validation Loss: 0.004265606824236994\n",
            "Epoch 529 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002270248396057837 \t\t Validation Loss: 0.001565773600864654\n",
            "Epoch 530 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002384733155407515 \t\t Validation Loss: 0.003118934853074069\n",
            "Epoch 531 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022346969250262388 \t\t Validation Loss: 0.001610498940303492\n",
            "Epoch 532 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023224782914464436 \t\t Validation Loss: 0.004341158254716832\n",
            "Epoch 533 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021501100147017153 \t\t Validation Loss: 0.0016791524249128997\n",
            "Epoch 534 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022316311935994875 \t\t Validation Loss: 0.0036874167674865862\n",
            "Epoch 535 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021940360578859377 \t\t Validation Loss: 0.001552171304445857\n",
            "Epoch 536 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002368623955565429 \t\t Validation Loss: 0.003850830324853842\n",
            "Epoch 537 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002224248363573507 \t\t Validation Loss: 0.0017354372047031154\n",
            "Epoch 538 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022374004954660966 \t\t Validation Loss: 0.003800724194241831\n",
            "Epoch 539 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002263210930962216 \t\t Validation Loss: 0.0016686074601378865\n",
            "Epoch 540 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002265160245745367 \t\t Validation Loss: 0.003681374337667456\n",
            "Epoch 541 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021788911827890253 \t\t Validation Loss: 0.0015297795639176352\n",
            "Epoch 542 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002193045165276507 \t\t Validation Loss: 0.0035865211259358777\n",
            "Epoch 543 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002296699317977638 \t\t Validation Loss: 0.0015516439374410906\n",
            "Epoch 544 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002349344554917283 \t\t Validation Loss: 0.003030670591844962\n",
            "Epoch 545 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021122738394911424 \t\t Validation Loss: 0.0016664605175789732\n",
            "Epoch 546 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002413859438289561 \t\t Validation Loss: 0.004466482966493528\n",
            "Epoch 547 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021555322549633078 \t\t Validation Loss: 0.0019342080833247076\n",
            "Epoch 548 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024590572457118716 \t\t Validation Loss: 0.004249366242080354\n",
            "Epoch 549 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002140246385179863 \t\t Validation Loss: 0.001597086829581083\n",
            "Epoch 550 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002229201911029884 \t\t Validation Loss: 0.003519128161804894\n",
            "Epoch 551 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021251366675101423 \t\t Validation Loss: 0.0015274931479675265\n",
            "Epoch 552 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00229282261539452 \t\t Validation Loss: 0.003659491758578672\n",
            "Epoch 553 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002287884385992043 \t\t Validation Loss: 0.0015896797032417874\n",
            "Epoch 554 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024577429143686756 \t\t Validation Loss: 0.0036831271942131794\n",
            "Epoch 555 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021939915164398984 \t\t Validation Loss: 0.0015890259103168948\n",
            "Epoch 556 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022769274301764025 \t\t Validation Loss: 0.0049687181868088934\n",
            "Epoch 557 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022872234269901105 \t\t Validation Loss: 0.00157341247317023\n",
            "Epoch 558 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025341615137467917 \t\t Validation Loss: 0.005836209586749856\n",
            "Epoch 559 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023001811791186198 \t\t Validation Loss: 0.0015453368653722394\n",
            "Epoch 560 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025339649291709065 \t\t Validation Loss: 0.00645397761120246\n",
            "Epoch 561 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002310420962277095 \t\t Validation Loss: 0.0017229016700114768\n",
            "Epoch 562 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002769661961891059 \t\t Validation Loss: 0.00746626197360456\n",
            "Epoch 563 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023141719095685796 \t\t Validation Loss: 0.0018739080450569207\n",
            "Epoch 564 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002792585811049149 \t\t Validation Loss: 0.006906494766903611\n",
            "Epoch 565 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002405169264199464 \t\t Validation Loss: 0.001896280195349111\n",
            "Epoch 566 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026937524301969018 \t\t Validation Loss: 0.005900512359893093\n",
            "Epoch 567 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024400483580885104 \t\t Validation Loss: 0.0021309781765851835\n",
            "Epoch 568 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002864092025226234 \t\t Validation Loss: 0.006342782316586146\n",
            "Epoch 569 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002410341603240955 \t\t Validation Loss: 0.002228364634972352\n",
            "Epoch 570 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0026469669207250953 \t\t Validation Loss: 0.005502387084281788\n",
            "Epoch 571 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024140171181234356 \t\t Validation Loss: 0.001999064662063924\n",
            "Epoch 572 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002714713229923635 \t\t Validation Loss: 0.006712948760161033\n",
            "Epoch 573 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00251601602114435 \t\t Validation Loss: 0.002604439066579709\n",
            "Epoch 574 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024717802203864463 \t\t Validation Loss: 0.00540899471917118\n",
            "Epoch 575 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025113438479476484 \t\t Validation Loss: 0.0021292698623325964\n",
            "Epoch 576 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002338809088640217 \t\t Validation Loss: 0.004319537860842852\n",
            "Epoch 577 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002259578162527366 \t\t Validation Loss: 0.0016908076314183956\n",
            "Epoch 578 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022508857254178037 \t\t Validation Loss: 0.00351193989179312\n",
            "Epoch 579 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022639273115232384 \t\t Validation Loss: 0.0016492039027910393\n",
            "Epoch 580 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023638191082313454 \t\t Validation Loss: 0.004297826080941237\n",
            "Epoch 581 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002146440473183788 \t\t Validation Loss: 0.0016092137218667911\n",
            "Epoch 582 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023702782652705806 \t\t Validation Loss: 0.0032793223508633673\n",
            "Epoch 583 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002302559036006396 \t\t Validation Loss: 0.0016433380884476579\n",
            "Epoch 584 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002140896216292891 \t\t Validation Loss: 0.0035897513600782706\n",
            "Epoch 585 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021005893358960748 \t\t Validation Loss: 0.0015544483220974843\n",
            "Epoch 586 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022683040525235637 \t\t Validation Loss: 0.0042227562875128705\n",
            "Epoch 587 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020867896351233328 \t\t Validation Loss: 0.001556538902849962\n",
            "Epoch 588 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002306479766392275 \t\t Validation Loss: 0.003530634168642931\n",
            "Epoch 589 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021640991814107307 \t\t Validation Loss: 0.0016325764568486752\n",
            "Epoch 590 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023488253268501343 \t\t Validation Loss: 0.00329534489607725\n",
            "Epoch 591 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020902331342694124 \t\t Validation Loss: 0.001590929131802673\n",
            "Epoch 592 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002689765928300551 \t\t Validation Loss: 0.0046386694840083904\n",
            "Epoch 593 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002224229888147297 \t\t Validation Loss: 0.0015897882101573767\n",
            "Epoch 594 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002328245175335355 \t\t Validation Loss: 0.004263508527611311\n",
            "Epoch 595 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002342367626575602 \t\t Validation Loss: 0.0015797753764603\n",
            "Epoch 596 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023262454157481223 \t\t Validation Loss: 0.0045165230269328905\n",
            "Epoch 597 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002196800275202337 \t\t Validation Loss: 0.001726041127067919\n",
            "Epoch 598 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024697414695323016 \t\t Validation Loss: 0.004252376249776437\n",
            "Epoch 599 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002224831470615558 \t\t Validation Loss: 0.001721339148039428\n",
            "Epoch 600 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023610427333797153 \t\t Validation Loss: 0.0041920424510653205\n",
            "Epoch 601 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002169839701162198 \t\t Validation Loss: 0.0017019861037484729\n",
            "Epoch 602 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024145466945687862 \t\t Validation Loss: 0.004261394783567924\n",
            "Epoch 603 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002225706369515408 \t\t Validation Loss: 0.001561467070132494\n",
            "Epoch 604 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023916178631495583 \t\t Validation Loss: 0.005431762851703052\n",
            "Epoch 605 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022055480701571985 \t\t Validation Loss: 0.0016028873147801138\n",
            "Epoch 606 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002283218893304316 \t\t Validation Loss: 0.004172324552200735\n",
            "Epoch 607 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002214723271380707 \t\t Validation Loss: 0.0015937719368734038\n",
            "Epoch 608 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022861107435027087 \t\t Validation Loss: 0.0042135376805582875\n",
            "Epoch 609 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021267340870926507 \t\t Validation Loss: 0.0017628780870626753\n",
            "Epoch 610 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002323667410245115 \t\t Validation Loss: 0.00410802520883198\n",
            "Epoch 611 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002097969574262262 \t\t Validation Loss: 0.0016876199694636923\n",
            "Epoch 612 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024569262656055995 \t\t Validation Loss: 0.00400184674295955\n",
            "Epoch 613 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021440168540622735 \t\t Validation Loss: 0.0018684824373429785\n",
            "Epoch 614 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002458849550552062 \t\t Validation Loss: 0.0053077798175553866\n",
            "Epoch 615 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024219954266791813 \t\t Validation Loss: 0.002034212856625135\n",
            "Epoch 616 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002519030006747139 \t\t Validation Loss: 0.006263612035232095\n",
            "Epoch 617 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022068346875744895 \t\t Validation Loss: 0.0017595777664190303\n",
            "Epoch 618 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025466894542143957 \t\t Validation Loss: 0.005277497443155601\n",
            "Epoch 619 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022883562896576885 \t\t Validation Loss: 0.001858411925450827\n",
            "Epoch 620 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023895784641452438 \t\t Validation Loss: 0.005418600272745467\n",
            "Epoch 621 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002194885954235655 \t\t Validation Loss: 0.0018771784156202697\n",
            "Epoch 622 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024986347809710817 \t\t Validation Loss: 0.005032971528215477\n",
            "Epoch 623 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023058083229320677 \t\t Validation Loss: 0.0016665602496896798\n",
            "Epoch 624 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024814729207522563 \t\t Validation Loss: 0.0044843403604598\n",
            "Epoch 625 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002255255419358208 \t\t Validation Loss: 0.0016446858845973532\n",
            "Epoch 626 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002634337643866201 \t\t Validation Loss: 0.004775477680735863\n",
            "Epoch 627 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002209126003829108 \t\t Validation Loss: 0.0015981556319345075\n",
            "Epoch 628 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002347899666150428 \t\t Validation Loss: 0.0034839442008748078\n",
            "Epoch 629 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002254971251130809 \t\t Validation Loss: 0.0016208939128913558\n",
            "Epoch 630 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022514719001609025 \t\t Validation Loss: 0.0034177279180417266\n",
            "Epoch 631 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022760903638963763 \t\t Validation Loss: 0.0015876353695952834\n",
            "Epoch 632 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002344194061053615 \t\t Validation Loss: 0.004330890629297266\n",
            "Epoch 633 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002261656232656458 \t\t Validation Loss: 0.001686688745394349\n",
            "Epoch 634 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002298940601601341 \t\t Validation Loss: 0.005946695392664809\n",
            "Epoch 635 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002225517630388306 \t\t Validation Loss: 0.0018191218528395088\n",
            "Epoch 636 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002549116809638469 \t\t Validation Loss: 0.004918084345543041\n",
            "Epoch 637 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023025806058777146 \t\t Validation Loss: 0.00159646722022444\n",
            "Epoch 638 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022668034456531845 \t\t Validation Loss: 0.00432256911881268\n",
            "Epoch 639 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022001041860848264 \t\t Validation Loss: 0.0018031855221264637\n",
            "Epoch 640 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024999529917762186 \t\t Validation Loss: 0.004520831799779374\n",
            "Epoch 641 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022541226337764514 \t\t Validation Loss: 0.0016016460610374522\n",
            "Epoch 642 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002325331237689416 \t\t Validation Loss: 0.004792403349151405\n",
            "Epoch 643 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022096286930546566 \t\t Validation Loss: 0.0017670583239613245\n",
            "Epoch 644 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025012069137930566 \t\t Validation Loss: 0.0037556238821707666\n",
            "Epoch 645 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002163546803294102 \t\t Validation Loss: 0.0016054688066315765\n",
            "Epoch 646 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002291219593717943 \t\t Validation Loss: 0.004377926941602849\n",
            "Epoch 647 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002193603991858057 \t\t Validation Loss: 0.0015994507770161503\n",
            "Epoch 648 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022970553655869553 \t\t Validation Loss: 0.0033261522453708146\n",
            "Epoch 649 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022486790816450644 \t\t Validation Loss: 0.0016752013941116345\n",
            "Epoch 650 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002106635843095294 \t\t Validation Loss: 0.003209111665805372\n",
            "Epoch 651 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021482223402582916 \t\t Validation Loss: 0.0015882401651022239\n",
            "Epoch 652 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022568782867007963 \t\t Validation Loss: 0.003679602827805166\n",
            "Epoch 653 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002088323311024421 \t\t Validation Loss: 0.0016638048311086516\n",
            "Epoch 654 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021937270766765986 \t\t Validation Loss: 0.0035033968116085115\n",
            "Epoch 655 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002131114301674471 \t\t Validation Loss: 0.0016549716058831948\n",
            "Epoch 656 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022988299417586342 \t\t Validation Loss: 0.0038922778599394057\n",
            "Epoch 657 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022728539597419266 \t\t Validation Loss: 0.0016432861963179535\n",
            "Epoch 658 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002290547880457362 \t\t Validation Loss: 0.00380693680087391\n",
            "Epoch 659 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021196673215190705 \t\t Validation Loss: 0.0015892018485688963\n",
            "Epoch 660 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021863476392133415 \t\t Validation Loss: 0.0029338753352371547\n",
            "Epoch 661 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002135310435315242 \t\t Validation Loss: 0.0015935634168832062\n",
            "Epoch 662 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021869549205262417 \t\t Validation Loss: 0.0034368100370137165\n",
            "Epoch 663 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020217706553159734 \t\t Validation Loss: 0.0016227223990986554\n",
            "Epoch 664 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002191854563755663 \t\t Validation Loss: 0.0033660657847156892\n",
            "Epoch 665 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022716550329523917 \t\t Validation Loss: 0.0016045936611660111\n",
            "Epoch 666 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023635895733998434 \t\t Validation Loss: 0.003514282579999417\n",
            "Epoch 667 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002121107906065378 \t\t Validation Loss: 0.0016392471510558748\n",
            "Epoch 668 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002585908414550579 \t\t Validation Loss: 0.004103525183521784\n",
            "Epoch 669 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023107150525509103 \t\t Validation Loss: 0.0016362566294936608\n",
            "Epoch 670 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002213024955897315 \t\t Validation Loss: 0.003154421834131846\n",
            "Epoch 671 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002156568198729105 \t\t Validation Loss: 0.0016322190483781295\n",
            "Epoch 672 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023558161219871428 \t\t Validation Loss: 0.0039594557064657025\n",
            "Epoch 673 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021810459926397212 \t\t Validation Loss: 0.0017589009635580273\n",
            "Epoch 674 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022963127553953812 \t\t Validation Loss: 0.00364572552140229\n",
            "Epoch 675 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002100451076969605 \t\t Validation Loss: 0.0015921515229050643\n",
            "Epoch 676 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024920121178257505 \t\t Validation Loss: 0.004741576643517384\n",
            "Epoch 677 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002272706793470157 \t\t Validation Loss: 0.0018134249436955613\n",
            "Epoch 678 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021498146682436504 \t\t Validation Loss: 0.0033681031961280564\n",
            "Epoch 679 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021010193772412634 \t\t Validation Loss: 0.0018913688433643144\n",
            "Epoch 680 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002548413297690048 \t\t Validation Loss: 0.004966756176704971\n",
            "Epoch 681 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002260005592393714 \t\t Validation Loss: 0.0017078621044325142\n",
            "Epoch 682 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023283631150686256 \t\t Validation Loss: 0.004415537803792036\n",
            "Epoch 683 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002347464698391992 \t\t Validation Loss: 0.001915459861405767\n",
            "Epoch 684 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024281791985550946 \t\t Validation Loss: 0.004976473130787222\n",
            "Epoch 685 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022471405996790004 \t\t Validation Loss: 0.001911276181300099\n",
            "Epoch 686 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002485118409920786 \t\t Validation Loss: 0.004521300863976089\n",
            "Epoch 687 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024039560450694044 \t\t Validation Loss: 0.0022039650581203974\n",
            "Epoch 688 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002309519231218744 \t\t Validation Loss: 0.003902091147700468\n",
            "Epoch 689 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00211786459879698 \t\t Validation Loss: 0.002324542756049106\n",
            "Epoch 690 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025050929626387014 \t\t Validation Loss: 0.003850441094702826\n",
            "Epoch 691 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021501701904108397 \t\t Validation Loss: 0.0017408942982840997\n",
            "Epoch 692 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002229780138067498 \t\t Validation Loss: 0.0037027724320068955\n",
            "Epoch 693 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00231775943649228 \t\t Validation Loss: 0.002372272608156961\n",
            "Epoch 694 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002413641061237384 \t\t Validation Loss: 0.004409151438337106\n",
            "Epoch 695 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002165297790417901 \t\t Validation Loss: 0.0022421270214880887\n",
            "Epoch 696 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023803700554511837 \t\t Validation Loss: 0.0037056654423045423\n",
            "Epoch 697 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002109376565754615 \t\t Validation Loss: 0.0017460039637696284\n",
            "Epoch 698 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022725003568543675 \t\t Validation Loss: 0.003606278194078746\n",
            "Epoch 699 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002158860875784445 \t\t Validation Loss: 0.0016508929771729386\n",
            "Epoch 700 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022026921985859706 \t\t Validation Loss: 0.0038891952225150396\n",
            "Epoch 701 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022169000333854675 \t\t Validation Loss: 0.0017587185407487245\n",
            "Epoch 702 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022743037938001893 \t\t Validation Loss: 0.0035924840664777616\n",
            "Epoch 703 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021039076319367095 \t\t Validation Loss: 0.001793560258542689\n",
            "Epoch 704 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002372843903218824 \t\t Validation Loss: 0.0037182393492772602\n",
            "Epoch 705 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020534992815825987 \t\t Validation Loss: 0.0017846807067354138\n",
            "Epoch 706 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023956115678506525 \t\t Validation Loss: 0.0042299925206372374\n",
            "Epoch 707 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002311255063899365 \t\t Validation Loss: 0.001818339223973453\n",
            "Epoch 708 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025165963939995178 \t\t Validation Loss: 0.0047775909232978635\n",
            "Epoch 709 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002156701420333136 \t\t Validation Loss: 0.0020567234605550766\n",
            "Epoch 710 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024585874162840883 \t\t Validation Loss: 0.004413277895834584\n",
            "Epoch 711 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00223687439024247 \t\t Validation Loss: 0.002153527681142665\n",
            "Epoch 712 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002295181247235804 \t\t Validation Loss: 0.0038297402744109812\n",
            "Epoch 713 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002260004769620561 \t\t Validation Loss: 0.0020500315407004496\n",
            "Epoch 714 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002393028256798012 \t\t Validation Loss: 0.003818257013335824\n",
            "Epoch 715 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002220206335501594 \t\t Validation Loss: 0.0017548791768674094\n",
            "Epoch 716 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002355532255810308 \t\t Validation Loss: 0.004128653851624291\n",
            "Epoch 717 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002144982582084029 \t\t Validation Loss: 0.0017493718858951558\n",
            "Epoch 718 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002353573461830918 \t\t Validation Loss: 0.003777503819527248\n",
            "Epoch 719 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022888642317313397 \t\t Validation Loss: 0.00172705368962712\n",
            "Epoch 720 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023141596308984868 \t\t Validation Loss: 0.003911695201308108\n",
            "Epoch 721 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022963558267046873 \t\t Validation Loss: 0.0016251563591560205\n",
            "Epoch 722 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00213514591057508 \t\t Validation Loss: 0.0031391668061797437\n",
            "Epoch 723 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020799754325904556 \t\t Validation Loss: 0.0016489373206590803\n",
            "Epoch 724 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021970639291622146 \t\t Validation Loss: 0.0031583276622069\n",
            "Epoch 725 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021057027231625005 \t\t Validation Loss: 0.0016589692587820957\n",
            "Epoch 726 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022075006477157207 \t\t Validation Loss: 0.0027549471124075353\n",
            "Epoch 727 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002065420470112381 \t\t Validation Loss: 0.0016436881728273316\n",
            "Epoch 728 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021527437202166766 \t\t Validation Loss: 0.002688523937160006\n",
            "Epoch 729 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021016459292228762 \t\t Validation Loss: 0.0016287330497736828\n",
            "Epoch 730 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020776048249162332 \t\t Validation Loss: 0.0025197402842772696\n",
            "Epoch 731 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020842618305815033 \t\t Validation Loss: 0.0016317141461723412\n",
            "Epoch 732 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021311981592490965 \t\t Validation Loss: 0.0030473183455447164\n",
            "Epoch 733 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002057995364019597 \t\t Validation Loss: 0.0016521573572670324\n",
            "Epoch 734 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022792869618766614 \t\t Validation Loss: 0.004203222459182143\n",
            "Epoch 735 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022712162305676452 \t\t Validation Loss: 0.0016712298361548723\n",
            "Epoch 736 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002122534832262711 \t\t Validation Loss: 0.00289132584629652\n",
            "Epoch 737 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020643120703941866 \t\t Validation Loss: 0.0017958257007054412\n",
            "Epoch 738 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022824563524950093 \t\t Validation Loss: 0.003321433842826921\n",
            "Epoch 739 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021201374128813275 \t\t Validation Loss: 0.0016899617151536334\n",
            "Epoch 740 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002142434596002253 \t\t Validation Loss: 0.002823174619921841\n",
            "Epoch 741 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002029264801987321 \t\t Validation Loss: 0.0016889481250053416\n",
            "Epoch 742 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021494373323622386 \t\t Validation Loss: 0.002360026282706083\n",
            "Epoch 743 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022614498754862595 \t\t Validation Loss: 0.001770159547539571\n",
            "Epoch 744 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021989079061706827 \t\t Validation Loss: 0.003258874550318489\n",
            "Epoch 745 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002152086199399688 \t\t Validation Loss: 0.001709594104725581\n",
            "Epoch 746 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022408321572190805 \t\t Validation Loss: 0.0031068823837603512\n",
            "Epoch 747 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002143472938080998 \t\t Validation Loss: 0.0017464489120846759\n",
            "Epoch 748 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002333701663726085 \t\t Validation Loss: 0.004128113532295594\n",
            "Epoch 749 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002096625919629996 \t\t Validation Loss: 0.0019115838311755885\n",
            "Epoch 750 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024044472225501225 \t\t Validation Loss: 0.004022884119946796\n",
            "Epoch 751 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002153800896406677 \t\t Validation Loss: 0.0019346384483819397\n",
            "Epoch 752 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002327549593000849 \t\t Validation Loss: 0.003814153391151474\n",
            "Epoch 753 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022327845831200278 \t\t Validation Loss: 0.001986747240432753\n",
            "Epoch 754 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002573859502680952 \t\t Validation Loss: 0.005323753531019275\n",
            "Epoch 755 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002482791122904903 \t\t Validation Loss: 0.0019029334966594784\n",
            "Epoch 756 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002387671303507444 \t\t Validation Loss: 0.004170095977874903\n",
            "Epoch 757 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002278802702455102 \t\t Validation Loss: 0.0026624550272782263\n",
            "Epoch 758 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002488247356440469 \t\t Validation Loss: 0.004472743424300391\n",
            "Epoch 759 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021744267307096036 \t\t Validation Loss: 0.002166887655710945\n",
            "Epoch 760 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024936208438530966 \t\t Validation Loss: 0.0050468678371264385\n",
            "Epoch 761 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023098907109375135 \t\t Validation Loss: 0.0017513959229780505\n",
            "Epoch 762 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021559250095821414 \t\t Validation Loss: 0.0032551633014988443\n",
            "Epoch 763 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022184652232288106 \t\t Validation Loss: 0.0020254568752044668\n",
            "Epoch 764 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021894612094714634 \t\t Validation Loss: 0.003845013024357076\n",
            "Epoch 765 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021344065993420176 \t\t Validation Loss: 0.001918103384713714\n",
            "Epoch 766 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023733248471634813 \t\t Validation Loss: 0.003321472483758743\n",
            "Epoch 767 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021852744496238695 \t\t Validation Loss: 0.0018517175966945405\n",
            "Epoch 768 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002309708957516311 \t\t Validation Loss: 0.004036659347968033\n",
            "Epoch 769 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022470261586981046 \t\t Validation Loss: 0.0017422223418879395\n",
            "Epoch 770 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002196534590267048 \t\t Validation Loss: 0.003241575587218484\n",
            "Epoch 771 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002162879258960586 \t\t Validation Loss: 0.0018768310054348637\n",
            "Epoch 772 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023366988367539503 \t\t Validation Loss: 0.0034187884168484462\n",
            "Epoch 773 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002171222402122677 \t\t Validation Loss: 0.0020013566424425403\n",
            "Epoch 774 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002271809702506289 \t\t Validation Loss: 0.0030342908307480123\n",
            "Epoch 775 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002213476211575137 \t\t Validation Loss: 0.001799542922526598\n",
            "Epoch 776 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002105537199453023 \t\t Validation Loss: 0.0031834933100841367\n",
            "Epoch 777 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019943084838372226 \t\t Validation Loss: 0.0018603854964917095\n",
            "Epoch 778 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023036368200681297 \t\t Validation Loss: 0.003738952988687043\n",
            "Epoch 779 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002190377243096009 \t\t Validation Loss: 0.0017065077554434538\n",
            "Epoch 780 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002162318284988303 \t\t Validation Loss: 0.00395811078711771\n",
            "Epoch 781 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002302764929088487 \t\t Validation Loss: 0.002199665383579066\n",
            "Epoch 782 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022826832609024604 \t\t Validation Loss: 0.0036318069861198845\n",
            "Epoch 783 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002167049774937835 \t\t Validation Loss: 0.0023355274998511258\n",
            "Epoch 784 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002437796486176651 \t\t Validation Loss: 0.004592554178088903\n",
            "Epoch 785 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021947709406172303 \t\t Validation Loss: 0.0023067436951140943\n",
            "Epoch 786 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025177049872188552 \t\t Validation Loss: 0.003853117536681776\n",
            "Epoch 787 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00218277212194595 \t\t Validation Loss: 0.002028129341152425\n",
            "Epoch 788 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022829872873813117 \t\t Validation Loss: 0.003384731974130353\n",
            "Epoch 789 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020926204113591767 \t\t Validation Loss: 0.0018206649737504239\n",
            "Epoch 790 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022148668665338206 \t\t Validation Loss: 0.0033883863943628967\n",
            "Epoch 791 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002398904158129684 \t\t Validation Loss: 0.002378346094001944\n",
            "Epoch 792 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023149629501072137 \t\t Validation Loss: 0.003757380589377135\n",
            "Epoch 793 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002130953296138024 \t\t Validation Loss: 0.0021620064955921127\n",
            "Epoch 794 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002174955919366078 \t\t Validation Loss: 0.0033229581447533118\n",
            "Epoch 795 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021215960591074987 \t\t Validation Loss: 0.0018350058414328557\n",
            "Epoch 796 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002359223283821012 \t\t Validation Loss: 0.003472226394268756\n",
            "Epoch 797 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022679375975417928 \t\t Validation Loss: 0.0017762755384095586\n",
            "Epoch 798 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022639087582246174 \t\t Validation Loss: 0.00295513456624646\n",
            "Epoch 799 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021247284430255357 \t\t Validation Loss: 0.0018618756932063172\n",
            "Epoch 800 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002145135061771332 \t\t Validation Loss: 0.0028658278796893475\n",
            "Epoch 801 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021280230894508596 \t\t Validation Loss: 0.002440122224820348\n",
            "Epoch 802 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002273039484862238 \t\t Validation Loss: 0.002841599146799686\n",
            "Epoch 803 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020556375305671747 \t\t Validation Loss: 0.0018893620760466617\n",
            "Epoch 804 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022720147185405163 \t\t Validation Loss: 0.0032621140148526486\n",
            "Epoch 805 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002073758756867736 \t\t Validation Loss: 0.0018607412930577993\n",
            "Epoch 806 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021979078387753485 \t\t Validation Loss: 0.0026665118505032016\n",
            "Epoch 807 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022161947114934287 \t\t Validation Loss: 0.0017179053370805027\n",
            "Epoch 808 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021557595708243853 \t\t Validation Loss: 0.002702058308596651\n",
            "Epoch 809 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020119219505530155 \t\t Validation Loss: 0.0019521516917918164\n",
            "Epoch 810 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021727064009315357 \t\t Validation Loss: 0.003174503521922116\n",
            "Epoch 811 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020278226741514095 \t\t Validation Loss: 0.0019801862027424458\n",
            "Epoch 812 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022215724063760324 \t\t Validation Loss: 0.0029527781208833824\n",
            "Epoch 813 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021036429436109656 \t\t Validation Loss: 0.0017791008352875137\n",
            "Epoch 814 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022190975141711533 \t\t Validation Loss: 0.002776696225807357\n",
            "Epoch 815 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002187899106207329 \t\t Validation Loss: 0.00197221224124615\n",
            "Epoch 816 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021213795918958714 \t\t Validation Loss: 0.0031345000696511795\n",
            "Epoch 817 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020043370597114837 \t\t Validation Loss: 0.0019766663989195456\n",
            "Epoch 818 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022278706813734527 \t\t Validation Loss: 0.003721704191635721\n",
            "Epoch 819 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021567086569612495 \t\t Validation Loss: 0.00200953950227883\n",
            "Epoch 820 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002266481501440442 \t\t Validation Loss: 0.004733005153516738\n",
            "Epoch 821 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022434246462039852 \t\t Validation Loss: 0.002268560499382707\n",
            "Epoch 822 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002296809231346423 \t\t Validation Loss: 0.003786726590568343\n",
            "Epoch 823 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022435716740359123 \t\t Validation Loss: 0.0026012195739895105\n",
            "Epoch 824 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002367386083131203 \t\t Validation Loss: 0.004017725233167697\n",
            "Epoch 825 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021313927183556998 \t\t Validation Loss: 0.0023906230299661937\n",
            "Epoch 826 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024667229169879007 \t\t Validation Loss: 0.0046751042559313085\n",
            "Epoch 827 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002137834468944552 \t\t Validation Loss: 0.0018391028189888368\n",
            "Epoch 828 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023122991150484194 \t\t Validation Loss: 0.004057812970131636\n",
            "Epoch 829 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021964905522386165 \t\t Validation Loss: 0.002453825311162151\n",
            "Epoch 830 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002206988044309656 \t\t Validation Loss: 0.0036453321002996885\n",
            "Epoch 831 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002086809078055258 \t\t Validation Loss: 0.0021107929687087354\n",
            "Epoch 832 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023329566583679233 \t\t Validation Loss: 0.003702759832287064\n",
            "Epoch 833 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002295610571997492 \t\t Validation Loss: 0.002368554472923279\n",
            "Epoch 834 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00242247837013565 \t\t Validation Loss: 0.0038345931097865105\n",
            "Epoch 835 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021388560841866843 \t\t Validation Loss: 0.002572949384697355\n",
            "Epoch 836 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002294437763142727 \t\t Validation Loss: 0.0035280156791066895\n",
            "Epoch 837 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002104071288281498 \t\t Validation Loss: 0.0024173214154031416\n",
            "Epoch 838 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0025493190482233628 \t\t Validation Loss: 0.0037549529851485905\n",
            "Epoch 839 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022246324554765344 \t\t Validation Loss: 0.0024447987447134578\n",
            "Epoch 840 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022577652180680655 \t\t Validation Loss: 0.003489172444320642\n",
            "Epoch 841 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021625108296378843 \t\t Validation Loss: 0.0018868528109473677\n",
            "Epoch 842 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002135902994022881 \t\t Validation Loss: 0.0026967496244021906\n",
            "Epoch 843 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021530307005033704 \t\t Validation Loss: 0.0021397929566983995\n",
            "Epoch 844 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002221290977841646 \t\t Validation Loss: 0.0030136154449652308\n",
            "Epoch 845 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00195754347903956 \t\t Validation Loss: 0.0017926118005282031\n",
            "Epoch 846 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002112402324250119 \t\t Validation Loss: 0.0021494631722676926\n",
            "Epoch 847 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020714029937202263 \t\t Validation Loss: 0.001874645710743677\n",
            "Epoch 848 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00208450187582875 \t\t Validation Loss: 0.002521677997160273\n",
            "Epoch 849 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019679640347414927 \t\t Validation Loss: 0.0018058808099550123\n",
            "Epoch 850 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002128085689430402 \t\t Validation Loss: 0.002279040596197144\n",
            "Epoch 851 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.001999114341152882 \t\t Validation Loss: 0.0019074404979339587\n",
            "Epoch 852 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002224369106713582 \t\t Validation Loss: 0.002004947151782779\n",
            "Epoch 853 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020309818210080266 \t\t Validation Loss: 0.0018099673772947146\n",
            "Epoch 854 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002036834107334348 \t\t Validation Loss: 0.0021326121462222477\n",
            "Epoch 855 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.001975790367814133 \t\t Validation Loss: 0.001921054777295257\n",
            "Epoch 856 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020510859966806666 \t\t Validation Loss: 0.0019883659966021348\n",
            "Epoch 857 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019743427369944954 \t\t Validation Loss: 0.0018156254037211721\n",
            "Epoch 858 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.001999221503029804 \t\t Validation Loss: 0.0022398664239937295\n",
            "Epoch 859 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020907541447495286 \t\t Validation Loss: 0.0018164562285304642\n",
            "Epoch 860 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021835425774239608 \t\t Validation Loss: 0.0021984998641598327\n",
            "Epoch 861 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019619639811886322 \t\t Validation Loss: 0.0019402679799196238\n",
            "Epoch 862 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002113253656126609 \t\t Validation Loss: 0.0025259810969985733\n",
            "Epoch 863 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020581184795151488 \t\t Validation Loss: 0.001750803756294772\n",
            "Epoch 864 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024280120497506513 \t\t Validation Loss: 0.0031804195672381096\n",
            "Epoch 865 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023495603031856386 \t\t Validation Loss: 0.0017505722823373687\n",
            "Epoch 866 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002484040257734926 \t\t Validation Loss: 0.0034936080830028425\n",
            "Epoch 867 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002050963423932223 \t\t Validation Loss: 0.001882925929609113\n",
            "Epoch 868 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023261194630223954 \t\t Validation Loss: 0.00349315409682906\n",
            "Epoch 869 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021479656051441626 \t\t Validation Loss: 0.0017654599729352272\n",
            "Epoch 870 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002230044429165286 \t\t Validation Loss: 0.003108517013830491\n",
            "Epoch 871 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021122603809360313 \t\t Validation Loss: 0.0021645677043125033\n",
            "Epoch 872 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002295703551656491 \t\t Validation Loss: 0.002798872499484927\n",
            "Epoch 873 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021692670825113718 \t\t Validation Loss: 0.0030892392029412664\n",
            "Epoch 874 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002380213197603872 \t\t Validation Loss: 0.002676532691111788\n",
            "Epoch 875 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022315265661155856 \t\t Validation Loss: 0.0019053999164428276\n",
            "Epoch 876 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00228802883774168 \t\t Validation Loss: 0.0026533803857791308\n",
            "Epoch 877 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022559090961098067 \t\t Validation Loss: 0.002857935570108776\n",
            "Epoch 878 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002217807637911441 \t\t Validation Loss: 0.0025484561707484177\n",
            "Epoch 879 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020890234500748686 \t\t Validation Loss: 0.002535694636977636\n",
            "Epoch 880 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023992338124977273 \t\t Validation Loss: 0.0034885853618526687\n",
            "Epoch 881 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021178671725195003 \t\t Validation Loss: 0.002306162308041866\n",
            "Epoch 882 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022320996048054782 \t\t Validation Loss: 0.002255760950412458\n",
            "Epoch 883 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020986572238364935 \t\t Validation Loss: 0.002404632055773758\n",
            "Epoch 884 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002299877065328277 \t\t Validation Loss: 0.002679591552945427\n",
            "Epoch 885 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019775797578355145 \t\t Validation Loss: 0.002287706181120414\n",
            "Epoch 886 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002171592746792059 \t\t Validation Loss: 0.0031739880393545786\n",
            "Epoch 887 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002159369777497911 \t\t Validation Loss: 0.002522193441668955\n",
            "Epoch 888 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002267027799216275 \t\t Validation Loss: 0.003326939906960783\n",
            "Epoch 889 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021396068607557666 \t\t Validation Loss: 0.002645435331102747\n",
            "Epoch 890 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002294518145562678 \t\t Validation Loss: 0.003792670824063512\n",
            "Epoch 891 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002194291976519634 \t\t Validation Loss: 0.0025123595976485657\n",
            "Epoch 892 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002349457355858003 \t\t Validation Loss: 0.0039589194213756574\n",
            "Epoch 893 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022061036434024572 \t\t Validation Loss: 0.0026377707791443053\n",
            "Epoch 894 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002192880736500327 \t\t Validation Loss: 0.003304595857536277\n",
            "Epoch 895 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022270088382946277 \t\t Validation Loss: 0.0024519329711508294\n",
            "Epoch 896 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002374528507651711 \t\t Validation Loss: 0.002780726848868653\n",
            "Epoch 897 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002209890485942565 \t\t Validation Loss: 0.0024615534169312855\n",
            "Epoch 898 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021184036158956587 \t\t Validation Loss: 0.0028384589509537015\n",
            "Epoch 899 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002067510099968294 \t\t Validation Loss: 0.0021791762063423027\n",
            "Epoch 900 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022110301274428697 \t\t Validation Loss: 0.0028173917873153607\n",
            "Epoch 901 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019750762510591666 \t\t Validation Loss: 0.0020880321166119897\n",
            "Epoch 902 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021175984324686026 \t\t Validation Loss: 0.0025782004847245002\n",
            "Epoch 903 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020736870686271906 \t\t Validation Loss: 0.0019764988772714366\n",
            "Epoch 904 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002071848335027745 \t\t Validation Loss: 0.0024964638399139335\n",
            "Epoch 905 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002096454719886989 \t\t Validation Loss: 0.002313488830203334\n",
            "Epoch 906 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002137298011409773 \t\t Validation Loss: 0.002488490496314346\n",
            "Epoch 907 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020426882893425993 \t\t Validation Loss: 0.0023443622765346216\n",
            "Epoch 908 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002228785939420246 \t\t Validation Loss: 0.0026064543277383423\n",
            "Epoch 909 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002169926167855895 \t\t Validation Loss: 0.00226717740476418\n",
            "Epoch 910 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021083934281361165 \t\t Validation Loss: 0.0027593005092617553\n",
            "Epoch 911 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002175561888646838 \t\t Validation Loss: 0.0026165217949220766\n",
            "Epoch 912 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002423317316365806 \t\t Validation Loss: 0.0031367991289768657\n",
            "Epoch 913 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022160157583326705 \t\t Validation Loss: 0.0027195109466377357\n",
            "Epoch 914 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021844036236871034 \t\t Validation Loss: 0.00290792587307246\n",
            "Epoch 915 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021444180291271893 \t\t Validation Loss: 0.0032004370318295863\n",
            "Epoch 916 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002284037855428618 \t\t Validation Loss: 0.0032608877361501353\n",
            "Epoch 917 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002123023646040442 \t\t Validation Loss: 0.0030240408735922896\n",
            "Epoch 918 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024896437818234837 \t\t Validation Loss: 0.0034456441443091114\n",
            "Epoch 919 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023608912849119187 \t\t Validation Loss: 0.0027174608638653387\n",
            "Epoch 920 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023742960821019135 \t\t Validation Loss: 0.0033911444681087653\n",
            "Epoch 921 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021830118871671526 \t\t Validation Loss: 0.0023057029376594494\n",
            "Epoch 922 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002225609123392182 \t\t Validation Loss: 0.0033876262806678335\n",
            "Epoch 923 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020527540426072037 \t\t Validation Loss: 0.002410234516271605\n",
            "Epoch 924 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022206607446187757 \t\t Validation Loss: 0.003440519835119351\n",
            "Epoch 925 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002123625867219793 \t\t Validation Loss: 0.002202971305590696\n",
            "Epoch 926 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020989292880167834 \t\t Validation Loss: 0.0022325745130029437\n",
            "Epoch 927 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020146648512800805 \t\t Validation Loss: 0.002043652845224222\n",
            "Epoch 928 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002414009702939389 \t\t Validation Loss: 0.0025566321464094263\n",
            "Epoch 929 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002157245801189109 \t\t Validation Loss: 0.0020047411089763045\n",
            "Epoch 930 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021138518004099258 \t\t Validation Loss: 0.0023895405823937976\n",
            "Epoch 931 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020428422124225748 \t\t Validation Loss: 0.001797665978441588\n",
            "Epoch 932 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00209209746665422 \t\t Validation Loss: 0.0026701542038398865\n",
            "Epoch 933 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020092336281605466 \t\t Validation Loss: 0.0021258176734241154\n",
            "Epoch 934 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022070262757306162 \t\t Validation Loss: 0.002616562728340236\n",
            "Epoch 935 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019705788361386875 \t\t Validation Loss: 0.0020168689930310044\n",
            "Epoch 936 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022054741590132785 \t\t Validation Loss: 0.002237774802219624\n",
            "Epoch 937 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020515374096764906 \t\t Validation Loss: 0.0018376239369480084\n",
            "Epoch 938 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002099966519430431 \t\t Validation Loss: 0.0024461853474629326\n",
            "Epoch 939 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019821458782472118 \t\t Validation Loss: 0.0019537309193625473\n",
            "Epoch 940 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020913506348340495 \t\t Validation Loss: 0.0022153648314997554\n",
            "Epoch 941 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020118801590256596 \t\t Validation Loss: 0.0018854937560927982\n",
            "Epoch 942 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020726597167840984 \t\t Validation Loss: 0.002816199081000657\n",
            "Epoch 943 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020661134118642155 \t\t Validation Loss: 0.002137912347769508\n",
            "Epoch 944 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021762848537528537 \t\t Validation Loss: 0.003608273904179581\n",
            "Epoch 945 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020728092042829943 \t\t Validation Loss: 0.0025979294889391614\n",
            "Epoch 946 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022069681758325344 \t\t Validation Loss: 0.0026136606443637554\n",
            "Epoch 947 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002162285386647626 \t\t Validation Loss: 0.002135789894964546\n",
            "Epoch 948 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002102186967857887 \t\t Validation Loss: 0.0028998810896435035\n",
            "Epoch 949 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002104994173891641 \t\t Validation Loss: 0.0027507694008258674\n",
            "Epoch 950 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022607899736613035 \t\t Validation Loss: 0.003014503286417144\n",
            "Epoch 951 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020654282258899025 \t\t Validation Loss: 0.0031286750364905368\n",
            "Epoch 952 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002186667827757176 \t\t Validation Loss: 0.0024338329497438213\n",
            "Epoch 953 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020532017460445293 \t\t Validation Loss: 0.002028302101490016\n",
            "Epoch 954 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021297319358013368 \t\t Validation Loss: 0.0029810279733143174\n",
            "Epoch 955 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020755326337608937 \t\t Validation Loss: 0.002111108275130391\n",
            "Epoch 956 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020721005047734426 \t\t Validation Loss: 0.002228096972640532\n",
            "Epoch 957 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002023738432939894 \t\t Validation Loss: 0.002236204230799698\n",
            "Epoch 958 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002187081510389878 \t\t Validation Loss: 0.0026778450914067575\n",
            "Epoch 959 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021861212004001276 \t\t Validation Loss: 0.0019111456036066206\n",
            "Epoch 960 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021530836537430012 \t\t Validation Loss: 0.0026626721530472143\n",
            "Epoch 961 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002128407809052717 \t\t Validation Loss: 0.002036363396865244\n",
            "Epoch 962 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021576772135606893 \t\t Validation Loss: 0.002769524493487552\n",
            "Epoch 963 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002135194491947422 \t\t Validation Loss: 0.0027968850201712204\n",
            "Epoch 964 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002325716253835708 \t\t Validation Loss: 0.002955206367634953\n",
            "Epoch 965 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002044391223682544 \t\t Validation Loss: 0.002426031426204225\n",
            "Epoch 966 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002247693427725116 \t\t Validation Loss: 0.002551199905260896\n",
            "Epoch 967 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002140698573753439 \t\t Validation Loss: 0.003259520064322994\n",
            "Epoch 968 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002209212925171832 \t\t Validation Loss: 0.002600571262659147\n",
            "Epoch 969 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020453062370998434 \t\t Validation Loss: 0.0026453429689774145\n",
            "Epoch 970 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024026560147192228 \t\t Validation Loss: 0.0034779278033126434\n",
            "Epoch 971 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022492293328222994 \t\t Validation Loss: 0.0020671685163576444\n",
            "Epoch 972 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002122268343432429 \t\t Validation Loss: 0.002871341712307185\n",
            "Epoch 973 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021061793526609405 \t\t Validation Loss: 0.0025841585414197584\n",
            "Epoch 974 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024102827710275714 \t\t Validation Loss: 0.0035664616824271013\n",
            "Epoch 975 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021044035171586518 \t\t Validation Loss: 0.003234380068114171\n",
            "Epoch 976 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0024197565885638265 \t\t Validation Loss: 0.00333074637903617\n",
            "Epoch 977 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00216262460201727 \t\t Validation Loss: 0.0031920546450867103\n",
            "Epoch 978 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022632918270254457 \t\t Validation Loss: 0.00323027634070828\n",
            "Epoch 979 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020710890011459188 \t\t Validation Loss: 0.0030853481909546713\n",
            "Epoch 980 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002387037738289281 \t\t Validation Loss: 0.003457092518846576\n",
            "Epoch 981 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020452111289282707 \t\t Validation Loss: 0.003291391669056163\n",
            "Epoch 982 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022121021048971327 \t\t Validation Loss: 0.002769858022265208\n",
            "Epoch 983 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002064068743493408 \t\t Validation Loss: 0.003282039578502568\n",
            "Epoch 984 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002386154621096982 \t\t Validation Loss: 0.003589806675266188\n",
            "Epoch 985 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.00214496862483085 \t\t Validation Loss: 0.0030182298684779266\n",
            "Epoch 986 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0023539602989330888 \t\t Validation Loss: 0.003105635455890129\n",
            "Epoch 987 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0022019771764658995 \t\t Validation Loss: 0.002723309999475112\n",
            "Epoch 988 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021044472460572078 \t\t Validation Loss: 0.0026852419765558667\n",
            "Epoch 989 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002014532156691358 \t\t Validation Loss: 0.002959064736317557\n",
            "Epoch 990 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002192756757328941 \t\t Validation Loss: 0.0026688671966369907\n",
            "Epoch 991 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002047580067458487 \t\t Validation Loss: 0.0026612811936781956\n",
            "Epoch 992 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002261423013437338 \t\t Validation Loss: 0.0028731389958501006\n",
            "Epoch 993 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.001995962084824773 \t\t Validation Loss: 0.0022364503193575028\n",
            "Epoch 994 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020755716478389158 \t\t Validation Loss: 0.0023013789854863156\n",
            "Epoch 995 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0019282413287893742 \t\t Validation Loss: 0.0023452333049275554\n",
            "Epoch 996 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0021521595056558885 \t\t Validation Loss: 0.002792135619809135\n",
            "Epoch 997 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.001982454993963443 \t\t Validation Loss: 0.0024347676752278437\n",
            "Epoch 998 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020441012889637635 \t\t Validation Loss: 0.0021166403211492044\n",
            "Epoch 999 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.0020119642942665597 \t\t Validation Loss: 0.002154049669535687\n",
            "Epoch 1000 \t\t Epoch time: 0m 1s\n",
            "\t\t Training Loss: 0.002065611699583462 \t\t Validation Loss: 0.0019815752871531565\n",
            "\n",
            "Score: 92.03131940038071\n",
            "\n",
            "Accuracy: 52.79187817258884\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL7WNPnjCAsh"
      },
      "source": [
        "model_name = [\"Linear_108\"]\n",
        "model = Linear_108_num()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "e_res = []\n",
        "acc_res = []\n",
        "batch_s_res = []\n",
        "optimizer_start_res = []\n",
        "optimizer_end_res = []\n",
        "dropout_res = []\n",
        "layer_dep_res = []\n",
        "act_res = []\n",
        "score_res = []\n",
        "plt_teach_res = []\n",
        "plt_pred_res = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxL-joRDCRQH",
        "outputId": "04d23c25-aac7-4a7c-e27e-aa687e42cf87"
      },
      "source": [
        "epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_num(batch_size_in=32,model_in=model,\n",
        "          optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "e_res.append(epoch_out)\n",
        "acc_res.append(acc_out)\n",
        "batch_s_res.append(\"32\")\n",
        "optimizer_start_res.append(\"0.0001\")\n",
        "optimizer_end_res.append(\"0.0001\")\n",
        "dropout_res.append(\"0\")\n",
        "layer_dep_res.append(\"11\")\n",
        "act_res.append(\"None\")\n",
        "score_res.append(score_out)\n",
        "plt_teach_res.append(plt_teach_out)\n",
        "plt_pred_res.append(plt_best_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8198406951300599 \t\t Validation Loss: 0.0977781331883027\n",
            "\t\t Validation Loss Decreased(inf--->0.097778) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03750567631544294 \t\t Validation Loss: 0.013064355481989108\n",
            "\t\t Validation Loss Decreased(0.097778--->0.013064) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0092601490936972 \t\t Validation Loss: 0.015160845807538582\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0043946910835173285 \t\t Validation Loss: 0.0034738015652132723\n",
            "\t\t Validation Loss Decreased(0.013064--->0.003474) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030229160911403596 \t\t Validation Loss: 0.0017206401680596173\n",
            "\t\t Validation Loss Decreased(0.003474--->0.001721) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028252919610413547 \t\t Validation Loss: 0.002196418985616989\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002913143123166535 \t\t Validation Loss: 0.00317554222419858\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031565702842812782 \t\t Validation Loss: 0.00445634126663208\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003482108215983907 \t\t Validation Loss: 0.005837284691201953\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0037551596084282406 \t\t Validation Loss: 0.006845310240840683\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0038899035709029114 \t\t Validation Loss: 0.0071002660104288505\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0038247274870455666 \t\t Validation Loss: 0.006490170006425335\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035673754912725576 \t\t Validation Loss: 0.0051665355881246235\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003186723434466969 \t\t Validation Loss: 0.0035415990546775553\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002805271899371685 \t\t Validation Loss: 0.002177569956984371\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002575808417876377 \t\t Validation Loss: 0.001458561867296409\n",
            "\t\t Validation Loss Decreased(0.001721--->0.001459) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026266558482815083 \t\t Validation Loss: 0.0012972855989713795\n",
            "\t\t Validation Loss Decreased(0.001459--->0.001297) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030272390215179404 \t\t Validation Loss: 0.0013772144089811123\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0037931783590465784 \t\t Validation Loss: 0.0015225115885886436\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004682028184745561 \t\t Validation Loss: 0.0016450478578917682\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004935768343243949 \t\t Validation Loss: 0.0017066058129645311\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00430898305274093 \t\t Validation Loss: 0.0017700570080286036\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003632381753492597 \t\t Validation Loss: 0.0018477575190795155\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003314702776323601 \t\t Validation Loss: 0.0019166314338620466\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032771304580131292 \t\t Validation Loss: 0.002014373048531035\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003439426886207248 \t\t Validation Loss: 0.00219979090616107\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003765043686144054 \t\t Validation Loss: 0.002482760133436666\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004159435403306742 \t\t Validation Loss: 0.002794714313215361\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004396689383980088 \t\t Validation Loss: 0.0030065400776668237\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004271835689684628 \t\t Validation Loss: 0.0030232227873057127\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003870039778711224 \t\t Validation Loss: 0.002868326255478538\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034574481259049797 \t\t Validation Loss: 0.00266315802358664\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031939222747963424 \t\t Validation Loss: 0.002533243825802436\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003110971281342712 \t\t Validation Loss: 0.002565746383669858\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032163786157537754 \t\t Validation Loss: 0.002828786579462198\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035442252882529755 \t\t Validation Loss: 0.003375223676602428\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004109942522592138 \t\t Validation Loss: 0.004141277364956645\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004735789082415805 \t\t Validation Loss: 0.004792571837942188\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004951095288757839 \t\t Validation Loss: 0.0048618430737406015\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004460112380518301 \t\t Validation Loss: 0.004214129124123316\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003621434751973563 \t\t Validation Loss: 0.0032428229729143474\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002946138583325051 \t\t Validation Loss: 0.002404758957429574\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002565351843431189 \t\t Validation Loss: 0.001863558803541729\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002386549981688527 \t\t Validation Loss: 0.001573506297203354\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023131623590440563 \t\t Validation Loss: 0.0014420256058936222\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002292859720418582 \t\t Validation Loss: 0.001409595671038215\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00230932653554388 \t\t Validation Loss: 0.001468561354648465\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023800399102509727 \t\t Validation Loss: 0.0016877914760978175\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002586262487818965 \t\t Validation Loss: 0.0023007011578346673\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031661170564679982 \t\t Validation Loss: 0.0038455862265366772\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004599224436222701 \t\t Validation Loss: 0.006880347240859499\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.007064126460259226 \t\t Validation Loss: 0.010612531314389063\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 83.22312222398477\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPXanh9IY3Fs"
      },
      "source": [
        "save_the_log(path=\"drive/MyDrive/complex/numerical_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXqMz0ELBf5r"
      },
      "source": [
        "## News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyBmg6U2zD9q"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pLahEYR7J0I"
      },
      "source": [
        "def runLogReg(batch_size_in, model_in, model_name_in):\n",
        "  # check if needed padding:\n",
        "  if len(train_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(train_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(train_X_news)),0)\n",
        "    padded_train_X_news = pad(train_X_news)\n",
        "    padded_train_Y_news = pad(train_Y_news)\n",
        "    train_news_tensor = data_utils.TensorDataset(padded_train_X_news, padded_train_Y_news)\n",
        "    train_news_loader = data_utils.DataLoader(dataset = train_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "  if len(valid_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(valid_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(valid_X_news)),0)\n",
        "    padded_valid_X_news = pad(valid_X_news)\n",
        "    padded_valid_Y_news = pad(valid_Y_news)\n",
        "    valid_news_tensor = data_utils.TensorDataset(padded_valid_X_news, padded_valid_Y_news)\n",
        "    valid_news_loader = data_utils.DataLoader(dataset = valid_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)                                 \n",
        "  \n",
        "  model = model_in\n",
        "\n",
        "  # train the model\n",
        "  model.train(train_X_news, train_Y_news)\n",
        "\n",
        "  # calculate accuraccy\n",
        "  global predict_test_array\n",
        "  predict_test_array = []\n",
        "\n",
        "  for x in test_X_news:\n",
        "      predict = model.forward(x)\n",
        "      predict_test_array.append(predict[0]) \n",
        "\n",
        "  test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "  acc = test_acc*100\n",
        "  print(f\"\\nAccuracy: {acc}%\\n\")\n",
        "\n",
        "  # round the prediction\n",
        "  rounded = []\n",
        "  for prediction in predict_test_array:\n",
        "      rounded.append(round(prediction))\n",
        "\n",
        "  print(pd.crosstab(torch.reshape(test_Y_news,(-1,)), np.array(rounded), rownames=[\"Actual\"], colnames=[\"Predicted\"]))\n",
        "  print()\n",
        "  print(classification_report(torch.reshape(test_Y_news,(-1,)), rounded))\n",
        "  print(accuracy_score(torch.reshape(test_Y_news,(-1,)), rounded))\n",
        "\n",
        "  # save the model to disk\n",
        "  filename = 'drive/MyDrive/complex/news_models/best_model_' + str(model_name_in) + '.pt'\n",
        "  pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72jEeRRKdq24",
        "outputId": "a613a74e-81aa-413f-e6f1-d741dc19d5f7"
      },
      "source": [
        "model_logreg = LogReg()\n",
        "runLogReg(batch_size_in=32, model_in=model_logreg, model_name_in=\"LogReg\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train done\n",
            "\n",
            "Accuracy: 51.776649475097656%\n",
            "\n",
            "Predicted   0    1\n",
            "Actual            \n",
            "0.0        86  107\n",
            "1.0        83  118\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.51      0.45      0.48       193\n",
            "         1.0       0.52      0.59      0.55       201\n",
            "\n",
            "    accuracy                           0.52       394\n",
            "   macro avg       0.52      0.52      0.51       394\n",
            "weighted avg       0.52      0.52      0.52       394\n",
            "\n",
            "0.5177664974619289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjcNni9wIUW_"
      },
      "source": [
        "def runVADER(batch_size_in, model_in):\n",
        "  # check if needed padding:\n",
        "  if len(train_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(train_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(train_X_news)),0)\n",
        "    padded_train_X_news = pad(train_X_news)\n",
        "    padded_train_Y_news = pad(train_Y_news)\n",
        "    train_news_tensor = data_utils.TensorDataset(padded_train_X_news, padded_train_Y_news)\n",
        "    train_news_loader = data_utils.DataLoader(dataset = train_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "  if len(valid_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(valid_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(valid_X_news)),0)\n",
        "    padded_valid_X_news = pad(valid_X_news)\n",
        "    padded_valid_Y_news = pad(valid_Y_news)\n",
        "    valid_news_tensor = data_utils.TensorDataset(padded_valid_X_news, padded_valid_Y_news)\n",
        "    valid_news_loader = data_utils.DataLoader(dataset = valid_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)                                 \n",
        "  \n",
        "  model = model_in\n",
        "\n",
        "  # calculate accuraccy\n",
        "  predict_test_array = []\n",
        "\n",
        "  for x in test_X_news:\n",
        "      predict = model.forward(x)\n",
        "      predict_test_array.append(predict) \n",
        "\n",
        "  test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "  acc = test_acc*100\n",
        "  print(f\"\\tAccuracy: {acc}%\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZeRZFNsJCmH",
        "outputId": "eba0991c-e648-4058-902c-c42ee6a1e7d8"
      },
      "source": [
        "model_vader = VADER()\n",
        "runVADER(batch_size_in=32, model_in=model_vader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tAccuracy: 50.76142120361328%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCr0iFVWH87T"
      },
      "source": [
        "def run_news_model(batch_size_in, model_in, optimizer_in, criterion_in, model_name_in):\n",
        "  # check if needed padding:\n",
        "  if len(train_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(train_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(train_X_news)),0)\n",
        "    padded_train_X_news = pad(train_X_news)\n",
        "    padded_train_Y_news = pad(train_Y_news)\n",
        "    train_news_tensor = data_utils.TensorDataset(padded_train_X_news, padded_train_Y_news)\n",
        "    train_news_loader = data_utils.DataLoader(dataset = train_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "  if len(valid_X_news) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(valid_X_news) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(valid_X_news)),0)\n",
        "    padded_valid_X_news = pad(valid_X_news)\n",
        "    padded_valid_Y_news = pad(valid_Y_news)\n",
        "    valid_news_tensor = data_utils.TensorDataset(padded_valid_X_news, padded_valid_Y_news)\n",
        "    valid_news_loader = data_utils.DataLoader(dataset = valid_news_tensor, \n",
        "                              batch_size = 32, shuffle = False)                                 \n",
        "  \n",
        "  model = model_in\n",
        "\n",
        "  optimizer = optimizer_in\n",
        "\n",
        "  # Set the device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  criterion = criterion_in\n",
        "\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  # Training with Validation\n",
        "  min_valid_loss = np.inf\n",
        "  epoch_num = 75\n",
        "\n",
        "  #store the losses\n",
        "  train_loss_array = []\n",
        "  train_acc_array = []\n",
        "  valid_loss_array = []\n",
        "  valid_acc_array = []\n",
        "  epoch = 0\n",
        "\n",
        "  early_stop = False\n",
        "  for e in range(epoch_num):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss = 0.0\n",
        "      train_acc = 0.0\n",
        "      model.train()\n",
        "      for x, y in train_news_loader: # x is news, y is trend target\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "            \n",
        "          # Clear the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Forward Pass\n",
        "          pred_y = model(x)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate gradients \n",
        "          loss.backward()\n",
        "          # Update Weights\n",
        "          optimizer.step()\n",
        "          # Calculate Loss\n",
        "          train_loss += loss.item() * x.size(0)\n",
        "          # Calculate acc\n",
        "          train_acc += binary_accuracy(pred_y, y).item() * x.size(0)\n",
        "\n",
        "\n",
        "      train_loss = train_loss / len(train_news_loader.sampler)\n",
        "      train_loss_array.append(train_loss) \n",
        "\n",
        "      train_acc = train_acc / len(train_news_loader.sampler)\n",
        "      train_acc_array.append(train_acc)\n",
        "\n",
        "      valid_loss = 0.0\n",
        "      valid_acc = 0.0\n",
        "      model.eval()     # Optional when not using Model Specific layer\n",
        "      for x, y in valid_news_loader:\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x, y = x.cuda(), y.cuda()\n",
        "            \n",
        "          # Forward Pass\n",
        "          pred_y = model(x)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate Loss\n",
        "          valid_loss += loss.item() * x.size(0)\n",
        "          # Calculate acc\n",
        "          valid_acc += binary_accuracy(pred_y, y).item() * x.size(0)\n",
        "    \n",
        "      valid_loss = valid_loss / len(valid_news_loader.sampler)\n",
        "      valid_loss_array.append(valid_loss)\n",
        "\n",
        "      valid_acc = valid_acc / len(valid_news_loader.sampler)\n",
        "      valid_acc_array.append(valid_acc)      \n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      \n",
        "      print(f'Epoch {e+1} \\t\\t Epoch time: {epoch_mins}m {epoch_secs}s\\n\\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
        "      print(f'\\tTrain Acc: {train_acc*100:.2f}% \\t\\ Val. Acc: {valid_acc*100:.2f}%')\n",
        "        \n",
        "      if min_valid_loss > valid_loss:\n",
        "          print(f'\\t\\t Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "                            \n",
        "          # Saving State Dict\n",
        "          torch.save(model.state_dict(), 'drive/MyDrive/complex/news_models/best_model_' + str(model_name_in) + '.pt')\n",
        "          epoch = e\n",
        "\n",
        "          # early stop\n",
        "          if valid_loss < 0.6 and (min_valid_loss - valid_loss) < 0.05:\n",
        "              early_stop = True\n",
        "\n",
        "          min_valid_loss = valid_loss\n",
        "\n",
        "      if (valid_loss > 0.7 and train_loss < 0.6):\n",
        "          early_stop = True\n",
        "        \n",
        "      if  early_stop == True and e > 10:\n",
        "          print(f'\\tEarly stop of the training')\n",
        "          break \n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(model.state_dict(), 'drive/MyDrive/complex/news_models/' + str(e) + '_model_' + str(model_name_in) + '.pt')  \n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(model.state_dict(), 'drive/MyDrive/complex/news_models/last_model_' + str(model_name_in) + '.pt')     \n",
        "\n",
        "  # Visualize the training\n",
        "  f1 = plt.figure(figsize=(16,8))\n",
        "  plt.title('Train and validation loss')\n",
        "  plt.plot(train_loss_array, color = \"green\", label = \"Train loss\")\n",
        "  plt.plot(valid_loss_array, color = \"blue\", label = \"Valid loss\")\n",
        "  plt.xlabel('Epoch',fontsize=18)\n",
        "  plt.ylabel('Loss',fontsize=18)\n",
        "  plt.legend(fontsize=18)\n",
        "  plt_teach = f1\n",
        "  plt.close()\n",
        "\n",
        "  model.load_state_dict(torch.load('drive/MyDrive/complex/news_models/best_model_' + str(model_name_in) + '.pt'))  \n",
        "\n",
        "  # calculate accuraccy\n",
        "  predict_test_array = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for x in test_X_news:\n",
        "          model.eval()\n",
        "          predict = model(torch.reshape(x, (1, len(x))).cuda())     # reshape for BERT   \n",
        "          predict_test_array.append(predict) \n",
        "\n",
        "  test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "  acc = test_acc*100\n",
        "  print(f\"\\tAccuracy: {acc}%\\n\")  \n",
        "\n",
        "  # return the results for compare:\n",
        "  #   epoch\n",
        "  #   trend result (best)\n",
        "  #   diagram (best)\n",
        "  return epoch, acc, plt_teach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys0n2duJVxxT",
        "outputId": "d3dca2c6-46e0-4422-a806-5bf43359b47d"
      },
      "source": [
        "# pretrain bert -> can be merge with the other cells later\n",
        "model_name = [\"BERT\"]\n",
        "model = BERT_news()\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# freeze the embedding layer / bert layer\n",
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "e_res = []\n",
        "acc_res = []\n",
        "batch_s_res = []\n",
        "optimizer_start_res = []\n",
        "optimizer_end_res = []\n",
        "dropout_res = []\n",
        "layer_dep_res = []\n",
        "act_res = []\n",
        "score_res = []\n",
        "plt_teach_res = []\n",
        "plt_pred_res = []\n",
        "\n",
        "epoch_out, acc_out, plt_teach_out = run_news_model(batch_size_in=32,model_in=model,\n",
        "          optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "e_res.append(epoch_out)\n",
        "acc_res.append(acc_out)\n",
        "batch_s_res.append(\"32\")\n",
        "optimizer_start_res.append(\"0.001\")\n",
        "optimizer_end_res.append(\"0.001\")\n",
        "dropout_res.append(\"0.3\")\n",
        "layer_dep_res.append(\"4\")\n",
        "act_res.append(\"Sigmoid\")\n",
        "score_res.append(\"NA\")\n",
        "plt_teach_res.append(plt_teach_out)\n",
        "plt_pred_res.append(\"None\")\n",
        "\n",
        "save_the_log(path=\"drive/MyDrive/complex/news_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 83,361 trainable parameters\n",
            "The model has 83,361 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 2m 1s\n",
            "\t\t Training Loss: 0.6970433045077968 \t\t Validation Loss: 0.6970557937255273\n",
            "\tTrain Acc: 45.19% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(inf--->0.697056) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6934060808774587 \t\t Validation Loss: 0.6952424003527715\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.697056--->0.695242) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6924466216886366 \t\t Validation Loss: 0.6943293167994573\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.695242--->0.694329) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6911123829918939 \t\t Validation Loss: 0.6938860416412354\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.694329--->0.693886) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6911462174879538 \t\t Validation Loss: 0.6936252438105069\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.693886--->0.693625) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6894209803761663 \t\t Validation Loss: 0.69354431445782\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.693625--->0.693544) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6906019916405549 \t\t Validation Loss: 0.6936692366233239\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.36%\n",
            "Epoch 8 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6915324088689443 \t\t Validation Loss: 0.6935356901242182\n",
            "\tTrain Acc: 46.62% \t\\ Val. Acc: 47.12%\n",
            "\t\t Validation Loss Decreased(0.693544--->0.693536) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6910076624638325 \t\t Validation Loss: 0.6937774649033179\n",
            "\tTrain Acc: 46.37% \t\\ Val. Acc: 47.60%\n",
            "Epoch 10 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6894622332340962 \t\t Validation Loss: 0.6937240408017085\n",
            "\tTrain Acc: 46.71% \t\\ Val. Acc: 47.60%\n",
            "Epoch 11 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6894079221261514 \t\t Validation Loss: 0.6930241126280564\n",
            "\tTrain Acc: 46.71% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.693536--->0.693024) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6895951644794361 \t\t Validation Loss: 0.693588009247413\n",
            "\tTrain Acc: 46.79% \t\\ Val. Acc: 47.36%\n",
            "Epoch 13 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6891798312599594 \t\t Validation Loss: 0.693927526473999\n",
            "\tTrain Acc: 46.62% \t\\ Val. Acc: 47.36%\n",
            "Epoch 14 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6899160501119252 \t\t Validation Loss: 0.6943932359035199\n",
            "\tTrain Acc: 47.38% \t\\ Val. Acc: 47.36%\n",
            "Epoch 15 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.688929562633102 \t\t Validation Loss: 0.6938365193513724\n",
            "\tTrain Acc: 46.71% \t\\ Val. Acc: 47.60%\n",
            "Epoch 16 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6877954634460243 \t\t Validation Loss: 0.694636189020597\n",
            "\tTrain Acc: 47.30% \t\\ Val. Acc: 47.60%\n",
            "Epoch 17 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6890284741247023 \t\t Validation Loss: 0.6941980490317712\n",
            "\tTrain Acc: 46.79% \t\\ Val. Acc: 47.36%\n",
            "Epoch 18 \t\t Epoch time: 2m 0s\n",
            "\t\t Training Loss: 0.6883163983757431 \t\t Validation Loss: 0.6936551194924575\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 47.60%\n",
            "Epoch 19 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6886569277660267 \t\t Validation Loss: 0.6929256961895869\n",
            "\tTrain Acc: 47.13% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.693024--->0.692926) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6878153730083156 \t\t Validation Loss: 0.6934955807832571\n",
            "\tTrain Acc: 47.21% \t\\ Val. Acc: 47.60%\n",
            "Epoch 21 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6903159328409143 \t\t Validation Loss: 0.6927568729107196\n",
            "\tTrain Acc: 47.13% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.692926--->0.692757) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6870683222203642 \t\t Validation Loss: 0.6933044653672439\n",
            "\tTrain Acc: 47.64% \t\\ Val. Acc: 48.56%\n",
            "Epoch 23 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6908061890988737 \t\t Validation Loss: 0.6921886572471032\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.692757--->0.692189) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.688497823637885 \t\t Validation Loss: 0.6928371282724234\n",
            "\tTrain Acc: 46.96% \t\\ Val. Acc: 48.32%\n",
            "Epoch 25 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6878852135426289 \t\t Validation Loss: 0.6930075241969182\n",
            "\tTrain Acc: 46.88% \t\\ Val. Acc: 47.60%\n",
            "Epoch 26 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6877903406684464 \t\t Validation Loss: 0.6922385875995343\n",
            "\tTrain Acc: 46.88% \t\\ Val. Acc: 48.80%\n",
            "Epoch 27 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6877730166589892 \t\t Validation Loss: 0.6936297279137832\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 47.36%\n",
            "Epoch 28 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6890861746427175 \t\t Validation Loss: 0.6917289082820599\n",
            "\tTrain Acc: 46.96% \t\\ Val. Acc: 48.80%\n",
            "\t\t Validation Loss Decreased(0.692189--->0.691729) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.688470949997773 \t\t Validation Loss: 0.6913938980836135\n",
            "\tTrain Acc: 47.21% \t\\ Val. Acc: 48.80%\n",
            "\t\t Validation Loss Decreased(0.691729--->0.691394) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6879915288976721 \t\t Validation Loss: 0.691806591474093\n",
            "\tTrain Acc: 47.04% \t\\ Val. Acc: 47.84%\n",
            "Epoch 31 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6877808425877545 \t\t Validation Loss: 0.6924410095581641\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 48.32%\n",
            "Epoch 32 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6890992941083135 \t\t Validation Loss: 0.6918000303781949\n",
            "\tTrain Acc: 47.04% \t\\ Val. Acc: 48.56%\n",
            "Epoch 33 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6875892507063376 \t\t Validation Loss: 0.6925088350589459\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 48.80%\n",
            "Epoch 34 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6865227592957986 \t\t Validation Loss: 0.6916465025681716\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 48.32%\n",
            "Epoch 35 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.687642389052623 \t\t Validation Loss: 0.6920944773233854\n",
            "\tTrain Acc: 47.13% \t\\ Val. Acc: 48.56%\n",
            "Epoch 36 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6859886565723935 \t\t Validation Loss: 0.6925429243307847\n",
            "\tTrain Acc: 47.89% \t\\ Val. Acc: 48.80%\n",
            "Epoch 37 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6848983136383263 \t\t Validation Loss: 0.6954839596381555\n",
            "\tTrain Acc: 48.14% \t\\ Val. Acc: 48.32%\n",
            "Epoch 38 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6851994008631319 \t\t Validation Loss: 0.6918577781090369\n",
            "\tTrain Acc: 48.06% \t\\ Val. Acc: 48.32%\n",
            "Epoch 39 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6866082339673429 \t\t Validation Loss: 0.6926904825063852\n",
            "\tTrain Acc: 47.13% \t\\ Val. Acc: 47.60%\n",
            "Epoch 40 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6855753708530117 \t\t Validation Loss: 0.6930015912422767\n",
            "\tTrain Acc: 47.64% \t\\ Val. Acc: 48.56%\n",
            "Epoch 41 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6870898926580274 \t\t Validation Loss: 0.695311216207651\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 48.32%\n",
            "Epoch 42 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6857285660666388 \t\t Validation Loss: 0.6927192348700303\n",
            "\tTrain Acc: 47.47% \t\\ Val. Acc: 48.08%\n",
            "Epoch 43 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6844958985174024 \t\t Validation Loss: 0.6931651463875403\n",
            "\tTrain Acc: 48.06% \t\\ Val. Acc: 47.60%\n",
            "Epoch 44 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6853749284873137 \t\t Validation Loss: 0.6922112886722271\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 47.84%\n",
            "Epoch 45 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6876797418336611 \t\t Validation Loss: 0.6936611808263339\n",
            "\tTrain Acc: 47.04% \t\\ Val. Acc: 48.08%\n",
            "Epoch 46 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6850496739954561 \t\t Validation Loss: 0.6925045481094947\n",
            "\tTrain Acc: 47.97% \t\\ Val. Acc: 47.60%\n",
            "Epoch 47 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6862489935514089 \t\t Validation Loss: 0.6913901384060199\n",
            "\tTrain Acc: 47.13% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.691394--->0.691390) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6889497541092537 \t\t Validation Loss: 0.6918822068434495\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 49 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6905476982529098 \t\t Validation Loss: 0.6917791366577148\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 50 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6892877076123212 \t\t Validation Loss: 0.6915294803105868\n",
            "\tTrain Acc: 46.20% \t\\ Val. Acc: 47.84%\n",
            "Epoch 51 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6857699445776038 \t\t Validation Loss: 0.6937152147293091\n",
            "\tTrain Acc: 47.30% \t\\ Val. Acc: 48.08%\n",
            "Epoch 52 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6845914963129405 \t\t Validation Loss: 0.6913446554770837\n",
            "\tTrain Acc: 48.06% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.691390--->0.691345) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6874938542778427 \t\t Validation Loss: 0.6908349807445819\n",
            "\tTrain Acc: 46.71% \t\\ Val. Acc: 47.84%\n",
            "\t\t Validation Loss Decreased(0.691345--->0.690835) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.681396134801813 \t\t Validation Loss: 0.6923697911776029\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 48.08%\n",
            "Epoch 55 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6869117375966665 \t\t Validation Loss: 0.6904690678303058\n",
            "\tTrain Acc: 47.55% \t\\ Val. Acc: 47.84%\n",
            "\t\t Validation Loss Decreased(0.690835--->0.690469) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6845850541784957 \t\t Validation Loss: 0.6916862176014826\n",
            "\tTrain Acc: 47.21% \t\\ Val. Acc: 48.56%\n",
            "Epoch 57 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6827903325493271 \t\t Validation Loss: 0.690235238808852\n",
            "\tTrain Acc: 48.56% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.690469--->0.690235) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6828312567762427 \t\t Validation Loss: 0.6897108050493094\n",
            "\tTrain Acc: 47.89% \t\\ Val. Acc: 48.80%\n",
            "\t\t Validation Loss Decreased(0.690235--->0.689711) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6827471304584194 \t\t Validation Loss: 0.6894331207642188\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 48.32%\n",
            "\t\t Validation Loss Decreased(0.689711--->0.689433) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.684174750302289 \t\t Validation Loss: 0.6932955109156095\n",
            "\tTrain Acc: 47.80% \t\\ Val. Acc: 49.04%\n",
            "Epoch 61 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6829727275951488 \t\t Validation Loss: 0.6913178333869348\n",
            "\tTrain Acc: 50.51% \t\\ Val. Acc: 48.80%\n",
            "Epoch 62 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6775122107686223 \t\t Validation Loss: 0.6924645396379324\n",
            "\tTrain Acc: 52.45% \t\\ Val. Acc: 50.24%\n",
            "Epoch 63 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6754884123802185 \t\t Validation Loss: 0.6926869429074801\n",
            "\tTrain Acc: 53.55% \t\\ Val. Acc: 52.16%\n",
            "Epoch 64 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6730552895649059 \t\t Validation Loss: 0.6977366117330698\n",
            "\tTrain Acc: 54.39% \t\\ Val. Acc: 50.72%\n",
            "Epoch 65 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6750962202613419 \t\t Validation Loss: 0.6952204016538767\n",
            "\tTrain Acc: 53.97% \t\\ Val. Acc: 53.37%\n",
            "Epoch 66 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6805098942808203 \t\t Validation Loss: 0.6905430784592261\n",
            "\tTrain Acc: 51.52% \t\\ Val. Acc: 50.00%\n",
            "Epoch 67 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6725781092772612 \t\t Validation Loss: 0.6889937795125521\n",
            "\tTrain Acc: 54.48% \t\\ Val. Acc: 51.44%\n",
            "\t\t Validation Loss Decreased(0.689433--->0.688994) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6698552982227223 \t\t Validation Loss: 0.6899074132625873\n",
            "\tTrain Acc: 55.15% \t\\ Val. Acc: 51.44%\n",
            "Epoch 69 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6704611617165643 \t\t Validation Loss: 0.6888322371702927\n",
            "\tTrain Acc: 56.17% \t\\ Val. Acc: 51.44%\n",
            "\t\t Validation Loss Decreased(0.688994--->0.688832) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6763767600059509 \t\t Validation Loss: 0.6923462519278893\n",
            "\tTrain Acc: 56.42% \t\\ Val. Acc: 48.56%\n",
            "Epoch 71 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6792689803484324 \t\t Validation Loss: 0.692195475101471\n",
            "\tTrain Acc: 54.48% \t\\ Val. Acc: 48.32%\n",
            "Epoch 72 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6766011167216945 \t\t Validation Loss: 0.7028491955537063\n",
            "\tTrain Acc: 50.84% \t\\ Val. Acc: 52.40%\n",
            "Epoch 73 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6620021188581312 \t\t Validation Loss: 0.6944768978999212\n",
            "\tTrain Acc: 59.38% \t\\ Val. Acc: 50.72%\n",
            "Epoch 74 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6640515134141252 \t\t Validation Loss: 0.7112834866230304\n",
            "\tTrain Acc: 57.85% \t\\ Val. Acc: 53.61%\n",
            "Epoch 75 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6716619762214454 \t\t Validation Loss: 0.6944058491633489\n",
            "\tTrain Acc: 56.33% \t\\ Val. Acc: 50.96%\n",
            "\tAccuracy: 49.746192932128906%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCq1sziHWWlI",
        "outputId": "f83a0762-32de-4c94-e5f2-660e48827357"
      },
      "source": [
        "for e in range(75):\n",
        "    model.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/{e}_model_' + str(model_name[0]) + '.pt'))  \n",
        "\n",
        "    # calculate accuraccy\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in test_X_news:\n",
        "            model.eval()\n",
        "            predict = model(torch.reshape(x, (1, len(x))).cuda())        \n",
        "            predict_test_array.append(predict) \n",
        "\n",
        "    test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "    acc = test_acc*100\n",
        "    print(f\"{e}. epoch\\n\")\n",
        "    print(f\"Accuracy: {acc}%\\n\")\n",
        "    print(\"------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Accuracy: 47.96954345703125%\n",
            "\n",
            "------------------\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Accuracy: 47.96954345703125%\n",
            "\n",
            "------------------\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Accuracy: 48.47715759277344%\n",
            "\n",
            "------------------\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Accuracy: 47.96954345703125%\n",
            "\n",
            "------------------\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Accuracy: 47.715736389160156%\n",
            "\n",
            "------------------\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnCTbCqpfD6z",
        "outputId": "f84c9eb9-2cc0-4571-d59d-49c61792f793"
      },
      "source": [
        "model_name = [\"BERT\"]\n",
        "model = BERT_news()\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# freeze the embedding layer / bert layer\n",
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 109,565,601 trainable parameters\n",
            "The model has 83,361 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z23bB2ce3ZI",
        "outputId": "5c35c0f3-7858-4671-c25f-643c7db44f88"
      },
      "source": [
        "for e in range(52,75):\n",
        "    model.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/{e}_model_' + str(model_name[0]) + '.pt', map_location=torch.device('cpu')))  \n",
        "\n",
        "    # calculate accuraccy\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in test_X_news:\n",
        "            model.eval()\n",
        "            predict = model(torch.reshape(x, (1, len(x))))        \n",
        "            predict_test_array.append(predict) \n",
        "\n",
        "    test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "    acc = test_acc*100\n",
        "    print(f\"{e}. epoch\\n\")\n",
        "    print(f\"Accuracy: {acc}%\\n\")\n",
        "    print(\"------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Accuracy: 49.23857879638672%\n",
            "\n",
            "------------------\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Accuracy: 51.26903533935547%\n",
            "\n",
            "------------------\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Accuracy: 52.284263610839844%\n",
            "\n",
            "------------------\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Accuracy: 52.5380744934082%\n",
            "\n",
            "------------------\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Accuracy: 51.26903533935547%\n",
            "\n",
            "------------------\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Accuracy: 48.73096466064453%\n",
            "\n",
            "------------------\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Accuracy: 49.746192932128906%\n",
            "\n",
            "------------------\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Accuracy: 49.746192932128906%\n",
            "\n",
            "------------------\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Accuracy: 52.791873931884766%\n",
            "\n",
            "------------------\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Accuracy: 50.50761795043945%\n",
            "\n",
            "------------------\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Accuracy: 51.52284622192383%\n",
            "\n",
            "------------------\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Accuracy: 50.76142120361328%\n",
            "\n",
            "------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8mTjtdtwbrA"
      },
      "source": [
        "e = 75\n",
        "model.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/{e}_model_' + str(model_name[0]) + '.pt', map_location=torch.device('cpu')))  \n",
        "\n",
        "# calculate accuraccy\n",
        "predict_test_array = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x in test_X_news:\n",
        "        model.eval()\n",
        "        predict = model(torch.reshape(x, (1, len(x))))        \n",
        "        predict_test_array.append(predict) \n",
        "\n",
        "test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "acc = test_acc*100\n",
        "print(f\"{e}. epoch\\n\")\n",
        "print(f\"Accuracy: {acc}%\\n\")\n",
        "print(\"------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hhad0qSy4fS",
        "outputId": "d7951033-2fe4-4530-f913-069225607107"
      },
      "source": [
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "\n",
        "model_name = [\"LSTM_pilot\"]\n",
        "model = LSTM_news(len(vocab),PAD_ID)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# copy the pretrained embedding weight\n",
        "model.embedding.weight.data.copy_(pretrained_embedding)\n",
        "\n",
        "# freeze the embedding layer\n",
        "model.embedding.weight.requires_grad = False\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "e_res = []\n",
        "acc_res = []\n",
        "batch_s_res = []\n",
        "optimizer_start_res = []\n",
        "optimizer_end_res = []\n",
        "dropout_res = []\n",
        "layer_dep_res = []\n",
        "act_res = []\n",
        "score_res = []\n",
        "plt_teach_res = []\n",
        "plt_pred_res = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 156,745 trainable parameters\n",
            "The model has 48,945 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRZGuIZAPw_z",
        "outputId": "e8d8a9fe-66e8-4a02-8045-a21b7bffdbcf"
      },
      "source": [
        "epoch_out, acc_out, plt_teach_out = run_news_model(batch_size_in=32,model_in=model,\n",
        "          optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "e_res.e_resappend(epoch_out)\n",
        "acc_res.append(acc_out)\n",
        "batch_s_res.append(\"32\")\n",
        "optimizer_start_res.append(\"0.001\")\n",
        "optimizer_end_res.append(\"0.001\")\n",
        "dropout_res.append(\"0.33\")\n",
        "layer_dep_res.append(\"6\")\n",
        "act_res.append(\"Sigmoid\")\n",
        "score_res.append(\"NA\")\n",
        "plt_teach_res.append(plt_teach_out)\n",
        "plt_pred_res.append(\"None\")\n",
        "\n",
        "save_the_log(path=\"drive/MyDrive/complex/news_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.7024878698426325 \t\t Validation Loss: 0.7062262159127456\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(inf--->0.706226) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6958418021330962 \t\t Validation Loss: 0.6946915021309485\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.706226--->0.694692) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6904327740540376 \t\t Validation Loss: 0.6926051011452308\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.694692--->0.692605) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6904663089159373 \t\t Validation Loss: 0.6923538263027484\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.692605--->0.692354) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6901416230846096 \t\t Validation Loss: 0.6923527671740606\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.692354--->0.692353) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.690439797736503 \t\t Validation Loss: 0.6923886262453519\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 7 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.691017476288048 \t\t Validation Loss: 0.6923206081757178\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "\t\t Validation Loss Decreased(0.692353--->0.692321) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6895681684081619 \t\t Validation Loss: 0.6923967324770414\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 9 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6897877664179415 \t\t Validation Loss: 0.6923510799041162\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 10 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6898907600222407 \t\t Validation Loss: 0.6925794390531687\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 11 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6877332684156057 \t\t Validation Loss: 0.6931141431515033\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 12 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6868712724866094 \t\t Validation Loss: 0.6931126117706299\n",
            "\tTrain Acc: 46.11% \t\\ Val. Acc: 47.60%\n",
            "Epoch 13 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.680146210902446 \t\t Validation Loss: 0.699277387215541\n",
            "\tTrain Acc: 51.35% \t\\ Val. Acc: 47.12%\n",
            "Epoch 14 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6741049563562548 \t\t Validation Loss: 0.701274023606227\n",
            "\tTrain Acc: 54.14% \t\\ Val. Acc: 48.08%\n",
            "Epoch 15 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6663218495008107 \t\t Validation Loss: 0.7005585982249334\n",
            "\tTrain Acc: 58.61% \t\\ Val. Acc: 48.56%\n",
            "Epoch 16 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6577829364183787 \t\t Validation Loss: 0.6976166046582736\n",
            "\tTrain Acc: 61.40% \t\\ Val. Acc: 48.32%\n",
            "Epoch 17 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6670423585015375 \t\t Validation Loss: 0.7208187580108643\n",
            "\tTrain Acc: 59.54% \t\\ Val. Acc: 51.20%\n",
            "Epoch 18 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6766635759456737 \t\t Validation Loss: 0.6996625661849976\n",
            "\tTrain Acc: 52.79% \t\\ Val. Acc: 48.32%\n",
            "Epoch 19 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6569190766360309 \t\t Validation Loss: 0.7255768134043767\n",
            "\tTrain Acc: 60.90% \t\\ Val. Acc: 48.32%\n",
            "Epoch 20 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6533424451544478 \t\t Validation Loss: 0.7329500684371362\n",
            "\tTrain Acc: 62.16% \t\\ Val. Acc: 48.80%\n",
            "Epoch 21 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6492725095233401 \t\t Validation Loss: 0.7238209201739385\n",
            "\tTrain Acc: 62.42% \t\\ Val. Acc: 49.28%\n",
            "Epoch 22 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6483597159385681 \t\t Validation Loss: 0.7248653769493103\n",
            "\tTrain Acc: 62.50% \t\\ Val. Acc: 48.80%\n",
            "Epoch 23 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6430350042678215 \t\t Validation Loss: 0.7177419891724219\n",
            "\tTrain Acc: 63.68% \t\\ Val. Acc: 49.52%\n",
            "Epoch 24 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6409899060790604 \t\t Validation Loss: 0.7089897164931664\n",
            "\tTrain Acc: 63.85% \t\\ Val. Acc: 50.48%\n",
            "Epoch 25 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6312118607598383 \t\t Validation Loss: 0.7255783677101135\n",
            "\tTrain Acc: 66.81% \t\\ Val. Acc: 48.08%\n",
            "Epoch 26 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6223581723264746 \t\t Validation Loss: 0.7088599021618183\n",
            "\tTrain Acc: 69.00% \t\\ Val. Acc: 49.76%\n",
            "Epoch 27 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6269343575915775 \t\t Validation Loss: 0.7040342871959393\n",
            "\tTrain Acc: 67.06% \t\\ Val. Acc: 49.04%\n",
            "Epoch 28 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6267309913764129 \t\t Validation Loss: 0.715432332112239\n",
            "\tTrain Acc: 68.16% \t\\ Val. Acc: 49.04%\n",
            "Epoch 29 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6310592213192502 \t\t Validation Loss: 0.7200072270173293\n",
            "\tTrain Acc: 68.67% \t\\ Val. Acc: 49.52%\n",
            "Epoch 30 \t\t Epoch time: 0m 14s\n",
            "\t\t Training Loss: 0.6347752938399444 \t\t Validation Loss: 0.7387751157467182\n",
            "\tTrain Acc: 65.88% \t\\ Val. Acc: 48.56%\n",
            "\tAccuracy: 48.984771728515625%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuscSXOCU2Eg",
        "outputId": "008cbda0-fdd8-4940-ba05-c15123a7ea44"
      },
      "source": [
        "for e in range(30):\n",
        "    model.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/{e}_model_' + str(model_name[0]) + '.pt'))  \n",
        "\n",
        "    # calculate accuraccy\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x in test_X_news:\n",
        "            model.eval()\n",
        "            predict = model(x.to(\"cuda\"))        \n",
        "            predict_test_array.append(predict) \n",
        "\n",
        "    test_acc = binary_accuracy(torch.FloatTensor(predict_test_array).reshape(len(predict_test_array),1),test_Y_news)  \n",
        "    acc = test_acc*100\n",
        "    print(f\"{e}. epoch\\n\")\n",
        "    print(f\"Accuracy: {acc}%\\n\")\n",
        "    print(\"------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Accuracy: 48.984771728515625%\n",
            "\n",
            "------------------\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Accuracy: 50.0%\n",
            "\n",
            "------------------\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Accuracy: 52.5380744934082%\n",
            "\n",
            "------------------\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Accuracy: 52.284263610839844%\n",
            "\n",
            "------------------\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Accuracy: 51.52284622192383%\n",
            "\n",
            "------------------\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Accuracy: 52.791873931884766%\n",
            "\n",
            "------------------\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Accuracy: 51.52284622192383%\n",
            "\n",
            "------------------\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Accuracy: 51.52284622192383%\n",
            "\n",
            "------------------\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Accuracy: 49.746192932128906%\n",
            "\n",
            "------------------\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Accuracy: 51.015228271484375%\n",
            "\n",
            "------------------\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Accuracy: 48.223350524902344%\n",
            "\n",
            "------------------\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Accuracy: 50.253807067871094%\n",
            "\n",
            "------------------\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Accuracy: 50.253807067871094%\n",
            "\n",
            "------------------\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Accuracy: 49.746192932128906%\n",
            "\n",
            "------------------\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Accuracy: 52.284263610839844%\n",
            "\n",
            "------------------\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Accuracy: 53.80710220336914%\n",
            "\n",
            "------------------\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Accuracy: 50.76142120361328%\n",
            "\n",
            "------------------\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Accuracy: 50.76142120361328%\n",
            "\n",
            "------------------\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Accuracy: 51.26903533935547%\n",
            "\n",
            "------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLkMZU8VBhRI"
      },
      "source": [
        "## Complex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kl0EV5uBl3z"
      },
      "source": [
        "def run_model_complex(batch_size_in, model_in, optimizer_in, criterion_in, model_name_in):\n",
        "  # check if needed padding:\n",
        "  if len(train_X_num) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(train_X_num) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(train_X_num)),0)\n",
        "    padded_train_X_num = pad(train_X_num)\n",
        "    padded_train_X_news = pad(train_X_news)\n",
        "    padded_train_Y_num = pad(train_Y_num)\n",
        "    train_all_tensor = data_utils.TensorDataset(padded_train_X_num, padded_train_X_news, padded_train_Y_num)\n",
        "    train_all_loader = data_utils.DataLoader(dataset = train_all_tensor, \n",
        "                              batch_size = 32, shuffle = False)\n",
        "\n",
        "  if len(valid_X_num) % batch_size_in == 0:\n",
        "    pass\n",
        "  else:\n",
        "    res = int(len(valid_X_num) / batch_size_in)\n",
        "    needed_length = (res + 1) * batch_size_in\n",
        "    pad = nn.ConstantPad2d((0,0,0,needed_length-len(valid_X_num)),0)\n",
        "    padded_valid_X_num = pad(valid_X_num)\n",
        "    padded_valid_X_news = pad(valid_X_news)    \n",
        "    padded_valid_Y_num = pad(valid_Y_num)\n",
        "    valid_all_tensor = data_utils.TensorDataset(padded_valid_X_num, padded_valid_X_news, padded_valid_Y_num)\n",
        "    valid_all_loader = data_utils.DataLoader(dataset = valid_all_tensor, \n",
        "                              batch_size = 32, shuffle = False)                                 \n",
        "  \n",
        "  model = model_in\n",
        "\n",
        "  optimizer = optimizer_in\n",
        "\n",
        "  # Set the device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  criterion = criterion_in\n",
        "\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  # Training with Validation\n",
        "  min_valid_loss = np.inf\n",
        "  #epoch_num = 200\n",
        "  epoch_num = 75 # bert\n",
        "\n",
        "  #store the losses\n",
        "  train_loss_array = []\n",
        "  valid_loss_array = []\n",
        "  epoch = 0\n",
        "\n",
        "  real_test_array = []\n",
        "  real_trend_array = test[\"Trend target\"]  \n",
        "  for y in test_Y_num:\n",
        "    real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "  early_stop = False\n",
        "  for e in range(epoch_num):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss = 0.0\n",
        "      model.train()\n",
        "      for x_num, x_text, y in train_all_loader:\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x_num, x_text, y = x_num.cuda(), x_text.cuda(), y.cuda()\n",
        "            \n",
        "          # Clear the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # Forward Pass\n",
        "          pred_y = model(x_num, x_text)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate gradients \n",
        "          loss.backward()\n",
        "          # Update Weights\n",
        "          optimizer.step()\n",
        "          # Calculate Loss\n",
        "          train_loss += loss.item() * x_num.size(0)\n",
        "\n",
        "      train_loss = train_loss / len(train_all_loader.sampler)\n",
        "      train_loss_array.append(train_loss) \n",
        "\n",
        "      valid_loss = 0.0\n",
        "      model.eval()     # Optional when not using Model Specific layer\n",
        "      for x_num, x_text, y in valid_all_loader:\n",
        "          # Transfer Data to GPU if available\n",
        "          if torch.cuda.is_available():\n",
        "              x_num, x_text, y = x_num.cuda(), x_text.cuda(), y.cuda()\n",
        "            \n",
        "          # Forward Pass\n",
        "          pred_y = model(x_num, x_text)\n",
        "          # Find the Loss\n",
        "          loss = criterion(pred_y,y)\n",
        "          # Calculate Loss\n",
        "          valid_loss += loss.item() * x_num.size(0)\n",
        "    \n",
        "      valid_loss = valid_loss / len(valid_all_loader.sampler)\n",
        "      valid_loss_array.append(valid_loss)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      \n",
        "      print(f'Epoch {e+1} \\t\\t Epoch time: {epoch_mins}m {epoch_secs}s\\n\\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n",
        "        \n",
        "      if min_valid_loss > valid_loss:\n",
        "          print(f'\\t\\t Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "                            \n",
        "          # Saving State Dict\n",
        "          torch.save(model.state_dict(), 'drive/MyDrive/complex/complex_models/best_model_' + str(model_name_in) + '.pt')\n",
        "          epoch = e\n",
        "\n",
        "          # early stop\n",
        "          if (valid_loss < 0.05 and train_loss < 0.05) and (min_valid_loss - valid_loss) < 0.0005:\n",
        "              early_stop = True\n",
        "\n",
        "          min_valid_loss = valid_loss\n",
        "        \n",
        "      if  early_stop == True and e > 25: # for bert\n",
        "          print(f'\\tEarly stop of the training')\n",
        "          break \n",
        "\n",
        "      # Saving State Dict\n",
        "      #torch.save(model.state_dict(), f'drive/MyDrive/complex/complex_models/{e}_model_' + str(model_name_in) + '.pt') -> only best for BERT\n",
        "      #epoch = e    \n",
        "\n",
        "  # Visualize the training\n",
        "  f1 = plt.figure(figsize=(16,8))\n",
        "  plt.title('Train and validation loss')\n",
        "  plt.plot(train_loss_array, color = \"green\", label = \"Train loss\")\n",
        "  plt.plot(valid_loss_array, color = \"blue\", label = \"Valid loss\")\n",
        "  plt.xlabel('Epoch',fontsize=18)\n",
        "  plt.ylabel('Loss',fontsize=18)\n",
        "  plt.legend(fontsize=18)\n",
        "  plt_teach = f1\n",
        "  plt.close()\n",
        "\n",
        "  model.load_state_dict(torch.load('drive/MyDrive/complex/complex_models/best_model_' + str(model_name_in) + '.pt'))\n",
        "\n",
        "  predict_test_array = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i in range(len(test_X_num)):\n",
        "          model.eval()\n",
        "          predict = model(test_X_num[i].reshape(1,-1).to(device), test_X_news[i].reshape(1,-1).to(device))        \n",
        "          predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "  # calculate score\n",
        "  score = calc_score(predict_test_array,real_test_array)    \n",
        "  print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "  #check the trend -> create an array with the real and with the predicted trend\n",
        "  #if the current value is bigger than before -> 1\n",
        "  #otherwise (same or smaller) -> 0\n",
        "  predicted_trend_array = []\n",
        "  for element in range(len(predict_test_array)):\n",
        "      real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "      if real_today_close > predict_test_array[element].values:\n",
        "          predicted_trend_array.append(0)\n",
        "      else:\n",
        "          predicted_trend_array.append(1)\n",
        "\n",
        "  #check the number of differences\n",
        "  trend_diff_array = []\n",
        "  for element in range(len(real_trend_array)):\n",
        "      if real_trend_array[element] != predicted_trend_array[element]:\n",
        "        trend_diff_array.append(element)\n",
        "\n",
        "  #percentage of good predict\n",
        "  acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "  print(f\"Accuracy: {acc}\\n\")  \n",
        "  #visualize it\n",
        "  f2 = plt.figure(figsize=(24,12))\n",
        "  plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "  plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "          label = \"Real\")\n",
        "  plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "          label = \"Predicted\")\n",
        "  plt.xlabel(\"Date\",fontsize = 18)\n",
        "  plt.legend(fontsize = 18)\n",
        "  f2.set_size_inches(12,6)\n",
        "  plt_best = f2\n",
        "  plt.close()\n",
        "\n",
        "  # return the results for compare:\n",
        "  #   epoch\n",
        "  #   trend result (best)\n",
        "  #   diagram (best)\n",
        "  #   score\n",
        "  return epoch, score, acc, plt_teach, plt_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gvm4oGdXnCg",
        "outputId": "bde34037-30d5-4132-d85e-2807baa390c7"
      },
      "source": [
        "model_name = [\"Complex_1\"]\n",
        "\n",
        "# create and load models\n",
        "model_num = Linear_108_num()\n",
        "model_num.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt'))\n",
        "\n",
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "model_news = LSTM_news(len(vocab),PAD_ID)\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/26_model_LSTM_pilot.pt'))\n",
        "\n",
        "model = Complex_1(model_num, model_news)\n",
        "\n",
        "# freeze the numerical and news weights\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "for p in model.model_num.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.model_news.parameters():\n",
        "    p.requires_grad = False\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "e_res = []\n",
        "acc_res = []\n",
        "batch_s_res = []\n",
        "optimizer_start_res = []\n",
        "optimizer_end_res = []\n",
        "dropout_res = []\n",
        "layer_dep_res = []\n",
        "act_res = []\n",
        "score_res = []\n",
        "plt_teach_res = []\n",
        "plt_pred_res = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,555,011 trainable parameters\n",
            "The model has 857 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x-mXeGRbUFe",
        "outputId": "da32919c-bc81-4d8c-cbf6-d4609ee31f68"
      },
      "source": [
        "epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "          optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "e_res.append(epoch_out)\n",
        "acc_res.append(acc_out)\n",
        "batch_s_res.append(\"32\")\n",
        "optimizer_start_res.append(\"0.0001\")\n",
        "optimizer_end_res.append(\"0.0001\")\n",
        "dropout_res.append(\"0\")\n",
        "layer_dep_res.append(\"6\")\n",
        "act_res.append(\"None\")\n",
        "score_res.append(score_out)\n",
        "plt_teach_res.append(plt_teach_out)\n",
        "plt_pred_res.append(plt_best_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.595920604710291 \t\t Validation Loss: 0.9398433359769675\n",
            "\t\t Validation Loss Decreased(inf--->0.939843) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.5258329630879736 \t\t Validation Loss: 1.0004539558520684\n",
            "Epoch 3 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.4643050474995697 \t\t Validation Loss: 1.048401727126195\n",
            "Epoch 4 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.4092565536801074 \t\t Validation Loss: 1.0818303502522981\n",
            "Epoch 5 \t\t Epoch time: 0m 7s\n",
            "\t\t Training Loss: 0.3617907448898296 \t\t Validation Loss: 1.0987170751278217\n",
            "Epoch 6 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.31714092094350504 \t\t Validation Loss: 1.0989564565511851\n",
            "Epoch 7 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.2813455330560336 \t\t Validation Loss: 1.0819417971831102\n",
            "Epoch 8 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.24919866287225 \t\t Validation Loss: 1.049562014066256\n",
            "Epoch 9 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.22254798903658585 \t\t Validation Loss: 1.003705409856943\n",
            "Epoch 10 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.19701957883867058 \t\t Validation Loss: 0.9461967165653522\n",
            "Epoch 11 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.17776189704199094 \t\t Validation Loss: 0.8800469934940338\n",
            "\t\t Validation Loss Decreased(0.939843--->0.880047) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.15851870113732042 \t\t Validation Loss: 0.8092902898788452\n",
            "\t\t Validation Loss Decreased(0.880047--->0.809290) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.14009363400573666 \t\t Validation Loss: 0.7351020918442652\n",
            "\t\t Validation Loss Decreased(0.809290--->0.735102) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.12579146425264912 \t\t Validation Loss: 0.660077853844716\n",
            "\t\t Validation Loss Decreased(0.735102--->0.660078) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.11102039462609871 \t\t Validation Loss: 0.5863573826276339\n",
            "\t\t Validation Loss Decreased(0.660078--->0.586357) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.09673934278858674 \t\t Validation Loss: 0.5148533834860876\n",
            "\t\t Validation Loss Decreased(0.586357--->0.514853) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.08536496083881404 \t\t Validation Loss: 0.4469325336126181\n",
            "\t\t Validation Loss Decreased(0.514853--->0.446933) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.07500476465636008 \t\t Validation Loss: 0.38378228820287263\n",
            "\t\t Validation Loss Decreased(0.446933--->0.383782) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.06554246396833174 \t\t Validation Loss: 0.3257394169385617\n",
            "\t\t Validation Loss Decreased(0.383782--->0.325739) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.056421785489530175 \t\t Validation Loss: 0.27299050757518184\n",
            "\t\t Validation Loss Decreased(0.325739--->0.272991) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.04821879967946459 \t\t Validation Loss: 0.22619458803763756\n",
            "\t\t Validation Loss Decreased(0.272991--->0.226195) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 7s\n",
            "\t\t Training Loss: 0.03993021158149113 \t\t Validation Loss: 0.18494733308370298\n",
            "\t\t Validation Loss Decreased(0.226195--->0.184947) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.03424949263147003 \t\t Validation Loss: 0.14939365421350187\n",
            "\t\t Validation Loss Decreased(0.184947--->0.149394) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.028513857289343268 \t\t Validation Loss: 0.11905299986784275\n",
            "\t\t Validation Loss Decreased(0.149394--->0.119053) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 7s\n",
            "\t\t Training Loss: 0.023756897605552867 \t\t Validation Loss: 0.09345772575873595\n",
            "\t\t Validation Loss Decreased(0.119053--->0.093458) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.019576723234274902 \t\t Validation Loss: 0.07255836834128086\n",
            "\t\t Validation Loss Decreased(0.093458--->0.072558) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.015538808474367534 \t\t Validation Loss: 0.05552390590310097\n",
            "\t\t Validation Loss Decreased(0.072558--->0.055524) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.012834604703695388 \t\t Validation Loss: 0.04226023030395691\n",
            "\t\t Validation Loss Decreased(0.055524--->0.042260) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.010520656763047382 \t\t Validation Loss: 0.03204064816236496\n",
            "\t\t Validation Loss Decreased(0.042260--->0.032041) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.008682860728554629 \t\t Validation Loss: 0.02395771644436396\n",
            "\t\t Validation Loss Decreased(0.032041--->0.023958) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.006906818629972436 \t\t Validation Loss: 0.017826883122324944\n",
            "\t\t Validation Loss Decreased(0.023958--->0.017827) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00570225587894989 \t\t Validation Loss: 0.013359521658947835\n",
            "\t\t Validation Loss Decreased(0.017827--->0.013360) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004803010854064612 \t\t Validation Loss: 0.010097534789775427\n",
            "\t\t Validation Loss Decreased(0.013360--->0.010098) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004093852743657457 \t\t Validation Loss: 0.007750936282368807\n",
            "\t\t Validation Loss Decreased(0.010098--->0.007751) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0036369835850908546 \t\t Validation Loss: 0.006001905359041233\n",
            "\t\t Validation Loss Decreased(0.007751--->0.006002) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0031862794799176423 \t\t Validation Loss: 0.004775468904811602\n",
            "\t\t Validation Loss Decreased(0.006002--->0.004775) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0028996695308140604 \t\t Validation Loss: 0.0038904052299375716\n",
            "\t\t Validation Loss Decreased(0.004775--->0.003890) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002696100376999459 \t\t Validation Loss: 0.003251094496450745\n",
            "\t\t Validation Loss Decreased(0.003890--->0.003251) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002549740901166523 \t\t Validation Loss: 0.0028078489256306337\n",
            "\t\t Validation Loss Decreased(0.003251--->0.002808) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002430593047442066 \t\t Validation Loss: 0.0025023029526122487\n",
            "\t\t Validation Loss Decreased(0.002808--->0.002502) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002352605905773974 \t\t Validation Loss: 0.0022776540762816486\n",
            "\t\t Validation Loss Decreased(0.002502--->0.002278) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023249075826685373 \t\t Validation Loss: 0.0021134265519392034\n",
            "\t\t Validation Loss Decreased(0.002278--->0.002113) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022603937588330054 \t\t Validation Loss: 0.0019976346878908\n",
            "\t\t Validation Loss Decreased(0.002113--->0.001998) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022572380556674623 \t\t Validation Loss: 0.00190631070962319\n",
            "\t\t Validation Loss Decreased(0.001998--->0.001906) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002242547315167817 \t\t Validation Loss: 0.0018423172027374117\n",
            "\t\t Validation Loss Decreased(0.001906--->0.001842) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022301318240351975 \t\t Validation Loss: 0.001795675627923069\n",
            "\t\t Validation Loss Decreased(0.001842--->0.001796) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022371864478053474 \t\t Validation Loss: 0.0017550180827339108\n",
            "\t\t Validation Loss Decreased(0.001796--->0.001755) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022236984443372567 \t\t Validation Loss: 0.001728716861599913\n",
            "\t\t Validation Loss Decreased(0.001755--->0.001729) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002224286237096011 \t\t Validation Loss: 0.0017075180523813916\n",
            "\t\t Validation Loss Decreased(0.001729--->0.001708) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022181568036447404 \t\t Validation Loss: 0.0016943637210016067\n",
            "\t\t Validation Loss Decreased(0.001708--->0.001694) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002223524929659844 \t\t Validation Loss: 0.0016792011089049852\n",
            "\t\t Validation Loss Decreased(0.001694--->0.001679) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002228278137359617 \t\t Validation Loss: 0.0016700695492685414\n",
            "\t\t Validation Loss Decreased(0.001679--->0.001670) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022196376372153894 \t\t Validation Loss: 0.0016610120566418539\n",
            "\t\t Validation Loss Decreased(0.001670--->0.001661) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002227519969204189 \t\t Validation Loss: 0.0016549373394809663\n",
            "\t\t Validation Loss Decreased(0.001661--->0.001655) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022248266165685914 \t\t Validation Loss: 0.0016516498245227223\n",
            "\t\t Validation Loss Decreased(0.001655--->0.001652) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022295891036707405 \t\t Validation Loss: 0.0016508285457698198\n",
            "\t\t Validation Loss Decreased(0.001652--->0.001651) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002223131723538343 \t\t Validation Loss: 0.0016483436562479115\n",
            "\t\t Validation Loss Decreased(0.001651--->0.001648) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022295106948051297 \t\t Validation Loss: 0.001644956933047909\n",
            "\t\t Validation Loss Decreased(0.001648--->0.001645) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002234287769219058 \t\t Validation Loss: 0.0016422610049350904\n",
            "\t\t Validation Loss Decreased(0.001645--->0.001642) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022314417489011445 \t\t Validation Loss: 0.0016381241282663094\n",
            "\t\t Validation Loss Decreased(0.001642--->0.001638) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022302516972067185 \t\t Validation Loss: 0.0016330298609458483\n",
            "\t\t Validation Loss Decreased(0.001638--->0.001633) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022212425191773452 \t\t Validation Loss: 0.0016283045540778683\n",
            "\t\t Validation Loss Decreased(0.001633--->0.001628) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022236814539929903 \t\t Validation Loss: 0.001629249895743739\n",
            "Epoch 64 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002223254315951537 \t\t Validation Loss: 0.0016294649909608639\n",
            "Epoch 65 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002241814644327639 \t\t Validation Loss: 0.0016303442937966723\n",
            "Epoch 66 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022206172941730837 \t\t Validation Loss: 0.001638777558512699\n",
            "Epoch 67 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002231911280997903 \t\t Validation Loss: 0.0016426216819896721\n",
            "Epoch 68 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022243481865656133 \t\t Validation Loss: 0.001636310476057518\n",
            "Epoch 69 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002225497422890889 \t\t Validation Loss: 0.0016344962104295308\n",
            "Epoch 70 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022297507730891578 \t\t Validation Loss: 0.0016316861370936609\n",
            "Epoch 71 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022221319448215435 \t\t Validation Loss: 0.001632397036211422\n",
            "Epoch 72 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022173169622751506 \t\t Validation Loss: 0.0016322925758476441\n",
            "Epoch 73 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022213569224019208 \t\t Validation Loss: 0.0016392550986403455\n",
            "Epoch 74 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002219825310513025 \t\t Validation Loss: 0.0016312654703282393\n",
            "Epoch 75 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002221870280934397 \t\t Validation Loss: 0.001633565675897094\n",
            "Epoch 76 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022368467400303562 \t\t Validation Loss: 0.001630475135663381\n",
            "Epoch 77 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002229464488934625 \t\t Validation Loss: 0.001627329401134585\n",
            "\t\t Validation Loss Decreased(0.001628--->0.001627) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 95.01712206535532\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W35SSsUvmlSQ",
        "outputId": "361edc4d-9888-4318-89da-52d1149849f9"
      },
      "source": [
        "for e in range(78):\n",
        "  print(f'{e}. epoch')\n",
        "  model.load_state_dict(torch.load(f'drive/MyDrive/complex/complex_models/{e}_model_Complex_1.pt'))\n",
        "\n",
        "  predict_test_array = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i in range(len(test_X_num)):\n",
        "          model.eval()\n",
        "          predict = model(test_X_num[i].reshape(1,-1).to(\"cpu\"), test_X_news[i].reshape(1,-1).to(\"cpu\"))        \n",
        "          predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "  real_test_array = []\n",
        "  real_trend_array = test[\"Trend target\"]  \n",
        "  for y in test_Y_num:\n",
        "    real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "  # calculate score\n",
        "  score = calc_score(predict_test_array,real_test_array)    \n",
        "  print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "  #check the trend -> create an array with the real and with the predicted trend\n",
        "  #if the current value is bigger than before -> 1\n",
        "  #otherwise (same or smaller) -> 0\n",
        "  predicted_trend_array = []\n",
        "  for element in range(len(predict_test_array)):\n",
        "      real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "      if real_today_close > predict_test_array[element].values:\n",
        "          predicted_trend_array.append(0)\n",
        "      else:\n",
        "          predicted_trend_array.append(1)\n",
        "\n",
        "  #check the number of differences\n",
        "  trend_diff_array = []\n",
        "  for element in range(len(real_trend_array)):\n",
        "      if real_trend_array[element] != predicted_trend_array[element]:\n",
        "        trend_diff_array.append(element)\n",
        "\n",
        "  #percentage of good predict\n",
        "  acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "  print(f\"Accuracy: {acc}\\n\")  \n",
        "  #visualize it\n",
        "  f2 = plt.figure(figsize=(24,12))\n",
        "  plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "  plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "          label = \"Real\")\n",
        "  plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "          label = \"Predicted\")\n",
        "  plt.xlabel(\"Date\",fontsize = 18)\n",
        "  plt.legend(fontsize = 18)\n",
        "  f2.set_size_inches(12,6)\n",
        "  plt_pred = f2\n",
        "  plt.close()\n",
        "\n",
        "  plt_pred.savefig('drive/MyDrive/complex/complex_models/'+str(e)+\"_log_pred_\" + str('Complex_1') + \"_.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. epoch\n",
            "\n",
            "Score: 45628.812182741116\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 47117.15736040609\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 47998.64467005076\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 48258.06598984772\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 47869.8730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 46853.84263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 45243.131979695434\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 43131.081218274114\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 40610.37817258883\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 37758.07614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 34698.82741116751\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 31563.5\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 28389.946700507615\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 25269.42385786802\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 22266.573604060915\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 19407.69923857868\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 16735.73730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 14278.345177664974\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 12045.093908629442\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 10036.615482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 8269.467639593908\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 6726.906725888325\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 5406.284898477157\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 4286.114530456853\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 3350.889276649746\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 2591.81154822335\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 1979.71875\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 1505.7572969543148\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 1142.9731123096446\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 857.970019035533\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 643.9886579949239\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 489.9709708121827\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 377.27030456852793\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 297.50210184010155\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 238.1159779505076\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 196.65476284898477\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 167.2446660850254\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 145.90986873413706\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 131.24748175761422\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 121.47165490164974\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 114.1334569321066\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 109.13787872779187\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 105.42586056472081\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 102.57076855964468\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 100.69372620558376\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 99.28423421637056\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 98.27595574238579\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 97.57262254124366\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 97.0259953997462\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 96.65845098350253\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 96.26517885469544\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 96.07826380076142\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 95.81929132296955\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 95.68585421954315\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 95.70009121192894\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 95.76675523477158\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 95.66373532677665\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 95.57037198604061\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 95.55320034898477\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 95.42023913388324\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 95.21211730647208\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 94.92602910850253\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 94.92597953680203\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 94.94879243337563\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 95.19040490164974\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 95.47481757614213\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 95.70262928299492\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 95.31352117703045\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 95.24174135469544\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 95.21299968274111\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 95.08167433375634\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 94.94718631027919\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 95.31824040291878\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 94.87678458121827\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 95.02192060596447\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 95.0390724143401\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "76. epoch\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-aa5f6ebb6a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{e}. epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'drive/MyDrive/complex/complex_models/{e}_model_Complex_1.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpredict_test_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/complex/complex_models/76_model_Complex_1.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3IobeVmbVl7"
      },
      "source": [
        "save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMr9P8IKxFpm",
        "outputId": "d27e29cd-22ea-4cc6-f3da-ebb5ca241309"
      },
      "source": [
        "model_name_tmp = [\"Complex_1_LSTM_92_LSTM_news\", \"Complex_1_LSTM_36_LSTM_news\"]\n",
        "model_num = []\n",
        "model_tmp = LSTM_92_num()\n",
        "model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt'))\n",
        "model_num.append(model_tmp)\n",
        "model_tmp = LSTM_36_num()\n",
        "model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt'))\n",
        "model_num.append(model_tmp)\n",
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "model_news = LSTM_news(len(vocab),PAD_ID)\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/26_model_LSTM_pilot.pt'))\n",
        "\n",
        "for i in range(2):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "    model_num_tmp = model_num[i]\n",
        "    model = Complex_1(model_num_tmp, model_news)\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"0\")\n",
        "    layer_dep_res.append(\"4\")\n",
        "    act_res.append(\"None\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 2,320,883 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.9182345874834101 \t\t Validation Loss: 0.4179050910931367\n",
            "\t\t Validation Loss Decreased(inf--->0.417905) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.7995279866044183 \t\t Validation Loss: 0.4770677788899495\n",
            "Epoch 3 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.6903636807676505 \t\t Validation Loss: 0.5320911155297205\n",
            "Epoch 4 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.5896841961940801 \t\t Validation Loss: 0.5814711485917752\n",
            "Epoch 5 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.49745096961615254 \t\t Validation Loss: 0.6226301926832932\n",
            "Epoch 6 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.4161853328775111 \t\t Validation Loss: 0.6540534289983603\n",
            "Epoch 7 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.34384101580173987 \t\t Validation Loss: 0.6729669502148261\n",
            "Epoch 8 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.28273807312487753 \t\t Validation Loss: 0.6779344517451066\n",
            "Epoch 9 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.2310436598679705 \t\t Validation Loss: 0.6677564199154193\n",
            "Epoch 10 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.18957749606311522 \t\t Validation Loss: 0.6429252234789041\n",
            "Epoch 11 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.1550364555866533 \t\t Validation Loss: 0.6052328760807331\n",
            "Epoch 12 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.12868240113193924 \t\t Validation Loss: 0.5573572424741892\n",
            "Epoch 13 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.10684320202248322 \t\t Validation Loss: 0.5024854059402759\n",
            "Epoch 14 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.09002851137287311 \t\t Validation Loss: 0.44403691245959354\n",
            "Epoch 15 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.07495863436771608 \t\t Validation Loss: 0.3850544977646608\n",
            "\t\t Validation Loss Decreased(0.417905--->0.385054) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.06350474629702198 \t\t Validation Loss: 0.3278663605451584\n",
            "\t\t Validation Loss Decreased(0.385054--->0.327866) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.052456863620947744 \t\t Validation Loss: 0.27403824031352997\n",
            "\t\t Validation Loss Decreased(0.327866--->0.274038) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.043776326775047426 \t\t Validation Loss: 0.2251992695606672\n",
            "\t\t Validation Loss Decreased(0.274038--->0.225199) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.036485964476407784 \t\t Validation Loss: 0.18156822942770445\n",
            "\t\t Validation Loss Decreased(0.225199--->0.181568) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.029873036131313122 \t\t Validation Loss: 0.14372066924205193\n",
            "\t\t Validation Loss Decreased(0.181568--->0.143721) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.024411657107742252 \t\t Validation Loss: 0.11166784912347794\n",
            "\t\t Validation Loss Decreased(0.143721--->0.111668) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.01985003616081903 \t\t Validation Loss: 0.08501850097225262\n",
            "\t\t Validation Loss Decreased(0.111668--->0.085019) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.016040120081900543 \t\t Validation Loss: 0.06317407838427104\n",
            "\t\t Validation Loss Decreased(0.085019--->0.063174) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.012890111661289592 \t\t Validation Loss: 0.04585028669008842\n",
            "\t\t Validation Loss Decreased(0.063174--->0.045850) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.01001452246831881 \t\t Validation Loss: 0.03246561996638775\n",
            "\t\t Validation Loss Decreased(0.045850--->0.032466) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.008159492221484715 \t\t Validation Loss: 0.022361008593669303\n",
            "\t\t Validation Loss Decreased(0.032466--->0.022361) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.006679009602128251 \t\t Validation Loss: 0.014931656062029876\n",
            "\t\t Validation Loss Decreased(0.022361--->0.014932) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.005575648081061002 \t\t Validation Loss: 0.009652588444833573\n",
            "\t\t Validation Loss Decreased(0.014932--->0.009653) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004561565397936548 \t\t Validation Loss: 0.006100696905587728\n",
            "\t\t Validation Loss Decreased(0.009653--->0.006101) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.003962021468937196 \t\t Validation Loss: 0.003813251870899246\n",
            "\t\t Validation Loss Decreased(0.006101--->0.003813) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0034771870532249276 \t\t Validation Loss: 0.0024377581609699587\n",
            "\t\t Validation Loss Decreased(0.003813--->0.002438) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0031174646369559137 \t\t Validation Loss: 0.0016888154115384589\n",
            "\t\t Validation Loss Decreased(0.002438--->0.001689) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002910249649446357 \t\t Validation Loss: 0.0013578164149433947\n",
            "\t\t Validation Loss Decreased(0.001689--->0.001358) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0027475548967895273 \t\t Validation Loss: 0.0012863768094505828\n",
            "\t\t Validation Loss Decreased(0.001358--->0.001286) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0026409916479039836 \t\t Validation Loss: 0.0013684447957740093\n",
            "Epoch 36 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0025146004312822747 \t\t Validation Loss: 0.0015244287832711751\n",
            "Epoch 37 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023712698823294123 \t\t Validation Loss: 0.0017075640073296828\n",
            "Epoch 38 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0024427350278553627 \t\t Validation Loss: 0.0018866635039627838\n",
            "Epoch 39 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0024127370136714467 \t\t Validation Loss: 0.002060771469796936\n",
            "Epoch 40 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023809821121520487 \t\t Validation Loss: 0.002226417662593751\n",
            "Epoch 41 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023780905758030713 \t\t Validation Loss: 0.0023466567867077314\n",
            "Epoch 42 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023500030896487973 \t\t Validation Loss: 0.0024318537191272927\n",
            "Epoch 43 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002356869036432456 \t\t Validation Loss: 0.0024810523967831754\n",
            "Epoch 44 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002208409134795688 \t\t Validation Loss: 0.002530264592048927\n",
            "Epoch 45 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023577146796899774 \t\t Validation Loss: 0.002547776183256736\n",
            "Epoch 46 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023610660164440807 \t\t Validation Loss: 0.0026030742581217335\n",
            "Epoch 47 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023072389759928795 \t\t Validation Loss: 0.002614381114164224\n",
            "Epoch 48 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023310924919536084 \t\t Validation Loss: 0.0025997189804911613\n",
            "Epoch 49 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022596731999058378 \t\t Validation Loss: 0.0025949242766588354\n",
            "Epoch 50 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023016044578980654 \t\t Validation Loss: 0.0026194942855419447\n",
            "Epoch 51 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002346945049499187 \t\t Validation Loss: 0.0026303983221833524\n",
            "Epoch 52 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022872575075755753 \t\t Validation Loss: 0.0026594792546417853\n",
            "Epoch 53 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002140552500841787 \t\t Validation Loss: 0.0027547573464779328\n",
            "Epoch 54 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022896270777375714 \t\t Validation Loss: 0.0027567023465123316\n",
            "Epoch 55 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002256284247630754 \t\t Validation Loss: 0.0027451277620947133\n",
            "Epoch 56 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002345537820302353 \t\t Validation Loss: 0.002709348345748507\n",
            "Epoch 57 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00233813385381615 \t\t Validation Loss: 0.002726150519895152\n",
            "Epoch 58 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002216373772417019 \t\t Validation Loss: 0.0026961913547263695\n",
            "Epoch 59 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002180445135407452 \t\t Validation Loss: 0.00266699564571564\n",
            "Epoch 60 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022419380241174352 \t\t Validation Loss: 0.0026684980302189407\n",
            "Epoch 61 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002329602900489762 \t\t Validation Loss: 0.002701975217160697\n",
            "Epoch 62 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002216185170943169 \t\t Validation Loss: 0.002761615628304963\n",
            "Epoch 63 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002241503165695011 \t\t Validation Loss: 0.002758380473376467\n",
            "Epoch 64 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022870245093882485 \t\t Validation Loss: 0.0027003135988846994\n",
            "Epoch 65 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002311963407083641 \t\t Validation Loss: 0.0026684108978280653\n",
            "Epoch 66 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023005626312255657 \t\t Validation Loss: 0.0027287599335138043\n",
            "Epoch 67 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0023812737270270954 \t\t Validation Loss: 0.0026772522295896825\n",
            "Epoch 68 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002186286941208449 \t\t Validation Loss: 0.0026784906692158142\n",
            "Epoch 69 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00214476915110985 \t\t Validation Loss: 0.002675075339189229\n",
            "Epoch 70 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022446699928165086 \t\t Validation Loss: 0.0027056989000322153\n",
            "Epoch 71 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022768222512929022 \t\t Validation Loss: 0.0027172706009318624\n",
            "Epoch 72 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00232174588018097 \t\t Validation Loss: 0.0027821656862775292\n",
            "Epoch 73 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022877407318446785 \t\t Validation Loss: 0.0027571633780518402\n",
            "Epoch 74 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002344266782086846 \t\t Validation Loss: 0.002774205587159556\n",
            "Epoch 75 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0022961616789846607 \t\t Validation Loss: 0.0028085251867126385\n",
            "Epoch 76 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0021717017091735192 \t\t Validation Loss: 0.0028412382145268987\n",
            "Epoch 77 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002240576391183847 \t\t Validation Loss: 0.0027424089449386182\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 94.83914974619289\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "The model has 545,658 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.716767108294408 \t\t Validation Loss: 1.3637334658549383\n",
            "\t\t Validation Loss Decreased(inf--->1.363733) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.6461926360262206 \t\t Validation Loss: 1.4072173191950872\n",
            "Epoch 3 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.586087734211941 \t\t Validation Loss: 1.4422579224293048\n",
            "Epoch 4 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.5317571539859716 \t\t Validation Loss: 1.4676268238287706\n",
            "Epoch 5 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.482248477499328 \t\t Validation Loss: 1.4819396229890676\n",
            "Epoch 6 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.439034645936477 \t\t Validation Loss: 1.4853535019434416\n",
            "Epoch 7 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.39911874846832174 \t\t Validation Loss: 1.476167183655959\n",
            "Epoch 8 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.36349094641470425 \t\t Validation Loss: 1.454496021454151\n",
            "Epoch 9 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.3323362231808337 \t\t Validation Loss: 1.421288224366995\n",
            "Epoch 10 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.30302167191749085 \t\t Validation Loss: 1.3768250025235689\n",
            "Epoch 11 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.27724151476915626 \t\t Validation Loss: 1.3224752086859484\n",
            "\t\t Validation Loss Decreased(1.363733--->1.322475) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.2539037592658723 \t\t Validation Loss: 1.2586626318784861\n",
            "\t\t Validation Loss Decreased(1.322475--->1.258663) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.23266211325755795 \t\t Validation Loss: 1.1862665093862093\n",
            "\t\t Validation Loss Decreased(1.258663--->1.186267) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.21054159960634 \t\t Validation Loss: 1.1072061153558583\n",
            "\t\t Validation Loss Decreased(1.186267--->1.107206) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.19246638780208053 \t\t Validation Loss: 1.023381118591015\n",
            "\t\t Validation Loss Decreased(1.107206--->1.023381) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.1733944341036919 \t\t Validation Loss: 0.9357925928556002\n",
            "\t\t Validation Loss Decreased(1.023381--->0.935793) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.15475754434796604 \t\t Validation Loss: 0.8466043701538672\n",
            "\t\t Validation Loss Decreased(0.935793--->0.846604) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.13939346493901433 \t\t Validation Loss: 0.7572944393524756\n",
            "\t\t Validation Loss Decreased(0.846604--->0.757294) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.1233973852232904 \t\t Validation Loss: 0.6687949735384721\n",
            "\t\t Validation Loss Decreased(0.757294--->0.668795) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.10946399869548308 \t\t Validation Loss: 0.583286328957631\n",
            "\t\t Validation Loss Decreased(0.668795--->0.583286) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.09663443229589108 \t\t Validation Loss: 0.5016128466679499\n",
            "\t\t Validation Loss Decreased(0.583286--->0.501613) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.08415480466509187 \t\t Validation Loss: 0.42474831067598784\n",
            "\t\t Validation Loss Decreased(0.501613--->0.424748) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.07285944530992089 \t\t Validation Loss: 0.3545693422739322\n",
            "\t\t Validation Loss Decreased(0.424748--->0.354569) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.06176028264736807 \t\t Validation Loss: 0.2913486338578738\n",
            "\t\t Validation Loss Decreased(0.354569--->0.291349) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.05283856575654165 \t\t Validation Loss: 0.23520846664905548\n",
            "\t\t Validation Loss Decreased(0.291349--->0.235208) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.044916504213737475 \t\t Validation Loss: 0.186161563373529\n",
            "\t\t Validation Loss Decreased(0.235208--->0.186162) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.038052079242628975 \t\t Validation Loss: 0.14441626920149878\n",
            "\t\t Validation Loss Decreased(0.186162--->0.144416) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.03160843762846009 \t\t Validation Loss: 0.11005911689538223\n",
            "\t\t Validation Loss Decreased(0.144416--->0.110059) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.02633167333850587 \t\t Validation Loss: 0.08213559366189517\n",
            "\t\t Validation Loss Decreased(0.110059--->0.082136) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.022255077505031147 \t\t Validation Loss: 0.0604277396431336\n",
            "\t\t Validation Loss Decreased(0.082136--->0.060428) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.019176548699269425 \t\t Validation Loss: 0.04378238091102013\n",
            "\t\t Validation Loss Decreased(0.060428--->0.043782) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.01625490371087516 \t\t Validation Loss: 0.031236505064253624\n",
            "\t\t Validation Loss Decreased(0.043782--->0.031237) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.013638139110864015 \t\t Validation Loss: 0.022044717190930478\n",
            "\t\t Validation Loss Decreased(0.031237--->0.022045) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.011842056938975647 \t\t Validation Loss: 0.015598839972741328\n",
            "\t\t Validation Loss Decreased(0.022045--->0.015599) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.010511581949587609 \t\t Validation Loss: 0.01118917426524254\n",
            "\t\t Validation Loss Decreased(0.015599--->0.011189) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00941931618679617 \t\t Validation Loss: 0.00818738672667398\n",
            "\t\t Validation Loss Decreased(0.011189--->0.008187) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.008199732738068781 \t\t Validation Loss: 0.006214166436201105\n",
            "\t\t Validation Loss Decreased(0.008187--->0.006214) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0076676112587085446 \t\t Validation Loss: 0.00489047565497458\n",
            "\t\t Validation Loss Decreased(0.006214--->0.004890) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.006702313986229333 \t\t Validation Loss: 0.004031323899443333\n",
            "\t\t Validation Loss Decreased(0.004890--->0.004031) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0063917043656614185 \t\t Validation Loss: 0.0034247874461400965\n",
            "\t\t Validation Loss Decreased(0.004031--->0.003425) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.005692747619783355 \t\t Validation Loss: 0.0030085348004761795\n",
            "\t\t Validation Loss Decreased(0.003425--->0.003009) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.005438990072926154 \t\t Validation Loss: 0.0027230386084948596\n",
            "\t\t Validation Loss Decreased(0.003009--->0.002723) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.005298195246330186 \t\t Validation Loss: 0.002515338629914018\n",
            "\t\t Validation Loss Decreased(0.002723--->0.002515) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004565237876581582 \t\t Validation Loss: 0.0023651212149371323\n",
            "\t\t Validation Loss Decreased(0.002515--->0.002365) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004633167906462945 \t\t Validation Loss: 0.002234212060172397\n",
            "\t\t Validation Loss Decreased(0.002365--->0.002234) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.004202131052954575 \t\t Validation Loss: 0.0021425413164811637\n",
            "\t\t Validation Loss Decreased(0.002234--->0.002143) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0037563848498000487 \t\t Validation Loss: 0.002067378390795336\n",
            "\t\t Validation Loss Decreased(0.002143--->0.002067) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0036401861564356935 \t\t Validation Loss: 0.001980957848270639\n",
            "\t\t Validation Loss Decreased(0.002067--->0.001981) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0036210687862507796 \t\t Validation Loss: 0.0019197495287069334\n",
            "\t\t Validation Loss Decreased(0.001981--->0.001920) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0033044894162968202 \t\t Validation Loss: 0.001869598296112739\n",
            "\t\t Validation Loss Decreased(0.001920--->0.001870) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0032967517649553513 \t\t Validation Loss: 0.0018419629374805551\n",
            "\t\t Validation Loss Decreased(0.001870--->0.001842) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00334511003362619 \t\t Validation Loss: 0.0017943174103633142\n",
            "\t\t Validation Loss Decreased(0.001842--->0.001794) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0031308534026548668 \t\t Validation Loss: 0.0017532627008712063\n",
            "\t\t Validation Loss Decreased(0.001794--->0.001753) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.003099604937675837 \t\t Validation Loss: 0.0017137222950203489\n",
            "\t\t Validation Loss Decreased(0.001753--->0.001714) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002988201444905654 \t\t Validation Loss: 0.00166893252529777\n",
            "\t\t Validation Loss Decreased(0.001714--->0.001669) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0028925497868299687 \t\t Validation Loss: 0.0016410224041185128\n",
            "\t\t Validation Loss Decreased(0.001669--->0.001641) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0027621869104788514 \t\t Validation Loss: 0.0016397802571121317\n",
            "\t\t Validation Loss Decreased(0.001641--->0.001640) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002854402985650341 \t\t Validation Loss: 0.001644217839034704\n",
            "Epoch 59 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0027744009118602685 \t\t Validation Loss: 0.0016352181107952045\n",
            "\t\t Validation Loss Decreased(0.001640--->0.001635) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0028676272974022337 \t\t Validation Loss: 0.0016365510169774867\n",
            "Epoch 61 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002765003789996577 \t\t Validation Loss: 0.001635571476072073\n",
            "Epoch 62 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0024447482239731864 \t\t Validation Loss: 0.0016294682461123627\n",
            "\t\t Validation Loss Decreased(0.001635--->0.001629) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002651388060012077 \t\t Validation Loss: 0.001643609385400151\n",
            "Epoch 64 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002543972122382272 \t\t Validation Loss: 0.0016584629327273713\n",
            "Epoch 65 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0025849715142661858 \t\t Validation Loss: 0.0016478386868794377\n",
            "Epoch 66 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0024148926573075557 \t\t Validation Loss: 0.0016445187793578953\n",
            "Epoch 67 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0025048741561083778 \t\t Validation Loss: 0.001643694279034837\n",
            "Epoch 68 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0024997463301956856 \t\t Validation Loss: 0.0016587917567588962\n",
            "Epoch 69 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0026699248858652004 \t\t Validation Loss: 0.0016561208624177827\n",
            "Epoch 70 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00263176768911197 \t\t Validation Loss: 0.0016605590980571623\n",
            "Epoch 71 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.00257782730890589 \t\t Validation Loss: 0.0016661853475782734\n",
            "Epoch 72 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002561293644641803 \t\t Validation Loss: 0.0016820175358309196\n",
            "Epoch 73 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0026165219951999955 \t\t Validation Loss: 0.0016804026045764869\n",
            "Epoch 74 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002463764712415837 \t\t Validation Loss: 0.001689864651640304\n",
            "Epoch 75 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.0025840623688103783 \t\t Validation Loss: 0.0016920144598071391\n",
            "Epoch 76 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002734244418821323 \t\t Validation Loss: 0.0016998217593377025\n",
            "Epoch 77 \t\t Epoch time: 0m 6s\n",
            "\t\t Training Loss: 0.002616255887079279 \t\t Validation Loss: 0.0017090570517421628\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 182.1770701142132\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpqlF_2jxTwb",
        "outputId": "2b499ade-f2e5-4e2e-d4f2-66401e21de8a"
      },
      "source": [
        "model_num = []\n",
        "model_tmp = LSTM_92_num()\n",
        "model_num.append(model_tmp)\n",
        "model_tmp = LSTM_36_num()\n",
        "model_num.append(model_tmp)\n",
        "\n",
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "model_news = LSTM_news(len(vocab),PAD_ID)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "e = 75\n",
        "for j in range(2):\n",
        "  for k in range(e):\n",
        "    print(f'{k}. epoch')\n",
        "    if j == 0:\n",
        "        name = \"LSTM_92_LSTM_news\"\n",
        "        model_num_tmp = model_num[j]\n",
        "        model = Complex_1(model_num_tmp, model_news)\n",
        "    else:\n",
        "        name = \"LSTM_36_LSTM_news\"\n",
        "        model_num_tmp = model_num[j]\n",
        "        model = Complex_1(model_num_tmp, model_news)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(f\"drive/MyDrive/complex/complex_models/{k}_model_['Complex_1_{name}'].pt\",map_location=torch.device('cpu')))\n",
        "\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_X_num)):\n",
        "            model.eval()\n",
        "            predict = model(test_X_num[i].reshape(1,-1).to(\"cpu\"), test_X_news[i].reshape(1,-1).to(\"cpu\"))        \n",
        "            predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "    real_test_array = []\n",
        "    real_trend_array = test[\"Trend target\"]  \n",
        "    for y in test_Y_num:\n",
        "      real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "    # calculate score\n",
        "    score = calc_score(predict_test_array,real_test_array)    \n",
        "    print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "    #check the trend -> create an array with the real and with the predicted trend\n",
        "    #if the current value is bigger than before -> 1\n",
        "    #otherwise (same or smaller) -> 0\n",
        "    predicted_trend_array = []\n",
        "    for element in range(len(predict_test_array)):\n",
        "        real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "        if real_today_close > predict_test_array[element].values:\n",
        "            predicted_trend_array.append(0)\n",
        "        else:\n",
        "            predicted_trend_array.append(1)\n",
        "\n",
        "    #check the number of differences\n",
        "    trend_diff_array = []\n",
        "    for element in range(len(real_trend_array)):\n",
        "        if real_trend_array[element] != predicted_trend_array[element]:\n",
        "          trend_diff_array.append(element)\n",
        "\n",
        "    #percentage of good predict\n",
        "    acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "    print(f\"Accuracy: {acc}\\n\")  \n",
        "    #visualize it\n",
        "    f2 = plt.figure(figsize=(24,12))\n",
        "    plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "    plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "            label = \"Real\")\n",
        "    plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "            label = \"Predicted\")\n",
        "    plt.xlabel(\"Date\",fontsize = 18)\n",
        "    plt.legend(fontsize = 18)\n",
        "    f2.set_size_inches(12,6)\n",
        "    plt_pred = f2\n",
        "    plt.close()\n",
        "\n",
        "    plt_pred.savefig('drive/MyDrive/complex/complex_models/'+str(k)+\"_log_pred_\" + str('Complex_1_') + str(name) + \"_.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. epoch\n",
            "\n",
            "Score: 25574.644670050762\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 27793.08375634518\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 29663.294416243654\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 31152.730964467006\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 32177.598984771572\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 32708.619289340102\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 32668.479695431473\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 32039.195431472082\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 30820.58121827411\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 29070.104060913705\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 26898.040609137057\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 24416.170050761422\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 21762.096446700507\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 19055.821065989847\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 16413.10152284264\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 13907.757614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 11589.831218274112\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 9508.553299492385\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 7664.3546954314725\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 6072.840736040609\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 4726.906091370558\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 3611.340418781726\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 2698.7239847715737\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 1975.1194479695432\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 1416.0529822335025\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 992.2151808375635\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 679.7514276649746\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 456.3914181472081\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 305.26518876903555\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 206.8364530456853\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 147.29218551713197\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 114.48846962246193\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 99.0534581218274\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 94.8391299175127\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 97.36613657994924\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 103.33672073286802\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 110.6816703680203\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 117.7089546319797\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 125.0064938927665\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 132.06034858819797\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 136.88837444479697\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 140.6542274746193\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 143.13392290609136\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 145.22760350571065\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 146.09361119923858\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 149.06023953045684\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 149.26202609454316\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 148.30498493020303\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 148.6581238102792\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 150.1613558851523\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 150.3281844860406\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 152.0826062817259\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 156.1225214149746\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 156.23413705583755\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 155.80870280774113\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 154.2090736040609\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 155.16711611675126\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 153.28421438769035\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 152.3704988895939\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 152.44740442576142\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 154.1549016497462\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 156.25920050761422\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 156.29584390862945\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 153.4076280932741\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 152.8072652284264\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 155.47487706218274\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 152.40312698286803\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 152.97028672271574\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 152.72740521890864\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 154.5123433534264\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 155.02634240164974\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 157.87290807423858\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 156.47125832804568\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 157.7076062817259\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 159.15595256979697\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 66198.47208121828\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 67357.59898477157\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 68153.68020304569\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 68553.27411167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 68510.58375634518\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 68039.70558375634\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 67084.3807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 65658.24873096446\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 63801.64467005076\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 61534.37055837563\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 58908.766497461926\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 55945.35532994924\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 52675.79695431472\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 49176.289340101524\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 45514.395939086295\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 41731.03553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 37904.81472081218\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 34088.63959390863\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 30316.401015228428\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 26668.880710659898\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 23180.34263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 19887.229695431473\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 16857.866751269037\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 14110.340101522843\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 11643.819796954314\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 9467.642766497462\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 7585.959390862944\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 6005.006345177665\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 4692.249365482234\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 3638.355964467005\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 2802.665609137056\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 2149.0466370558374\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 1645.8569162436547\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 1268.8761897208121\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 991.9262373096446\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 787.9596288071066\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 640.9604616116751\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 532.2460342639594\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 454.29338515228426\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 392.93131345177665\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 342.71117544416245\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 305.95603981598987\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 273.99704552664974\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 255.57334628807106\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 244.27494447969542\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 229.9779901649746\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 218.7005274428934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 211.99522128807106\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 204.63880076142132\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 198.89381741751268\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 192.72156567258884\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 190.1477831535533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 188.24300047588832\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 187.2115918464467\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 188.9147168464467\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 189.28743654822335\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 185.93567576142132\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 183.60761024746193\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 183.75890307741116\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 182.4238380393401\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 181.870994606599\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 182.1771296002538\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 180.15747937817258\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 179.65081694162436\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 182.81668385152284\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 184.2602712563452\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 184.83092084390864\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 183.08867385786803\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 184.3731162753807\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 184.6563293147208\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 184.98974857233503\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 183.86417354060913\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 183.80327966370558\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 182.18210659898477\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 182.764256821066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1gQRlcVq5Yw",
        "outputId": "77e32623-8a69-494b-85bd-9495f90dc6ba"
      },
      "source": [
        "# train with complex 2 and 3 the LSTM news\n",
        "complex_name_tmp = [\"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"LSTM_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "model_news = LSTM_news(len(vocab),PAD_ID)\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/26_model_LSTM_pilot.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,558,035 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.7777004946665084 \t\t Validation Loss: 0.9232713671830984\n",
            "\t\t Validation Loss Decreased(inf--->0.923271) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.5972942649386823 \t\t Validation Loss: 1.107208673770611\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.4517491597782921 \t\t Validation Loss: 1.2916871355130122\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.346736437779524 \t\t Validation Loss: 1.418138311459468\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2834654058380103 \t\t Validation Loss: 1.4520949813035817\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2453059133998043 \t\t Validation Loss: 1.4119866765462434\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.22140789150285559 \t\t Validation Loss: 1.3343511407191937\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2015156445496187 \t\t Validation Loss: 1.2397804856300354\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.18420157365727466 \t\t Validation Loss: 1.1361833489858186\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.1665712563396507 \t\t Validation Loss: 1.0290826604916499\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.14958230207195958 \t\t Validation Loss: 0.9179943112226633\n",
            "\t\t Validation Loss Decreased(0.923271--->0.917994) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.13240997226455728 \t\t Validation Loss: 0.8040344669268682\n",
            "\t\t Validation Loss Decreased(0.917994--->0.804034) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.11501331509065789 \t\t Validation Loss: 0.6886713527716123\n",
            "\t\t Validation Loss Decreased(0.804034--->0.688671) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.09801075416239532 \t\t Validation Loss: 0.5735284479764792\n",
            "\t\t Validation Loss Decreased(0.688671--->0.573528) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.08052915671362064 \t\t Validation Loss: 0.46126436728697556\n",
            "\t\t Validation Loss Decreased(0.573528--->0.461264) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.06418258809434199 \t\t Validation Loss: 0.3562871767924382\n",
            "\t\t Validation Loss Decreased(0.461264--->0.356287) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.049318589392822944 \t\t Validation Loss: 0.26174688797730666\n",
            "\t\t Validation Loss Decreased(0.356287--->0.261747) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.03603893963972459 \t\t Validation Loss: 0.18090716004371643\n",
            "\t\t Validation Loss Decreased(0.261747--->0.180907) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.024903802594446856 \t\t Validation Loss: 0.11679107466569313\n",
            "\t\t Validation Loss Decreased(0.180907--->0.116791) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0163560481301534 \t\t Validation Loss: 0.06993939269047517\n",
            "\t\t Validation Loss Decreased(0.116791--->0.069939) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.010301241651177406 \t\t Validation Loss: 0.038862552493810654\n",
            "\t\t Validation Loss Decreased(0.069939--->0.038863) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.006424552108148566 \t\t Validation Loss: 0.020280221118949927\n",
            "\t\t Validation Loss Decreased(0.038863--->0.020280) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004201058789181548 \t\t Validation Loss: 0.010289630100417595\n",
            "\t\t Validation Loss Decreased(0.020280--->0.010290) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003072013957363025 \t\t Validation Loss: 0.005415285436006693\n",
            "\t\t Validation Loss Decreased(0.010290--->0.005415) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025616829484902525 \t\t Validation Loss: 0.0032092450419440866\n",
            "\t\t Validation Loss Decreased(0.005415--->0.003209) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023541817169422537 \t\t Validation Loss: 0.002258220974069375\n",
            "\t\t Validation Loss Decreased(0.003209--->0.002258) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 10s\n",
            "\t\t Training Loss: 0.0022770663057894423 \t\t Validation Loss: 0.0018574423962630904\n",
            "\t\t Validation Loss Decreased(0.002258--->0.001857) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022597112268133946 \t\t Validation Loss: 0.0016882025878518247\n",
            "\t\t Validation Loss Decreased(0.001857--->0.001688) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022493342788751564 \t\t Validation Loss: 0.001614359959673423\n",
            "\t\t Validation Loss Decreased(0.001688--->0.001614) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022499662921512246 \t\t Validation Loss: 0.0015783858969091224\n",
            "\t\t Validation Loss Decreased(0.001614--->0.001578) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022428653791320284 \t\t Validation Loss: 0.00156689928441595\n",
            "\t\t Validation Loss Decreased(0.001578--->0.001567) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022323632422469658 \t\t Validation Loss: 0.0015621116237105946\n",
            "\t\t Validation Loss Decreased(0.001567--->0.001562) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022366601655989683 \t\t Validation Loss: 0.0015630996293531586\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002243239726757983 \t\t Validation Loss: 0.0015702937402798294\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022344645686963624 \t\t Validation Loss: 0.0015704539076138574\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022558045079554053 \t\t Validation Loss: 0.0015738880924450664\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002245677474609299 \t\t Validation Loss: 0.0015703136495386178\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022367902408429495 \t\t Validation Loss: 0.0015709203860471742\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022340149868820868 \t\t Validation Loss: 0.0015750879000944013\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002231901677520442 \t\t Validation Loss: 0.0015841141128196167\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022355950928318337 \t\t Validation Loss: 0.0015882153328185757\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022495602889968133 \t\t Validation Loss: 0.0015879640301976067\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022352659379484482 \t\t Validation Loss: 0.0015903900269992077\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022660278852059935 \t\t Validation Loss: 0.0015876559168995859\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022418067784275154 \t\t Validation Loss: 0.0015846153479427672\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022357094488610085 \t\t Validation Loss: 0.0015767675537902575\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022411992029299508 \t\t Validation Loss: 0.0015734954258033002\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022417930555781603 \t\t Validation Loss: 0.0015762052453982716\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022489010628526777 \t\t Validation Loss: 0.0015727439738559322\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00223086287685972 \t\t Validation Loss: 0.001564584347831372\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022603390436317468 \t\t Validation Loss: 0.0015685529864500635\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002253818204563514 \t\t Validation Loss: 0.0015644052092773984\n",
            "Epoch 53 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022489104924937457 \t\t Validation Loss: 0.0015570136512486408\n",
            "\t\t Validation Loss Decreased(0.001562--->0.001557) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022416119392948677 \t\t Validation Loss: 0.0015556456794281704\n",
            "\t\t Validation Loss Decreased(0.001557--->0.001556) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002246656360708781 \t\t Validation Loss: 0.0015517391976363098\n",
            "\t\t Validation Loss Decreased(0.001556--->0.001552) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022530683545974664 \t\t Validation Loss: 0.0015538930646681155\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022562888397432463 \t\t Validation Loss: 0.0015551133860404103\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022545425060859605 \t\t Validation Loss: 0.0015463035289520542\n",
            "\t\t Validation Loss Decreased(0.001552--->0.001546) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002256361867546582 \t\t Validation Loss: 0.0015325832914990874\n",
            "\t\t Validation Loss Decreased(0.001546--->0.001533) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022562676002397327 \t\t Validation Loss: 0.0015257579395368409\n",
            "\t\t Validation Loss Decreased(0.001533--->0.001526) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022502383256545037 \t\t Validation Loss: 0.0015368861965655994\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002266884845920612 \t\t Validation Loss: 0.0015410634133821498\n",
            "Epoch 63 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002262188180670696 \t\t Validation Loss: 0.0015260813972697808\n",
            "Epoch 64 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002254096498536701 \t\t Validation Loss: 0.0015239882878421878\n",
            "\t\t Validation Loss Decreased(0.001526--->0.001524) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022552864644928157 \t\t Validation Loss: 0.0015159913119322692\n",
            "\t\t Validation Loss Decreased(0.001524--->0.001516) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002270653736903458 \t\t Validation Loss: 0.001517023761362697\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022618005396735324 \t\t Validation Loss: 0.0015188825961488944\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022544980700429833 \t\t Validation Loss: 0.0015104514627287595\n",
            "\t\t Validation Loss Decreased(0.001516--->0.001510) \t Saving The Model\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022624505068666325 \t\t Validation Loss: 0.0015051755337760998\n",
            "\t\t Validation Loss Decreased(0.001510--->0.001505) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002264445848964356 \t\t Validation Loss: 0.001501462657380706\n",
            "\t\t Validation Loss Decreased(0.001505--->0.001501) \t Saving The Model\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002266382914967835 \t\t Validation Loss: 0.0015000021600057012\n",
            "\t\t Validation Loss Decreased(0.001501--->0.001500) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022621541362683717 \t\t Validation Loss: 0.0014843930458972375\n",
            "\t\t Validation Loss Decreased(0.001500--->0.001484) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022523385941403337 \t\t Validation Loss: 0.0014797879512815808\n",
            "\t\t Validation Loss Decreased(0.001484--->0.001480) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002283735584618675 \t\t Validation Loss: 0.001486559053703856\n",
            "Epoch 75 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002276578889782163 \t\t Validation Loss: 0.0014769850956956642\n",
            "\t\t Validation Loss Decreased(0.001480--->0.001477) \t Saving The Model\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002275972201558132 \t\t Validation Loss: 0.0014689813744133483\n",
            "\t\t Validation Loss Decreased(0.001477--->0.001469) \t Saving The Model\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002278623216772905 \t\t Validation Loss: 0.0014729805608602385\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002291835952279586 \t\t Validation Loss: 0.0014608587035371994\n",
            "\t\t Validation Loss Decreased(0.001469--->0.001461) \t Saving The Model\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002269524577501658 \t\t Validation Loss: 0.0014520060235204606\n",
            "\t\t Validation Loss Decreased(0.001461--->0.001452) \t Saving The Model\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002281223179149517 \t\t Validation Loss: 0.0014577536136270142\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002268561631648776 \t\t Validation Loss: 0.0014528261023000455\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002286226612931067 \t\t Validation Loss: 0.0014516105987310696\n",
            "\t\t Validation Loss Decreased(0.001452--->0.001452) \t Saving The Model\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022918157432089887 \t\t Validation Loss: 0.0014504315376353378\n",
            "\t\t Validation Loss Decreased(0.001452--->0.001450) \t Saving The Model\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022843885188293016 \t\t Validation Loss: 0.0014451937526106262\n",
            "\t\t Validation Loss Decreased(0.001450--->0.001445) \t Saving The Model\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022939825233864925 \t\t Validation Loss: 0.0014363424807715302\n",
            "\t\t Validation Loss Decreased(0.001445--->0.001436) \t Saving The Model\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002284350650812927 \t\t Validation Loss: 0.0014317654959785824\n",
            "\t\t Validation Loss Decreased(0.001436--->0.001432) \t Saving The Model\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022881674734447654 \t\t Validation Loss: 0.0014317786509099489\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022765295823872394 \t\t Validation Loss: 0.0014301323860238951\n",
            "\t\t Validation Loss Decreased(0.001432--->0.001430) \t Saving The Model\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002298629797862943 \t\t Validation Loss: 0.001432832120022235\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002279845913526023 \t\t Validation Loss: 0.001428629153031999\n",
            "\t\t Validation Loss Decreased(0.001430--->0.001429) \t Saving The Model\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002282031035409435 \t\t Validation Loss: 0.0014286567210757101\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002290495335457947 \t\t Validation Loss: 0.0014234144585386205\n",
            "\t\t Validation Loss Decreased(0.001429--->0.001423) \t Saving The Model\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002291466108668042 \t\t Validation Loss: 0.0014168575244884079\n",
            "\t\t Validation Loss Decreased(0.001423--->0.001417) \t Saving The Model\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023012549720226308 \t\t Validation Loss: 0.001414534051526481\n",
            "\t\t Validation Loss Decreased(0.001417--->0.001415) \t Saving The Model\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022916239788566993 \t\t Validation Loss: 0.0014147163623979746\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023024815789147004 \t\t Validation Loss: 0.0014134851875356757\n",
            "\t\t Validation Loss Decreased(0.001415--->0.001413) \t Saving The Model\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002281403438049696 \t\t Validation Loss: 0.0014108035852237104\n",
            "\t\t Validation Loss Decreased(0.001413--->0.001411) \t Saving The Model\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023010727096776907 \t\t Validation Loss: 0.0014112953750344997\n",
            "Epoch 99 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.0022800649434479106 \t\t Validation Loss: 0.0014175469785606344\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023075451018804735 \t\t Validation Loss: 0.0014073113404894965\n",
            "\t\t Validation Loss Decreased(0.001411--->0.001407) \t Saving The Model\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002297440729406033 \t\t Validation Loss: 0.0014046299239942948\n",
            "\t\t Validation Loss Decreased(0.001407--->0.001405) \t Saving The Model\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002291291801107896 \t\t Validation Loss: 0.0014029557306247835\n",
            "\t\t Validation Loss Decreased(0.001405--->0.001403) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 83.16034462246193\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "The model has 2,167,162 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.8658168195067225 \t\t Validation Loss: 0.8695102907144107\n",
            "\t\t Validation Loss Decreased(inf--->0.869510) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.698670084468668 \t\t Validation Loss: 0.9602040442136618\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.5429577495449701 \t\t Validation Loss: 1.0499241306231573\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.4022235616135436 \t\t Validation Loss: 1.1136856996096098\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.29164791138012064 \t\t Validation Loss: 1.1193150190206675\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.21582188453474963 \t\t Validation Loss: 1.053073071516477\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.1674785153512415 \t\t Validation Loss: 0.9345408036158636\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.13472710948003255 \t\t Validation Loss: 0.7937609048990103\n",
            "\t\t Validation Loss Decreased(0.869510--->0.793761) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.10899202123510877 \t\t Validation Loss: 0.6509892184000748\n",
            "\t\t Validation Loss Decreased(0.793761--->0.650989) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.08707105360195241 \t\t Validation Loss: 0.5160736785485194\n",
            "\t\t Validation Loss Decreased(0.650989--->0.516074) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.06881805027306483 \t\t Validation Loss: 0.39392824127123904\n",
            "\t\t Validation Loss Decreased(0.516074--->0.393928) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.053263800451532006 \t\t Validation Loss: 0.2873137295246124\n",
            "\t\t Validation Loss Decreased(0.393928--->0.287314) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.03828037718336123 \t\t Validation Loss: 0.19860849242943984\n",
            "\t\t Validation Loss Decreased(0.287314--->0.198608) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.028050119762082357 \t\t Validation Loss: 0.12880646322782224\n",
            "\t\t Validation Loss Decreased(0.198608--->0.128806) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.01868516710788213 \t\t Validation Loss: 0.07765938685490535\n",
            "\t\t Validation Loss Decreased(0.128806--->0.077659) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.012535595067625714 \t\t Validation Loss: 0.04310035805862684\n",
            "\t\t Validation Loss Decreased(0.077659--->0.043100) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.008524609836623879 \t\t Validation Loss: 0.021633921692577694\n",
            "\t\t Validation Loss Decreased(0.043100--->0.021634) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005797337715488834 \t\t Validation Loss: 0.010067478395425357\n",
            "\t\t Validation Loss Decreased(0.021634--->0.010067) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004133138056794131 \t\t Validation Loss: 0.004527996652401411\n",
            "\t\t Validation Loss Decreased(0.010067--->0.004528) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033961610059958657 \t\t Validation Loss: 0.002295889461842867\n",
            "\t\t Validation Loss Decreased(0.004528--->0.002296) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002888582947324156 \t\t Validation Loss: 0.0016757163998241036\n",
            "\t\t Validation Loss Decreased(0.002296--->0.001676) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026504785008099234 \t\t Validation Loss: 0.001713669106650811\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002358263243631636 \t\t Validation Loss: 0.001916820308766686\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023972252934441173 \t\t Validation Loss: 0.0021719242392394403\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002548447467160185 \t\t Validation Loss: 0.002420071315450164\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023201931381245725 \t\t Validation Loss: 0.0025529931971015264\n",
            "Epoch 27 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002472693633093071 \t\t Validation Loss: 0.0025472034333058847\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025518271156486022 \t\t Validation Loss: 0.0026990755174595574\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022858030326362395 \t\t Validation Loss: 0.0027038289207177092\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024788411006935545 \t\t Validation Loss: 0.002886459643307787\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002530982011513835 \t\t Validation Loss: 0.0028671863614223325\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002468587855047018 \t\t Validation Loss: 0.00277607159044307\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023302992969449305 \t\t Validation Loss: 0.0027283790628783978\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025783835067036184 \t\t Validation Loss: 0.0027690272545441985\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002428965348863622 \t\t Validation Loss: 0.0028815381631899914\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002413127788443219 \t\t Validation Loss: 0.002786058614639422\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024043474860829177 \t\t Validation Loss: 0.00275859782525983\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025304237441977837 \t\t Validation Loss: 0.002815198036842048\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024442522632386033 \t\t Validation Loss: 0.0028989163200514247\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024864113688267565 \t\t Validation Loss: 0.0029729508633653703\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023743982184552463 \t\t Validation Loss: 0.0028512670497338358\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002486028437930587 \t\t Validation Loss: 0.0028641452574027846\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002507502615854547 \t\t Validation Loss: 0.0030311525109797143\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025304495294049783 \t\t Validation Loss: 0.0029615822176520643\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002447613568447933 \t\t Validation Loss: 0.002872989804018289\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023703134683199932 \t\t Validation Loss: 0.002805389965382906\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025749990741747455 \t\t Validation Loss: 0.00267054141463282\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024879540763543665 \t\t Validation Loss: 0.0028969445803131047\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002417385921895353 \t\t Validation Loss: 0.0033963467800416625\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023780485640305118 \t\t Validation Loss: 0.003062860995459442\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002314913188849853 \t\t Validation Loss: 0.0028278650984597895\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023715414737691047 \t\t Validation Loss: 0.002765932290528256\n",
            "Epoch 53 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002295526784429377 \t\t Validation Loss: 0.0029738853336311877\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025383619767789904 \t\t Validation Loss: 0.0031132214263869594\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023316318433530426 \t\t Validation Loss: 0.003073846531781153\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002310198498889804 \t\t Validation Loss: 0.003015608679001721\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002469991514698376 \t\t Validation Loss: 0.0028158392927322825\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023283599105009154 \t\t Validation Loss: 0.0031814441407242646\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023718685685732476 \t\t Validation Loss: 0.0030118075038234773\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002478762378765119 \t\t Validation Loss: 0.0031673276904397286\n",
            "Epoch 61 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025140988920513236 \t\t Validation Loss: 0.0031383607778340005\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024798967044901204 \t\t Validation Loss: 0.0030066313159365496\n",
            "Epoch 63 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024750285114891624 \t\t Validation Loss: 0.0030958637630996797\n",
            "Epoch 64 \t\t Epoch time: 0m 9s\n",
            "\t\t Training Loss: 0.002447766067799986 \t\t Validation Loss: 0.003305667921757469\n",
            "Epoch 65 \t\t Epoch time: 0m 10s\n",
            "\t\t Training Loss: 0.002547125035513041 \t\t Validation Loss: 0.003152251579404737\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024762550271367908 \t\t Validation Loss: 0.0032226881280971263\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002445978982487341 \t\t Validation Loss: 0.0031752584036439657\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025673585965902217 \t\t Validation Loss: 0.0033117795783954742\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024711126744797504 \t\t Validation Loss: 0.003406029221458504\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024037888443500205 \t\t Validation Loss: 0.0030811945549570597\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00243138154100224 \t\t Validation Loss: 0.0035026856483175205\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002420721748667593 \t\t Validation Loss: 0.0036173341079400135\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025323757231613068 \t\t Validation Loss: 0.0030812536402103994\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024750246304540416 \t\t Validation Loss: 0.0032497390639036894\n",
            "Epoch 75 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023978244563615 \t\t Validation Loss: 0.0034384133536010408\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024152003650276645 \t\t Validation Loss: 0.00311634368979587\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002638729749329237 \t\t Validation Loss: 0.0034447687660129024\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002519171644077712 \t\t Validation Loss: 0.003635985696186813\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002377843824089379 \t\t Validation Loss: 0.0032243649654376963\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002376055991516222 \t\t Validation Loss: 0.0034828026187964357\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00250029334219525 \t\t Validation Loss: 0.003522352190115131\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002621629848019094 \t\t Validation Loss: 0.003558488177637068\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025528398817530958 \t\t Validation Loss: 0.0034800968401563857\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002545636008820824 \t\t Validation Loss: 0.0036214182171254205\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002551934710150031 \t\t Validation Loss: 0.003229689439579558\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024217982317094465 \t\t Validation Loss: 0.003878674890774374\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002573281937165538 \t\t Validation Loss: 0.0036550875520333648\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002496311908298348 \t\t Validation Loss: 0.0036473988841932556\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002415882447904379 \t\t Validation Loss: 0.0038203414296731353\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002465037119297965 \t\t Validation Loss: 0.0038549311082953443\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002648147342198 \t\t Validation Loss: 0.003623202658043458\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002336052351794835 \t\t Validation Loss: 0.0038043164128724197\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002454160017313788 \t\t Validation Loss: 0.0037792200365891824\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002614627727559088 \t\t Validation Loss: 0.0036551692934993366\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025377983630106254 \t\t Validation Loss: 0.003723621234082832\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002536848680638532 \t\t Validation Loss: 0.0038067883937261426\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026387414561483908 \t\t Validation Loss: 0.0037372447908497774\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00255886186386232 \t\t Validation Loss: 0.0037511528845733176\n",
            "Epoch 99 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002597367123549653 \t\t Validation Loss: 0.004080215495867798\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025254260194865433 \t\t Validation Loss: 0.003706512121985165\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002656233893405344 \t\t Validation Loss: 0.003954789637086483\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002433513508278977 \t\t Validation Loss: 0.0035650442408111235\n",
            "Epoch 103 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002552376050074157 \t\t Validation Loss: 0.0037896720549234976\n",
            "Epoch 104 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002435575769522001 \t\t Validation Loss: 0.0040351663931058\n",
            "Epoch 105 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024689888185192204 \t\t Validation Loss: 0.0037795851956336545\n",
            "Epoch 106 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025115120665145083 \t\t Validation Loss: 0.00399281364829781\n",
            "Epoch 107 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002637154170991601 \t\t Validation Loss: 0.004031275193063686\n",
            "Epoch 108 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002544383759094352 \t\t Validation Loss: 0.004079855844163551\n",
            "Epoch 109 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002565816538785962 \t\t Validation Loss: 0.0036161812022328377\n",
            "Epoch 110 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025438553719730997 \t\t Validation Loss: 0.004247636982024862\n",
            "Epoch 111 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002334261860547436 \t\t Validation Loss: 0.0034594456172691514\n",
            "Epoch 112 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026180055055364565 \t\t Validation Loss: 0.004071317505664551\n",
            "Epoch 113 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024729311824902085 \t\t Validation Loss: 0.0033913060861568037\n",
            "Epoch 114 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026276105215672302 \t\t Validation Loss: 0.003636563357968743\n",
            "Epoch 115 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00247959145879675 \t\t Validation Loss: 0.0036896028054448273\n",
            "Epoch 116 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002562999745793137 \t\t Validation Loss: 0.0038925179531081365\n",
            "Epoch 117 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025371965068090403 \t\t Validation Loss: 0.0031584430208358052\n",
            "Epoch 118 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00253470752677704 \t\t Validation Loss: 0.003295882502033447\n",
            "Epoch 119 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002424914836631836 \t\t Validation Loss: 0.0035086074718632377\n",
            "Epoch 120 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002537081845582941 \t\t Validation Loss: 0.003664484955012225\n",
            "Epoch 121 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002576444938313216 \t\t Validation Loss: 0.0034646946005523205\n",
            "Epoch 122 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026059893268236033 \t\t Validation Loss: 0.0036409892714940584\n",
            "Epoch 123 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002631417060321247 \t\t Validation Loss: 0.003103652982435261\n",
            "Epoch 124 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002619192181973139 \t\t Validation Loss: 0.003371164256420273\n",
            "Epoch 125 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002716787428456686 \t\t Validation Loss: 0.0029930411524569187\n",
            "Epoch 126 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002648687123862171 \t\t Validation Loss: 0.0036854154179589106\n",
            "Epoch 127 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026027852279209606 \t\t Validation Loss: 0.0031100668353386796\n",
            "Epoch 128 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024737840222597523 \t\t Validation Loss: 0.003093956530882189\n",
            "Epoch 129 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027105801092221628 \t\t Validation Loss: 0.0031832234459356046\n",
            "Epoch 130 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026986609902107032 \t\t Validation Loss: 0.0030971395138364574\n",
            "Epoch 131 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025643402068699535 \t\t Validation Loss: 0.0036938410037412094\n",
            "Epoch 132 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023523870292371392 \t\t Validation Loss: 0.0030840078476243294\n",
            "Epoch 133 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002436253652794639 \t\t Validation Loss: 0.002857888650256567\n",
            "Epoch 134 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.002514509259207124 \t\t Validation Loss: 0.00294024427421391\n",
            "Epoch 135 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002663468868106704 \t\t Validation Loss: 0.0029405542076207125\n",
            "Epoch 136 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002583851499759869 \t\t Validation Loss: 0.002571240165987267\n",
            "Epoch 137 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002678655459218331 \t\t Validation Loss: 0.0028163970295841303\n",
            "Epoch 138 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025644439203362608 \t\t Validation Loss: 0.0025025576156062577\n",
            "Epoch 139 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002569303218577359 \t\t Validation Loss: 0.00260046491614328\n",
            "Epoch 140 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027381815472491892 \t\t Validation Loss: 0.0027573086061658193\n",
            "Epoch 141 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002620987469846433 \t\t Validation Loss: 0.00261158523006508\n",
            "Epoch 142 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025700763138514515 \t\t Validation Loss: 0.002646721581606051\n",
            "Epoch 143 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027509690388231663 \t\t Validation Loss: 0.002677971394195293\n",
            "Epoch 144 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002647571386832341 \t\t Validation Loss: 0.002820152252052839\n",
            "Epoch 145 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025108414717849243 \t\t Validation Loss: 0.0023136102692940487\n",
            "Epoch 146 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026399257657357266 \t\t Validation Loss: 0.002451797965197609\n",
            "Epoch 147 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025352164308502768 \t\t Validation Loss: 0.0022890723955172757\n",
            "Epoch 148 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002623361681676092 \t\t Validation Loss: 0.0027196674939030064\n",
            "Epoch 149 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024206828510048924 \t\t Validation Loss: 0.002340679862894691\n",
            "Epoch 150 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025396060884451946 \t\t Validation Loss: 0.002824007439462898\n",
            "Epoch 151 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025806957305207646 \t\t Validation Loss: 0.0024905553571163462\n",
            "Epoch 152 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027243160120387737 \t\t Validation Loss: 0.0024071138692446626\n",
            "Epoch 153 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026069880853331574 \t\t Validation Loss: 0.0021441279453798556\n",
            "Epoch 154 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002588561597607426 \t\t Validation Loss: 0.0022647636517201765\n",
            "Epoch 155 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026330594390290914 \t\t Validation Loss: 0.002469552479361972\n",
            "Epoch 156 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025910844025831365 \t\t Validation Loss: 0.0019570005959114777\n",
            "Epoch 157 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002662916449087395 \t\t Validation Loss: 0.0021243735547893895\n",
            "Epoch 158 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002803991559729282 \t\t Validation Loss: 0.002375885253199018\n",
            "Epoch 159 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026319958096546296 \t\t Validation Loss: 0.002093253977363929\n",
            "Epoch 160 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025556012877967914 \t\t Validation Loss: 0.002092187942113154\n",
            "Epoch 161 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027886432693088174 \t\t Validation Loss: 0.002373507962777064\n",
            "Epoch 162 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025390622574156404 \t\t Validation Loss: 0.0021163719491316723\n",
            "Epoch 163 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002673857831207381 \t\t Validation Loss: 0.0024715618749793907\n",
            "Epoch 164 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002506960095283953 \t\t Validation Loss: 0.0020010600788876987\n",
            "Epoch 165 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026892391348106635 \t\t Validation Loss: 0.0022167650212605414\n",
            "Epoch 166 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002647004324653362 \t\t Validation Loss: 0.0021679244975810153\n",
            "Epoch 167 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002695547289378639 \t\t Validation Loss: 0.0022922096205337974\n",
            "Epoch 168 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002695422901180447 \t\t Validation Loss: 0.002206618644637414\n",
            "Epoch 169 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024940786659843413 \t\t Validation Loss: 0.0021172851333036446\n",
            "Epoch 170 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026509901445761724 \t\t Validation Loss: 0.002120869478228717\n",
            "Epoch 171 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026864721927464612 \t\t Validation Loss: 0.0019949138222727925\n",
            "Epoch 172 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002567250026717178 \t\t Validation Loss: 0.0018176738522015512\n",
            "Epoch 173 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027559739285828292 \t\t Validation Loss: 0.0021916706441865805\n",
            "Epoch 174 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002542311091265465 \t\t Validation Loss: 0.001900942964801708\n",
            "Epoch 175 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002681937738088891 \t\t Validation Loss: 0.0023059235088742124\n",
            "Epoch 176 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002523777388174691 \t\t Validation Loss: 0.0019052970523122125\n",
            "Epoch 177 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026023551566853515 \t\t Validation Loss: 0.002375924297106954\n",
            "Epoch 178 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025551049101695017 \t\t Validation Loss: 0.0018803254508664114\n",
            "Epoch 179 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002734406668743169 \t\t Validation Loss: 0.002018284923486555\n",
            "Epoch 180 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002811254157898696 \t\t Validation Loss: 0.0019173690435798983\n",
            "Epoch 181 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002749997960416147 \t\t Validation Loss: 0.00192530843644188\n",
            "Epoch 182 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002741669924501833 \t\t Validation Loss: 0.0018685379584964651\n",
            "Epoch 183 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028196898575343593 \t\t Validation Loss: 0.002087311261745456\n",
            "Epoch 184 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028984168783535024 \t\t Validation Loss: 0.002096181092873359\n",
            "Epoch 185 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028192487079650164 \t\t Validation Loss: 0.0020564019594950457\n",
            "Epoch 186 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027785588093605395 \t\t Validation Loss: 0.002173599339305208\n",
            "Epoch 187 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002664569571828218 \t\t Validation Loss: 0.0020903259842620734\n",
            "Epoch 188 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028315380772550567 \t\t Validation Loss: 0.0021633714182266537\n",
            "Epoch 189 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026292760315275677 \t\t Validation Loss: 0.002348977830619193\n",
            "Epoch 190 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00254105675078274 \t\t Validation Loss: 0.0023505839008766296\n",
            "Epoch 191 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002517291229860102 \t\t Validation Loss: 0.002439088009011287\n",
            "Epoch 192 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024925553935227565 \t\t Validation Loss: 0.0022778451227797912\n",
            "Epoch 193 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026113833033328724 \t\t Validation Loss: 0.002645118614264692\n",
            "Epoch 194 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025752512832499437 \t\t Validation Loss: 0.0021202273671336183\n",
            "Epoch 195 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027746847780371034 \t\t Validation Loss: 0.002153137199526939\n",
            "Epoch 196 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028185911737759976 \t\t Validation Loss: 0.002132573046345407\n",
            "Epoch 197 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00254289764042541 \t\t Validation Loss: 0.002040432698917217\n",
            "Epoch 198 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027111592054417408 \t\t Validation Loss: 0.0021863340046435883\n",
            "Epoch 199 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026262580004923447 \t\t Validation Loss: 0.0022006835027311286\n",
            "Epoch 200 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00255846741414201 \t\t Validation Loss: 0.0018654451481639766\n",
            "\n",
            "Score: 100.89007971129442\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "The model has 548,682 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.9221732126599228 \t\t Validation Loss: 0.8173356377161466\n",
            "\t\t Validation Loss Decreased(inf--->0.817336) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.7727157137532895 \t\t Validation Loss: 0.8785848640478574\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.6377051801647287 \t\t Validation Loss: 0.9326436106975262\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.5089065147802938 \t\t Validation Loss: 0.9756575983304244\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.3861674797419157 \t\t Validation Loss: 0.9911082845467788\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.28263058874605074 \t\t Validation Loss: 0.9633996165715731\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.20694900044818987 \t\t Validation Loss: 0.8876697971270635\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.158001429426509 \t\t Validation Loss: 0.7778149751516489\n",
            "\t\t Validation Loss Decreased(0.817336--->0.777815) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.12661608472164418 \t\t Validation Loss: 0.6563200950622559\n",
            "\t\t Validation Loss Decreased(0.777815--->0.656320) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.1029343602272707 \t\t Validation Loss: 0.5379013029428629\n",
            "\t\t Validation Loss Decreased(0.656320--->0.537901) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.08658453102248746 \t\t Validation Loss: 0.42987746917284453\n",
            "\t\t Validation Loss Decreased(0.537901--->0.429877) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.07047252890628737 \t\t Validation Loss: 0.3356138227077631\n",
            "\t\t Validation Loss Decreased(0.429877--->0.335614) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.05824342483302226 \t\t Validation Loss: 0.2538765852267926\n",
            "\t\t Validation Loss Decreased(0.335614--->0.253877) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.04738139486997514 \t\t Validation Loss: 0.18625396432784888\n",
            "\t\t Validation Loss Decreased(0.253877--->0.186254) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.03761704351652313 \t\t Validation Loss: 0.13147862771382698\n",
            "\t\t Validation Loss Decreased(0.186254--->0.131479) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.02999132390863992 \t\t Validation Loss: 0.089881825619019\n",
            "\t\t Validation Loss Decreased(0.131479--->0.089882) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.023115677457000758 \t\t Validation Loss: 0.05885646291650259\n",
            "\t\t Validation Loss Decreased(0.089882--->0.058856) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.017811532698672364 \t\t Validation Loss: 0.0371356073480386\n",
            "\t\t Validation Loss Decreased(0.058856--->0.037136) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.013370873041551662 \t\t Validation Loss: 0.023095070885924194\n",
            "\t\t Validation Loss Decreased(0.037136--->0.023095) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.010215349961072206 \t\t Validation Loss: 0.013670171718471326\n",
            "\t\t Validation Loss Decreased(0.023095--->0.013670) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.007838280695314342 \t\t Validation Loss: 0.007959784062292714\n",
            "\t\t Validation Loss Decreased(0.013670--->0.007960) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.006108318864544098 \t\t Validation Loss: 0.004970624523523908\n",
            "\t\t Validation Loss Decreased(0.007960--->0.004971) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004879359074087965 \t\t Validation Loss: 0.003147630191121537\n",
            "\t\t Validation Loss Decreased(0.004971--->0.003148) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004446282385017823 \t\t Validation Loss: 0.002355540475736444\n",
            "\t\t Validation Loss Decreased(0.003148--->0.002356) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003586256121461456 \t\t Validation Loss: 0.0018854238666020907\n",
            "\t\t Validation Loss Decreased(0.002356--->0.001885) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033278956748167606 \t\t Validation Loss: 0.0016633525732546472\n",
            "\t\t Validation Loss Decreased(0.001885--->0.001663) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031169574578745743 \t\t Validation Loss: 0.0015752822062215554\n",
            "\t\t Validation Loss Decreased(0.001663--->0.001575) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002923704922904034 \t\t Validation Loss: 0.0015741368227351743\n",
            "\t\t Validation Loss Decreased(0.001575--->0.001574) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002820135764400097 \t\t Validation Loss: 0.0015904644566874665\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026982395588797895 \t\t Validation Loss: 0.0016145084115963143\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002878102324462521 \t\t Validation Loss: 0.0016387524396682589\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027649555421384 \t\t Validation Loss: 0.0016537995274680166\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002863708564168396 \t\t Validation Loss: 0.001671724200535279\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028229707653785277 \t\t Validation Loss: 0.0017098423044304722\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029351912246941515 \t\t Validation Loss: 0.001740478986623482\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029014842694851796 \t\t Validation Loss: 0.0017186567241039414\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002696283533697243 \t\t Validation Loss: 0.0017201142909470946\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002795138630726551 \t\t Validation Loss: 0.0017034661297937138\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028072697404658772 \t\t Validation Loss: 0.0017104129745768239\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002548288884320976 \t\t Validation Loss: 0.0017601498178779506\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029152670522799365 \t\t Validation Loss: 0.0017016688675189821\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00274690573349142 \t\t Validation Loss: 0.0016993025581961353\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026200681616476663 \t\t Validation Loss: 0.0017207915437980914\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002658357540132931 \t\t Validation Loss: 0.0017260504783525204\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028666559200910095 \t\t Validation Loss: 0.0017461492948663922\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002946477465180529 \t\t Validation Loss: 0.0017154608615853179\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029421052124628142 \t\t Validation Loss: 0.001714045472908765\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002683715015410672 \t\t Validation Loss: 0.0017009257459833932\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002961926523287705 \t\t Validation Loss: 0.0017149936197361408\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026873555097326235 \t\t Validation Loss: 0.0017083327962049784\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002864355477819068 \t\t Validation Loss: 0.001704985949730214\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028922110544265926 \t\t Validation Loss: 0.0017353264288081287\n",
            "Epoch 53 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002794255660626584 \t\t Validation Loss: 0.0017703121904140483\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002773399062051966 \t\t Validation Loss: 0.001791239519102069\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002915644626498122 \t\t Validation Loss: 0.0017292508631586456\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002657523885293788 \t\t Validation Loss: 0.0017480464929786439\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002883253340621957 \t\t Validation Loss: 0.0018140841947080423\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002752433346924246 \t\t Validation Loss: 0.0017313507269136608\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027410543753413133 \t\t Validation Loss: 0.0017260082957084076\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027936529981702363 \t\t Validation Loss: 0.0017523525121550148\n",
            "Epoch 61 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027454104966710548 \t\t Validation Loss: 0.0018076987202780752\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002818673048357202 \t\t Validation Loss: 0.0017377298768573941\n",
            "Epoch 63 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027517932766066813 \t\t Validation Loss: 0.0017603507462459116\n",
            "Epoch 64 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030332407819711276 \t\t Validation Loss: 0.001751348734475099\n",
            "Epoch 65 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002787338340076039 \t\t Validation Loss: 0.001806371276660894\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002987856967248828 \t\t Validation Loss: 0.0017557849373238592\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026045619790140235 \t\t Validation Loss: 0.0017607337302671601\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028366804409243568 \t\t Validation Loss: 0.001796750038360747\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028639907769330247 \t\t Validation Loss: 0.0017308651429350274\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029391443460701485 \t\t Validation Loss: 0.0017662822546509022\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002601486666920922 \t\t Validation Loss: 0.0017319758212229668\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030243266948083466 \t\t Validation Loss: 0.0017136993074718004\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026798647131481383 \t\t Validation Loss: 0.0018932600505650043\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002701659984981037 \t\t Validation Loss: 0.001850910448970703\n",
            "Epoch 75 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.002983589600016539 \t\t Validation Loss: 0.0018228886541552269\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002749262991159953 \t\t Validation Loss: 0.0017498364278043692\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002782844192650471 \t\t Validation Loss: 0.0018734979094006121\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027923310990445316 \t\t Validation Loss: 0.0017617294842448945\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030041590350289903 \t\t Validation Loss: 0.0017684981083640684\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00289181833835067 \t\t Validation Loss: 0.0018030497012659907\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028581313806195817 \t\t Validation Loss: 0.0017609652042245637\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028843158821424317 \t\t Validation Loss: 0.0018269151198462797\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00299369910971035 \t\t Validation Loss: 0.0017012850529191871\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026324506773537882 \t\t Validation Loss: 0.001880439052071709\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002742326572990498 \t\t Validation Loss: 0.0019041868374468042\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002820934751071036 \t\t Validation Loss: 0.0018267091274118195\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027458165029718264 \t\t Validation Loss: 0.0020075393348144223\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003008124786680816 \t\t Validation Loss: 0.0017891882900865031\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027696044951979375 \t\t Validation Loss: 0.0018743489009256547\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00294858288032481 \t\t Validation Loss: 0.0020313771719184634\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027430283214041106 \t\t Validation Loss: 0.0018270355380641727\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002913571422214846 \t\t Validation Loss: 0.0017427626768879306\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027568198361345945 \t\t Validation Loss: 0.002005029742856725\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029620667990309666 \t\t Validation Loss: 0.001943895526122875\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027254443392283408 \t\t Validation Loss: 0.0018528496058514486\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002905260132801895 \t\t Validation Loss: 0.001985127992856388\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030176300489394045 \t\t Validation Loss: 0.0019788153854628596\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031475580212458765 \t\t Validation Loss: 0.001812059020337004\n",
            "Epoch 99 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002747560300662912 \t\t Validation Loss: 0.0018882918789481313\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029283006482040256 \t\t Validation Loss: 0.0019546590046957135\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029611038657635248 \t\t Validation Loss: 0.0019564954563975334\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002832739629682053 \t\t Validation Loss: 0.002040033071982459\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 205.37735961294416\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "The model has 1,458,682 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.6932170484215021 \t\t Validation Loss: 1.079190024962792\n",
            "\t\t Validation Loss Decreased(inf--->1.079190) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.5621057415532099 \t\t Validation Loss: 1.2743752323664153\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.42134971393121257 \t\t Validation Loss: 1.4405652788969188\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2966073711427885 \t\t Validation Loss: 1.3072435672466571\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2249515534605126 \t\t Validation Loss: 1.0816916548288786\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.16489979758153897 \t\t Validation Loss: 0.8539344255740826\n",
            "\t\t Validation Loss Decreased(1.079190--->0.853934) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.1020213430495681 \t\t Validation Loss: 0.6411758867593912\n",
            "\t\t Validation Loss Decreased(0.853934--->0.641176) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.050698545862089946 \t\t Validation Loss: 0.47426302845661455\n",
            "\t\t Validation Loss Decreased(0.641176--->0.474263) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.021847716947967135 \t\t Validation Loss: 0.4012892166009316\n",
            "\t\t Validation Loss Decreased(0.474263--->0.401289) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.015466019813273404 \t\t Validation Loss: 0.39780954730052215\n",
            "\t\t Validation Loss Decreased(0.401289--->0.397810) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.011846706346684211 \t\t Validation Loss: 0.3853097426203581\n",
            "\t\t Validation Loss Decreased(0.397810--->0.385310) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.008898530696594232 \t\t Validation Loss: 0.36432304233312607\n",
            "\t\t Validation Loss Decreased(0.385310--->0.364323) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0070061936999696334 \t\t Validation Loss: 0.3456248795756927\n",
            "\t\t Validation Loss Decreased(0.364323--->0.345625) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005680566372010051 \t\t Validation Loss: 0.3283072285927259\n",
            "\t\t Validation Loss Decreased(0.345625--->0.328307) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004810124350950826 \t\t Validation Loss: 0.31118017635666406\n",
            "\t\t Validation Loss Decreased(0.328307--->0.311180) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004167165157284487 \t\t Validation Loss: 0.2949463194952561\n",
            "\t\t Validation Loss Decreased(0.311180--->0.294946) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0037411183521554275 \t\t Validation Loss: 0.2808438677054185\n",
            "\t\t Validation Loss Decreased(0.294946--->0.280844) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0034736543657837085 \t\t Validation Loss: 0.268039147154643\n",
            "\t\t Validation Loss Decreased(0.280844--->0.268039) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003265829204669776 \t\t Validation Loss: 0.2560534511621182\n",
            "\t\t Validation Loss Decreased(0.268039--->0.256053) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003120935452799942 \t\t Validation Loss: 0.2449874823483137\n",
            "\t\t Validation Loss Decreased(0.256053--->0.244987) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030157721123179873 \t\t Validation Loss: 0.23475849270247495\n",
            "\t\t Validation Loss Decreased(0.244987--->0.234758) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002903054913543668 \t\t Validation Loss: 0.22515538821999842\n",
            "\t\t Validation Loss Decreased(0.234758--->0.225155) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002806633529945503 \t\t Validation Loss: 0.2165691376878665\n",
            "\t\t Validation Loss Decreased(0.225155--->0.216569) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002724615309538471 \t\t Validation Loss: 0.20903385387590298\n",
            "\t\t Validation Loss Decreased(0.216569--->0.209034) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002660805529386208 \t\t Validation Loss: 0.20152455095488292\n",
            "\t\t Validation Loss Decreased(0.209034--->0.201525) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025756954257942838 \t\t Validation Loss: 0.19473393361728925\n",
            "\t\t Validation Loss Decreased(0.201525--->0.194734) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025480117875377874 \t\t Validation Loss: 0.1888288494486075\n",
            "\t\t Validation Loss Decreased(0.194734--->0.188829) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025123392595166995 \t\t Validation Loss: 0.18351892797419658\n",
            "\t\t Validation Loss Decreased(0.188829--->0.183519) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024826378526946378 \t\t Validation Loss: 0.1781641164651284\n",
            "\t\t Validation Loss Decreased(0.183519--->0.178164) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002441876014140812 \t\t Validation Loss: 0.17359026342343825\n",
            "\t\t Validation Loss Decreased(0.178164--->0.173590) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002425900017024597 \t\t Validation Loss: 0.1688883306745153\n",
            "\t\t Validation Loss Decreased(0.173590--->0.168888) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002383213577559218 \t\t Validation Loss: 0.16454341627943975\n",
            "\t\t Validation Loss Decreased(0.168888--->0.164543) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023357921069160708 \t\t Validation Loss: 0.16122868227270934\n",
            "\t\t Validation Loss Decreased(0.164543--->0.161229) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002357081599797852 \t\t Validation Loss: 0.1581202419474721\n",
            "\t\t Validation Loss Decreased(0.161229--->0.158120) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002319245215354336 \t\t Validation Loss: 0.15422001151511303\n",
            "\t\t Validation Loss Decreased(0.158120--->0.154220) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002315119781997055 \t\t Validation Loss: 0.15125282932646\n",
            "\t\t Validation Loss Decreased(0.154220--->0.151253) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002302663989138563 \t\t Validation Loss: 0.1482067511488612\n",
            "\t\t Validation Loss Decreased(0.151253--->0.148207) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022897196200525236 \t\t Validation Loss: 0.14490650522594267\n",
            "\t\t Validation Loss Decreased(0.148207--->0.144907) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022734744636329343 \t\t Validation Loss: 0.14194144007678217\n",
            "\t\t Validation Loss Decreased(0.144907--->0.141941) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002270984652175291 \t\t Validation Loss: 0.1387769885791036\n",
            "\t\t Validation Loss Decreased(0.141941--->0.138777) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022657139358941364 \t\t Validation Loss: 0.136460204823659\n",
            "\t\t Validation Loss Decreased(0.138777--->0.136460) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022544252349542 \t\t Validation Loss: 0.13296882718658218\n",
            "\t\t Validation Loss Decreased(0.136460--->0.132969) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022623632685281336 \t\t Validation Loss: 0.13062460598750755\n",
            "\t\t Validation Loss Decreased(0.132969--->0.130625) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022320056749457443 \t\t Validation Loss: 0.12813954173515624\n",
            "\t\t Validation Loss Decreased(0.130625--->0.128140) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002241980094097655 \t\t Validation Loss: 0.1254578300823386\n",
            "\t\t Validation Loss Decreased(0.128140--->0.125458) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002235046263809341 \t\t Validation Loss: 0.12301843488015808\n",
            "\t\t Validation Loss Decreased(0.125458--->0.123018) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002221032778883504 \t\t Validation Loss: 0.12108519981400324\n",
            "\t\t Validation Loss Decreased(0.123018--->0.121085) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022592565195785987 \t\t Validation Loss: 0.11866272190729013\n",
            "\t\t Validation Loss Decreased(0.121085--->0.118663) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022197629095407553 \t\t Validation Loss: 0.11754803735619554\n",
            "\t\t Validation Loss Decreased(0.118663--->0.117548) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022298654170102767 \t\t Validation Loss: 0.11536903016699049\n",
            "\t\t Validation Loss Decreased(0.117548--->0.115369) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002226470856348405 \t\t Validation Loss: 0.11402589745389727\n",
            "\t\t Validation Loss Decreased(0.115369--->0.114026) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022320342470413526 \t\t Validation Loss: 0.11182672530412674\n",
            "\t\t Validation Loss Decreased(0.114026--->0.111827) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 10s\n",
            "\t\t Training Loss: 0.0022152655544011176 \t\t Validation Loss: 0.11055321826671179\n",
            "\t\t Validation Loss Decreased(0.111827--->0.110553) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022320972639136016 \t\t Validation Loss: 0.10898856886734183\n",
            "\t\t Validation Loss Decreased(0.110553--->0.108989) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022133552513362185 \t\t Validation Loss: 0.10785921295889868\n",
            "\t\t Validation Loss Decreased(0.108989--->0.107859) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022317498286783294 \t\t Validation Loss: 0.10638068267144263\n",
            "\t\t Validation Loss Decreased(0.107859--->0.106381) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002226571975640852 \t\t Validation Loss: 0.10570348743707515\n",
            "\t\t Validation Loss Decreased(0.106381--->0.105703) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002216249131723433 \t\t Validation Loss: 0.10430750759461752\n",
            "\t\t Validation Loss Decreased(0.105703--->0.104308) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022173185983823764 \t\t Validation Loss: 0.10359455913735124\n",
            "\t\t Validation Loss Decreased(0.104308--->0.103595) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022444210244921617 \t\t Validation Loss: 0.1021486555154507\n",
            "\t\t Validation Loss Decreased(0.103595--->0.102149) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002221725571183236 \t\t Validation Loss: 0.10152660410564679\n",
            "\t\t Validation Loss Decreased(0.102149--->0.101527) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022413820119549497 \t\t Validation Loss: 0.10055387863674416\n",
            "\t\t Validation Loss Decreased(0.101527--->0.100554) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022217705782905622 \t\t Validation Loss: 0.09943607687735213\n",
            "\t\t Validation Loss Decreased(0.100554--->0.099436) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002234774036018329 \t\t Validation Loss: 0.0991510939426147\n",
            "\t\t Validation Loss Decreased(0.099436--->0.099151) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002245582058094442 \t\t Validation Loss: 0.09794979041012433\n",
            "\t\t Validation Loss Decreased(0.099151--->0.097950) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022179989641330933 \t\t Validation Loss: 0.09697860730095552\n",
            "\t\t Validation Loss Decreased(0.097950--->0.096979) \t Saving The Model\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002249049239892613 \t\t Validation Loss: 0.09609934956265184\n",
            "\t\t Validation Loss Decreased(0.096979--->0.096099) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002227313114909103 \t\t Validation Loss: 0.09539560293062375\n",
            "\t\t Validation Loss Decreased(0.096099--->0.095396) \t Saving The Model\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002234684776930088 \t\t Validation Loss: 0.09475318560949884\n",
            "\t\t Validation Loss Decreased(0.095396--->0.094753) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022804860113742383 \t\t Validation Loss: 0.0943969503725664\n",
            "\t\t Validation Loss Decreased(0.094753--->0.094397) \t Saving The Model\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022937220275855144 \t\t Validation Loss: 0.09339818465881623\n",
            "\t\t Validation Loss Decreased(0.094397--->0.093398) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002251892210601049 \t\t Validation Loss: 0.09278713535660735\n",
            "\t\t Validation Loss Decreased(0.093398--->0.092787) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002281313351468762 \t\t Validation Loss: 0.09283744456031574\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023313138549014723 \t\t Validation Loss: 0.09245623173550345\n",
            "\t\t Validation Loss Decreased(0.092787--->0.092456) \t Saving The Model\n",
            "Epoch 75 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023012095889252786 \t\t Validation Loss: 0.09114488108585087\n",
            "\t\t Validation Loss Decreased(0.092456--->0.091145) \t Saving The Model\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023160765318804093 \t\t Validation Loss: 0.09135212597007361\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024603212730505983 \t\t Validation Loss: 0.09152874280698597\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002424360207294592 \t\t Validation Loss: 0.09003962411616857\n",
            "\t\t Validation Loss Decreased(0.091145--->0.090040) \t Saving The Model\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002384647925233317 \t\t Validation Loss: 0.09020588061629006\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025657747960271867 \t\t Validation Loss: 0.09103089121456903\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026524458190652768 \t\t Validation Loss: 0.09089991953582145\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026637336626220997 \t\t Validation Loss: 0.09093167124172816\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028865121594453987 \t\t Validation Loss: 0.09250177523622719\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030450984313328926 \t\t Validation Loss: 0.0930876899510622\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003330541724288786 \t\t Validation Loss: 0.09492603540778734\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0039022574861961846 \t\t Validation Loss: 0.09839192455491194\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004680553987634847 \t\t Validation Loss: 0.10227093519642949\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0058004938025103025 \t\t Validation Loss: 0.10501323052896903\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.006432869737477017 \t\t Validation Loss: 0.10921778436750174\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005934062056759423 \t\t Validation Loss: 0.10378859808238652\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031616107255497293 \t\t Validation Loss: 0.10551495649493657\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00255634801657367 \t\t Validation Loss: 0.10054409758259471\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002326255925377277 \t\t Validation Loss: 0.09877759490448695\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023098043661652686 \t\t Validation Loss: 0.09399365522683813\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002271289504573655 \t\t Validation Loss: 0.09474070255572979\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002313306022630501 \t\t Validation Loss: 0.09173098856654878\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002258684014196734 \t\t Validation Loss: 0.09119858091267255\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023026787251158544 \t\t Validation Loss: 0.08877274429855439\n",
            "\t\t Validation Loss Decreased(0.090040--->0.088773) \t Saving The Model\n",
            "Epoch 99 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022586528023050443 \t\t Validation Loss: 0.08833452681294428\n",
            "\t\t Validation Loss Decreased(0.088773--->0.088335) \t Saving The Model\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023344466108765854 \t\t Validation Loss: 0.08622432847578938\n",
            "\t\t Validation Loss Decreased(0.088335--->0.086224) \t Saving The Model\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022677158310732528 \t\t Validation Loss: 0.08562812311216615\n",
            "\t\t Validation Loss Decreased(0.086224--->0.085628) \t Saving The Model\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002299963970939201 \t\t Validation Loss: 0.08334959839255764\n",
            "\t\t Validation Loss Decreased(0.085628--->0.083350) \t Saving The Model\n",
            "Epoch 103 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002250064826708891 \t\t Validation Loss: 0.08353312419225964\n",
            "Epoch 104 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002343232329695712 \t\t Validation Loss: 0.08104466415870075\n",
            "\t\t Validation Loss Decreased(0.083350--->0.081045) \t Saving The Model\n",
            "Epoch 105 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002212695782135769 \t\t Validation Loss: 0.08080489750808248\n",
            "\t\t Validation Loss Decreased(0.081045--->0.080805) \t Saving The Model\n",
            "Epoch 106 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023298507494965218 \t\t Validation Loss: 0.07929815341217014\n",
            "\t\t Validation Loss Decreased(0.080805--->0.079298) \t Saving The Model\n",
            "Epoch 107 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002281885059248354 \t\t Validation Loss: 0.07870353885496464\n",
            "\t\t Validation Loss Decreased(0.079298--->0.078704) \t Saving The Model\n",
            "Epoch 108 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002315740830994941 \t\t Validation Loss: 0.07838323355938953\n",
            "\t\t Validation Loss Decreased(0.078704--->0.078383) \t Saving The Model\n",
            "Epoch 109 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002315286808093456 \t\t Validation Loss: 0.0770791767953107\n",
            "\t\t Validation Loss Decreased(0.078383--->0.077079) \t Saving The Model\n",
            "Epoch 110 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002321143974424214 \t\t Validation Loss: 0.07714639011269006\n",
            "Epoch 111 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023531263436165615 \t\t Validation Loss: 0.07683219144550654\n",
            "\t\t Validation Loss Decreased(0.077079--->0.076832) \t Saving The Model\n",
            "Epoch 112 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023593977848823007 \t\t Validation Loss: 0.07603348807718319\n",
            "\t\t Validation Loss Decreased(0.076832--->0.076033) \t Saving The Model\n",
            "Epoch 113 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002371661696973181 \t\t Validation Loss: 0.07542660416891941\n",
            "\t\t Validation Loss Decreased(0.076033--->0.075427) \t Saving The Model\n",
            "Epoch 114 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023900401806836395 \t\t Validation Loss: 0.07581174731827699\n",
            "Epoch 115 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024942409206257276 \t\t Validation Loss: 0.07523612524024569\n",
            "\t\t Validation Loss Decreased(0.075427--->0.075236) \t Saving The Model\n",
            "Epoch 116 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002496360580915132 \t\t Validation Loss: 0.07493777637584852\n",
            "\t\t Validation Loss Decreased(0.075236--->0.074938) \t Saving The Model\n",
            "Epoch 117 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00260887059817292 \t\t Validation Loss: 0.07468597447642913\n",
            "\t\t Validation Loss Decreased(0.074938--->0.074686) \t Saving The Model\n",
            "Epoch 118 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002586642500466189 \t\t Validation Loss: 0.075867902702437\n",
            "Epoch 119 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002846576363112576 \t\t Validation Loss: 0.07548237864214641\n",
            "Epoch 120 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027798018858742877 \t\t Validation Loss: 0.07572002255787644\n",
            "Epoch 121 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030217063219940945 \t\t Validation Loss: 0.07659684614135095\n",
            "Epoch 122 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031714454478886283 \t\t Validation Loss: 0.07664293222702466\n",
            "Epoch 123 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032470577264305306 \t\t Validation Loss: 0.07870167955899468\n",
            "Epoch 124 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.003623953935488857 \t\t Validation Loss: 0.07804588608157176\n",
            "Epoch 125 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003445808278598093 \t\t Validation Loss: 0.0807585370225402\n",
            "Epoch 126 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004010002625904776 \t\t Validation Loss: 0.07751791678870526\n",
            "Epoch 127 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003064577146810857 \t\t Validation Loss: 0.08024752073777983\n",
            "Epoch 128 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003543940298825603 \t\t Validation Loss: 0.07419640782217567\n",
            "\t\t Validation Loss Decreased(0.074686--->0.074196) \t Saving The Model\n",
            "Epoch 129 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002595416853846227 \t\t Validation Loss: 0.07859083591029048\n",
            "Epoch 130 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031510822544176436 \t\t Validation Loss: 0.0682790176763844\n",
            "\t\t Validation Loss Decreased(0.074196--->0.068279) \t Saving The Model\n",
            "Epoch 131 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00228805306078111 \t\t Validation Loss: 0.0777293007570104\n",
            "Epoch 132 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003083335839029452 \t\t Validation Loss: 0.06507288421002719\n",
            "\t\t Validation Loss Decreased(0.068279--->0.065073) \t Saving The Model\n",
            "Epoch 133 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023603532840875355 \t\t Validation Loss: 0.07809186982922256\n",
            "Epoch 134 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032194384784324445 \t\t Validation Loss: 0.06433938142771904\n",
            "\t\t Validation Loss Decreased(0.065073--->0.064339) \t Saving The Model\n",
            "Epoch 135 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025070017530910067 \t\t Validation Loss: 0.07963811131552435\n",
            "Epoch 136 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033416469114190724 \t\t Validation Loss: 0.06302868995743875\n",
            "\t\t Validation Loss Decreased(0.064339--->0.063029) \t Saving The Model\n",
            "Epoch 137 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002624150653482994 \t\t Validation Loss: 0.07959287450648844\n",
            "Epoch 138 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003275769595937753 \t\t Validation Loss: 0.06275149592413352\n",
            "\t\t Validation Loss Decreased(0.063029--->0.062751) \t Saving The Model\n",
            "Epoch 139 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002686584338457701 \t\t Validation Loss: 0.07988796438663624\n",
            "Epoch 140 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003251507589624993 \t\t Validation Loss: 0.06223398035105605\n",
            "\t\t Validation Loss Decreased(0.062751--->0.062234) \t Saving The Model\n",
            "Epoch 141 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002740572002162603 \t\t Validation Loss: 0.08045272154805179\n",
            "Epoch 142 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031711586131210868 \t\t Validation Loss: 0.06215158703092199\n",
            "\t\t Validation Loss Decreased(0.062234--->0.062152) \t Saving The Model\n",
            "Epoch 143 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00275852729258052 \t\t Validation Loss: 0.07942886996226242\n",
            "Epoch 144 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030696195852313495 \t\t Validation Loss: 0.062212409218773246\n",
            "Epoch 145 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026440457456644523 \t\t Validation Loss: 0.07849185197399212\n",
            "Epoch 146 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002911169731649696 \t\t Validation Loss: 0.06189543001640301\n",
            "\t\t Validation Loss Decreased(0.062152--->0.061895) \t Saving The Model\n",
            "Epoch 147 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002665099082200008 \t\t Validation Loss: 0.07711192066423021\n",
            "Epoch 148 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002857088211237931 \t\t Validation Loss: 0.06166639449433065\n",
            "\t\t Validation Loss Decreased(0.061895--->0.061666) \t Saving The Model\n",
            "Epoch 149 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026995077417735513 \t\t Validation Loss: 0.07575730793178082\n",
            "Epoch 150 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028068739173594055 \t\t Validation Loss: 0.06146967991326864\n",
            "\t\t Validation Loss Decreased(0.061666--->0.061470) \t Saving The Model\n",
            "Epoch 151 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002670728214513007 \t\t Validation Loss: 0.07470775526375152\n",
            "Epoch 152 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002746352940841855 \t\t Validation Loss: 0.06153140543028712\n",
            "Epoch 153 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002622782839525088 \t\t Validation Loss: 0.07401556165244144\n",
            "Epoch 154 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00266904551598772 \t\t Validation Loss: 0.06139705340879468\n",
            "\t\t Validation Loss Decreased(0.061470--->0.061397) \t Saving The Model\n",
            "Epoch 155 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025529841958494806 \t\t Validation Loss: 0.07273072742212278\n",
            "Epoch 156 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026340904493023314 \t\t Validation Loss: 0.06100797911102955\n",
            "\t\t Validation Loss Decreased(0.061397--->0.061008) \t Saving The Model\n",
            "Epoch 157 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025201408566932217 \t\t Validation Loss: 0.07096606208226429\n",
            "Epoch 158 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025842810219556494 \t\t Validation Loss: 0.06074199564038561\n",
            "\t\t Validation Loss Decreased(0.061008--->0.060742) \t Saving The Model\n",
            "Epoch 159 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002479597567454786 \t\t Validation Loss: 0.06977827576562189\n",
            "Epoch 160 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025683489637575237 \t\t Validation Loss: 0.06126894105154161\n",
            "Epoch 161 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023785221053765635 \t\t Validation Loss: 0.06834779943052965\n",
            "Epoch 162 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024585146488420462 \t\t Validation Loss: 0.06146751209878577\n",
            "Epoch 163 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002321386162342655 \t\t Validation Loss: 0.06660420304307571\n",
            "Epoch 164 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024386142731354747 \t\t Validation Loss: 0.06134987446980981\n",
            "Epoch 165 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002268544118222151 \t\t Validation Loss: 0.06440258063734151\n",
            "Epoch 166 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002355439294242879 \t\t Validation Loss: 0.060829995934349984\n",
            "Epoch 167 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002238955090728563 \t\t Validation Loss: 0.0634300211766878\n",
            "Epoch 168 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002334433009948682 \t\t Validation Loss: 0.06111987221699495\n",
            "Epoch 169 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002239450579509139 \t\t Validation Loss: 0.06142868597705204\n",
            "Epoch 170 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002278797056311993 \t\t Validation Loss: 0.06127028957080956\n",
            "Epoch 171 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002230868231177934 \t\t Validation Loss: 0.05967937006901663\n",
            "\t\t Validation Loss Decreased(0.060742--->0.059679) \t Saving The Model\n",
            "Epoch 172 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022368126570851215 \t\t Validation Loss: 0.060221567642516814\n",
            "Epoch 173 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002256811489476948 \t\t Validation Loss: 0.05931818977786371\n",
            "\t\t Validation Loss Decreased(0.059679--->0.059318) \t Saving The Model\n",
            "Epoch 174 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022394074067297216 \t\t Validation Loss: 0.05950980615587188\n",
            "Epoch 175 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002271195058710873 \t\t Validation Loss: 0.05871348353461004\n",
            "\t\t Validation Loss Decreased(0.059318--->0.058713) \t Saving The Model\n",
            "Epoch 176 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022483294549700177 \t\t Validation Loss: 0.05858786909196239\n",
            "\t\t Validation Loss Decreased(0.058713--->0.058588) \t Saving The Model\n",
            "Epoch 177 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022955425697177446 \t\t Validation Loss: 0.057695254074552886\n",
            "\t\t Validation Loss Decreased(0.058588--->0.057695) \t Saving The Model\n",
            "Epoch 178 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022818067715176054 \t\t Validation Loss: 0.05749683959696155\n",
            "\t\t Validation Loss Decreased(0.057695--->0.057497) \t Saving The Model\n",
            "Epoch 179 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023334482574608882 \t\t Validation Loss: 0.058095175557984755\n",
            "Epoch 180 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002397489081124297 \t\t Validation Loss: 0.05746072555820529\n",
            "\t\t Validation Loss Decreased(0.057497--->0.057461) \t Saving The Model\n",
            "Epoch 181 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023721042356604862 \t\t Validation Loss: 0.057433726463037044\n",
            "\t\t Validation Loss Decreased(0.057461--->0.057434) \t Saving The Model\n",
            "Epoch 182 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002418582481835541 \t\t Validation Loss: 0.057466230498483546\n",
            "Epoch 183 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002476642227643547 \t\t Validation Loss: 0.05714443483604835\n",
            "\t\t Validation Loss Decreased(0.057434--->0.057144) \t Saving The Model\n",
            "Epoch 184 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024892275844625123 \t\t Validation Loss: 0.05740681912105244\n",
            "Epoch 185 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025485004801806565 \t\t Validation Loss: 0.05810286785260989\n",
            "Epoch 186 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027196145430836522 \t\t Validation Loss: 0.058684572470016204\n",
            "Epoch 187 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002801675568145977 \t\t Validation Loss: 0.058344244598769225\n",
            "Epoch 188 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028726174543309653 \t\t Validation Loss: 0.05905409751889797\n",
            "Epoch 189 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031272752204205135 \t\t Validation Loss: 0.060410808628568284\n",
            "Epoch 190 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0034043292744664125 \t\t Validation Loss: 0.06172034979009858\n",
            "Epoch 191 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003669822064694017 \t\t Validation Loss: 0.06329067552892062\n",
            "Epoch 192 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004183507475662171 \t\t Validation Loss: 0.06388195808260487\n",
            "Epoch 193 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004277131056123589 \t\t Validation Loss: 0.06582404472506963\n",
            "Epoch 194 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004366166598629206 \t\t Validation Loss: 0.0628121855059782\n",
            "Epoch 195 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033849593915825558 \t\t Validation Loss: 0.06263953764349796\n",
            "Epoch 196 \t\t Epoch time: 0m 10s\n",
            "\t\t Training Loss: 0.0029614801719986105 \t\t Validation Loss: 0.05661816128457968\n",
            "\t\t Validation Loss Decreased(0.057144--->0.056618) \t Saving The Model\n",
            "Epoch 197 \t\t Epoch time: 0m 9s\n",
            "\t\t Training Loss: 0.00237473301712197 \t\t Validation Loss: 0.058635198672373705\n",
            "Epoch 198 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002604033739771694 \t\t Validation Loss: 0.050601016282318875\n",
            "\t\t Validation Loss Decreased(0.056618--->0.050601) \t Saving The Model\n",
            "Epoch 199 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023072679854325346 \t\t Validation Loss: 0.05931928753852844\n",
            "Epoch 200 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002846175038595558 \t\t Validation Loss: 0.046214444598612875\n",
            "\t\t Validation Loss Decreased(0.050601--->0.046214) \t Saving The Model\n",
            "\n",
            "Score: 4037.519035532995\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "The model has 2,224,554 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.5486159321751345 \t\t Validation Loss: 1.3434926454837506\n",
            "\t\t Validation Loss Decreased(inf--->1.343493) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.45560905001302426 \t\t Validation Loss: 1.4616196247247548\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.35449775056661786 \t\t Validation Loss: 1.4540325173964868\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.2473401593837283 \t\t Validation Loss: 1.2191321437175457\n",
            "\t\t Validation Loss Decreased(1.343493--->1.219132) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.16323669524737508 \t\t Validation Loss: 0.9675204478777372\n",
            "\t\t Validation Loss Decreased(1.219132--->0.967520) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.08207761021205098 \t\t Validation Loss: 0.6986218553323013\n",
            "\t\t Validation Loss Decreased(0.967520--->0.698622) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.025051743984599976 \t\t Validation Loss: 0.46586959064006805\n",
            "\t\t Validation Loss Decreased(0.698622--->0.465870) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.006509903527330607 \t\t Validation Loss: 0.3835639592546683\n",
            "\t\t Validation Loss Decreased(0.465870--->0.383564) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.007880314330356446 \t\t Validation Loss: 0.37144186462347323\n",
            "\t\t Validation Loss Decreased(0.383564--->0.371442) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0056142578581096355 \t\t Validation Loss: 0.33328238072303623\n",
            "\t\t Validation Loss Decreased(0.371442--->0.333282) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005019346436145841 \t\t Validation Loss: 0.30004049321779835\n",
            "\t\t Validation Loss Decreased(0.333282--->0.300040) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004463756906900655 \t\t Validation Loss: 0.2733375091965382\n",
            "\t\t Validation Loss Decreased(0.300040--->0.273338) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003935881064437027 \t\t Validation Loss: 0.2475394385938461\n",
            "\t\t Validation Loss Decreased(0.273338--->0.247539) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00373284743683463 \t\t Validation Loss: 0.22262682020664215\n",
            "\t\t Validation Loss Decreased(0.247539--->0.222627) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003316071139321335 \t\t Validation Loss: 0.20333026320888445\n",
            "\t\t Validation Loss Decreased(0.222627--->0.203330) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030477024729653086 \t\t Validation Loss: 0.18845166638493538\n",
            "\t\t Validation Loss Decreased(0.203330--->0.188452) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029566621815320104 \t\t Validation Loss: 0.17252683152373022\n",
            "\t\t Validation Loss Decreased(0.188452--->0.172527) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027502610307262353 \t\t Validation Loss: 0.1619748196636255\n",
            "\t\t Validation Loss Decreased(0.172527--->0.161975) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027445952205075504 \t\t Validation Loss: 0.15187940030143812\n",
            "\t\t Validation Loss Decreased(0.161975--->0.151879) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002708670007524307 \t\t Validation Loss: 0.1429980299077355\n",
            "\t\t Validation Loss Decreased(0.151879--->0.142998) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0023702187243395964 \t\t Validation Loss: 0.13637349425027004\n",
            "\t\t Validation Loss Decreased(0.142998--->0.136373) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026124986077381953 \t\t Validation Loss: 0.13069521449506283\n",
            "\t\t Validation Loss Decreased(0.136373--->0.130695) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025351820073115663 \t\t Validation Loss: 0.12468482546794873\n",
            "\t\t Validation Loss Decreased(0.130695--->0.124685) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002619118819869048 \t\t Validation Loss: 0.11968891422909039\n",
            "\t\t Validation Loss Decreased(0.124685--->0.119689) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024626706365017672 \t\t Validation Loss: 0.11545747912560518\n",
            "\t\t Validation Loss Decreased(0.119689--->0.115457) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002442939255726398 \t\t Validation Loss: 0.1107635384855362\n",
            "\t\t Validation Loss Decreased(0.115457--->0.110764) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026358061051625454 \t\t Validation Loss: 0.10904697264329745\n",
            "\t\t Validation Loss Decreased(0.110764--->0.109047) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024786966827672883 \t\t Validation Loss: 0.10425202924614915\n",
            "\t\t Validation Loss Decreased(0.109047--->0.104252) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002552873340932451 \t\t Validation Loss: 0.09973777816272698\n",
            "\t\t Validation Loss Decreased(0.104252--->0.099738) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002560633763268187 \t\t Validation Loss: 0.09853895037220074\n",
            "\t\t Validation Loss Decreased(0.099738--->0.098539) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002477363308079893 \t\t Validation Loss: 0.09434395365846845\n",
            "\t\t Validation Loss Decreased(0.098539--->0.094344) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024071566703905527 \t\t Validation Loss: 0.09421986226852123\n",
            "\t\t Validation Loss Decreased(0.094344--->0.094220) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025045647863235726 \t\t Validation Loss: 0.09012423928540486\n",
            "\t\t Validation Loss Decreased(0.094220--->0.090124) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002577556651148542 \t\t Validation Loss: 0.08777423695517847\n",
            "\t\t Validation Loss Decreased(0.090124--->0.087774) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026017044343658396 \t\t Validation Loss: 0.08690147909622353\n",
            "\t\t Validation Loss Decreased(0.087774--->0.086901) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024678501917200315 \t\t Validation Loss: 0.08287369409719339\n",
            "\t\t Validation Loss Decreased(0.086901--->0.082874) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024976068706490805 \t\t Validation Loss: 0.08226972191522901\n",
            "\t\t Validation Loss Decreased(0.082874--->0.082270) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002428230896397066 \t\t Validation Loss: 0.082826780119481\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024658155649573215 \t\t Validation Loss: 0.07756672851526393\n",
            "\t\t Validation Loss Decreased(0.082270--->0.077567) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026002611219253695 \t\t Validation Loss: 0.07720552664250135\n",
            "\t\t Validation Loss Decreased(0.077567--->0.077206) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024975428949236063 \t\t Validation Loss: 0.07638926320494367\n",
            "\t\t Validation Loss Decreased(0.077206--->0.076389) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025108738431102924 \t\t Validation Loss: 0.07541880556024037\n",
            "\t\t Validation Loss Decreased(0.076389--->0.075419) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002413360633245493 \t\t Validation Loss: 0.0752440003427462\n",
            "\t\t Validation Loss Decreased(0.075419--->0.075244) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00246999625468979 \t\t Validation Loss: 0.07339353517342645\n",
            "\t\t Validation Loss Decreased(0.075244--->0.073394) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0022790072142851313 \t\t Validation Loss: 0.07128721663656716\n",
            "\t\t Validation Loss Decreased(0.073394--->0.071287) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025811266890299078 \t\t Validation Loss: 0.07142435875721276\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026000915905397787 \t\t Validation Loss: 0.07072813105053054\n",
            "\t\t Validation Loss Decreased(0.071287--->0.070728) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002570147705547562 \t\t Validation Loss: 0.07042880076915026\n",
            "\t\t Validation Loss Decreased(0.070728--->0.070429) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025976218528592507 \t\t Validation Loss: 0.06600560512966834\n",
            "\t\t Validation Loss Decreased(0.070429--->0.066006) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002750703025093252 \t\t Validation Loss: 0.06867676117242529\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026510819679478534 \t\t Validation Loss: 0.06604022504045413\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026083136971637205 \t\t Validation Loss: 0.06531683249900547\n",
            "\t\t Validation Loss Decreased(0.066006--->0.065317) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002587481894266062 \t\t Validation Loss: 0.06783806919478454\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024522802559658885 \t\t Validation Loss: 0.06499011098192288\n",
            "\t\t Validation Loss Decreased(0.065317--->0.064990) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002422729225489437 \t\t Validation Loss: 0.06550600752234459\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002617401974684735 \t\t Validation Loss: 0.06427641196821171\n",
            "\t\t Validation Loss Decreased(0.064990--->0.064276) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026737365358811176 \t\t Validation Loss: 0.06432981476581727\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002492189362513002 \t\t Validation Loss: 0.061842880032670036\n",
            "\t\t Validation Loss Decreased(0.064276--->0.061843) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00268727628124022 \t\t Validation Loss: 0.06575073896405789\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025269913650747084 \t\t Validation Loss: 0.06108643449484729\n",
            "\t\t Validation Loss Decreased(0.061843--->0.061086) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026825340653492792 \t\t Validation Loss: 0.06551087302012512\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002628780092030915 \t\t Validation Loss: 0.06216899144391601\n",
            "Epoch 63 \t\t Epoch time: 0m 12s\n",
            "\t\t Training Loss: 0.002732198282673552 \t\t Validation Loss: 0.06447308610838193\n",
            "Epoch 64 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025408627281305256 \t\t Validation Loss: 0.06088038788248713\n",
            "\t\t Validation Loss Decreased(0.061086--->0.060880) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029347773493468964 \t\t Validation Loss: 0.06268375592592818\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002523782173001736 \t\t Validation Loss: 0.06021069982447303\n",
            "\t\t Validation Loss Decreased(0.060880--->0.060211) \t Saving The Model\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002659091799098344 \t\t Validation Loss: 0.06092989860245815\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026856254521958733 \t\t Validation Loss: 0.06006568753554557\n",
            "\t\t Validation Loss Decreased(0.060211--->0.060066) \t Saving The Model\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025422286125831306 \t\t Validation Loss: 0.06051847913589042\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026734467938768903 \t\t Validation Loss: 0.05833012513064135\n",
            "\t\t Validation Loss Decreased(0.060066--->0.058330) \t Saving The Model\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025346829057230637 \t\t Validation Loss: 0.058902171171771794\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002570599286087059 \t\t Validation Loss: 0.06169384870176705\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025497737035432176 \t\t Validation Loss: 0.05718947948816304\n",
            "\t\t Validation Loss Decreased(0.058330--->0.057189) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027831780033601397 \t\t Validation Loss: 0.057436381556236975\n",
            "Epoch 75 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002734443625888309 \t\t Validation Loss: 0.058593213343276426\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002532968631901501 \t\t Validation Loss: 0.05442919524816366\n",
            "\t\t Validation Loss Decreased(0.057189--->0.054429) \t Saving The Model\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031296942765333667 \t\t Validation Loss: 0.05936709520980143\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026601242724290067 \t\t Validation Loss: 0.05775029804163541\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030069182936819524 \t\t Validation Loss: 0.058086134027689695\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002700366695707261 \t\t Validation Loss: 0.053472194784822374\n",
            "\t\t Validation Loss Decreased(0.054429--->0.053472) \t Saving The Model\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035926115658209733 \t\t Validation Loss: 0.062397397929229416\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025491207992230113 \t\t Validation Loss: 0.05831577555419734\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003013498763352431 \t\t Validation Loss: 0.05689730267756833\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00274569183160123 \t\t Validation Loss: 0.057433562577128984\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031436707909420335 \t\t Validation Loss: 0.05991929844738199\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024791743725890644 \t\t Validation Loss: 0.058279609450927146\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002716460021376308 \t\t Validation Loss: 0.05870854166837839\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026664046643024965 \t\t Validation Loss: 0.05851417006208347\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028955825632815627 \t\t Validation Loss: 0.05867235640135522\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002866760321036988 \t\t Validation Loss: 0.05862426412148544\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028821781224168436 \t\t Validation Loss: 0.05921591970568093\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028104560946844315 \t\t Validation Loss: 0.05761074079558826\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026877233923752668 \t\t Validation Loss: 0.057235358820225186\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030299772860834726 \t\t Validation Loss: 0.05854670965793328\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002875975300117421 \t\t Validation Loss: 0.05624463435154981\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031078806071466692 \t\t Validation Loss: 0.058776573869041525\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028481374312242544 \t\t Validation Loss: 0.05725522633964339\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003053911856212024 \t\t Validation Loss: 0.06097497780305835\n",
            "Epoch 99 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026538643735851087 \t\t Validation Loss: 0.056583556901806824\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003459024932109625 \t\t Validation Loss: 0.06466321177923909\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002792429005116778 \t\t Validation Loss: 0.05430855881422758\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0041566842213600266 \t\t Validation Loss: 0.06885308645164165\n",
            "Epoch 103 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027106374547804226 \t\t Validation Loss: 0.054142833705275104\n",
            "Epoch 104 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0044843983031003865 \t\t Validation Loss: 0.07262339799378353\n",
            "Epoch 105 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027494886504624643 \t\t Validation Loss: 0.05318813489821668\n",
            "\t\t Validation Loss Decreased(0.053472--->0.053188) \t Saving The Model\n",
            "Epoch 106 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004726216314215767 \t\t Validation Loss: 0.0814173141709314\n",
            "Epoch 107 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030632134034562653 \t\t Validation Loss: 0.055040547964521326\n",
            "Epoch 108 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004812071018071996 \t\t Validation Loss: 0.08707421720744325\n",
            "Epoch 109 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030926885759785167 \t\t Validation Loss: 0.05543929607105943\n",
            "Epoch 110 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00464229383096854 \t\t Validation Loss: 0.08555302700887506\n",
            "Epoch 111 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002761581799367795 \t\t Validation Loss: 0.057645555764723286\n",
            "Epoch 112 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035535461116720595 \t\t Validation Loss: 0.07493261960693277\n",
            "Epoch 113 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026253732770832408 \t\t Validation Loss: 0.05912483854290958\n",
            "Epoch 114 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003380125791237161 \t\t Validation Loss: 0.06396561277170594\n",
            "Epoch 115 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027075185986374178 \t\t Validation Loss: 0.0563035431282165\n",
            "Epoch 116 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030132421281352335 \t\t Validation Loss: 0.06229517051878457\n",
            "Epoch 117 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0024421809231736573 \t\t Validation Loss: 0.05292110934925194\n",
            "\t\t Validation Loss Decreased(0.053188--->0.052921) \t Saving The Model\n",
            "Epoch 118 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028652349696180004 \t\t Validation Loss: 0.05941461358004464\n",
            "Epoch 119 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026307224557181267 \t\t Validation Loss: 0.0524284728946021\n",
            "\t\t Validation Loss Decreased(0.052921--->0.052428) \t Saving The Model\n",
            "Epoch 120 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031782871968037375 \t\t Validation Loss: 0.05728359201636452\n",
            "Epoch 121 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026673074179278636 \t\t Validation Loss: 0.05259934245035625\n",
            "Epoch 122 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032977773713552066 \t\t Validation Loss: 0.05771466383997064\n",
            "Epoch 123 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025992924992523686 \t\t Validation Loss: 0.04851575764433409\n",
            "\t\t Validation Loss Decreased(0.052428--->0.048516) \t Saving The Model\n",
            "Epoch 124 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003252076137672506 \t\t Validation Loss: 0.060326709412038326\n",
            "Epoch 125 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028312108660355916 \t\t Validation Loss: 0.04758127436686594\n",
            "\t\t Validation Loss Decreased(0.048516--->0.047581) \t Saving The Model\n",
            "Epoch 126 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035709733996112402 \t\t Validation Loss: 0.05995659671652202\n",
            "Epoch 127 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026373002289580435 \t\t Validation Loss: 0.04692351304961798\n",
            "\t\t Validation Loss Decreased(0.047581--->0.046924) \t Saving The Model\n",
            "Epoch 128 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0036182736669591554 \t\t Validation Loss: 0.060975385579065636\n",
            "Epoch 129 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002568752547361057 \t\t Validation Loss: 0.04641328966961457\n",
            "\t\t Validation Loss Decreased(0.046924--->0.046413) \t Saving The Model\n",
            "Epoch 130 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003906797532722153 \t\t Validation Loss: 0.06590038000677641\n",
            "Epoch 131 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002631742356194032 \t\t Validation Loss: 0.04699122575290788\n",
            "Epoch 132 \t\t Epoch time: 0m 12s\n",
            "\t\t Training Loss: 0.003805113138278594 \t\t Validation Loss: 0.06819849031475875\n",
            "Epoch 133 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026710662791014626 \t\t Validation Loss: 0.044476304973404\n",
            "\t\t Validation Loss Decreased(0.046413--->0.044476) \t Saving The Model\n",
            "Epoch 134 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00443270244018955 \t\t Validation Loss: 0.07263224822683977\n",
            "Epoch 135 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00272589925962903 \t\t Validation Loss: 0.045907924363676175\n",
            "Epoch 136 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004522374687980659 \t\t Validation Loss: 0.06692309272833742\n",
            "Epoch 137 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026954737495052047 \t\t Validation Loss: 0.04503395458540091\n",
            "Epoch 138 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003843021883497115 \t\t Validation Loss: 0.06054458826278838\n",
            "Epoch 139 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0026949754297871747 \t\t Validation Loss: 0.046213984973012254\n",
            "Epoch 140 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032846996768352552 \t\t Validation Loss: 0.05757976735297304\n",
            "Epoch 141 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0025207470218348944 \t\t Validation Loss: 0.044048862621331446\n",
            "\t\t Validation Loss Decreased(0.044476--->0.044049) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 3992.3721446700506\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "The model has 606,074 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.8274695497899744 \t\t Validation Loss: 0.9076784895016596\n",
            "\t\t Validation Loss Decreased(inf--->0.907678) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.6534758862548482 \t\t Validation Loss: 1.199928384560805\n",
            "Epoch 3 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.4355458089377026 \t\t Validation Loss: 1.413166541319627\n",
            "Epoch 4 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.27334304788225405 \t\t Validation Loss: 1.233534533243913\n",
            "Epoch 5 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.20263114543883382 \t\t Validation Loss: 1.0054980424734263\n",
            "Epoch 6 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.14446150715387351 \t\t Validation Loss: 0.821319967508316\n",
            "\t\t Validation Loss Decreased(0.907678--->0.821320) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0843789758211052 \t\t Validation Loss: 0.6805817874578329\n",
            "\t\t Validation Loss Decreased(0.821320--->0.680582) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.04034413286560291 \t\t Validation Loss: 0.5759669106740218\n",
            "\t\t Validation Loss Decreased(0.680582--->0.575967) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.014857810192011498 \t\t Validation Loss: 0.5019066173296708\n",
            "\t\t Validation Loss Decreased(0.575967--->0.501907) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.008253836006907796 \t\t Validation Loss: 0.47309819895487565\n",
            "\t\t Validation Loss Decreased(0.501907--->0.473098) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.007362595015218029 \t\t Validation Loss: 0.4491637681539242\n",
            "\t\t Validation Loss Decreased(0.473098--->0.449164) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005657022612836413 \t\t Validation Loss: 0.41841475894817937\n",
            "\t\t Validation Loss Decreased(0.449164--->0.418415) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005364703656350439 \t\t Validation Loss: 0.3928223103284836\n",
            "\t\t Validation Loss Decreased(0.418415--->0.392822) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004791136705190749 \t\t Validation Loss: 0.36923336123044675\n",
            "\t\t Validation Loss Decreased(0.392822--->0.369233) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004641093035241136 \t\t Validation Loss: 0.3465621666266368\n",
            "\t\t Validation Loss Decreased(0.369233--->0.346562) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00414630414987285 \t\t Validation Loss: 0.32468715653969693\n",
            "\t\t Validation Loss Decreased(0.346562--->0.324687) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00405465581964047 \t\t Validation Loss: 0.3056703324501331\n",
            "\t\t Validation Loss Decreased(0.324687--->0.305670) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003776900328670603 \t\t Validation Loss: 0.2871251759620813\n",
            "\t\t Validation Loss Decreased(0.305670--->0.287125) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003606719701122996 \t\t Validation Loss: 0.27074798798331845\n",
            "\t\t Validation Loss Decreased(0.287125--->0.270748) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003252274782327633 \t\t Validation Loss: 0.25540018626130545\n",
            "\t\t Validation Loss Decreased(0.270748--->0.255400) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003282546087772258 \t\t Validation Loss: 0.24079610521976763\n",
            "\t\t Validation Loss Decreased(0.255400--->0.240796) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003309053840820451 \t\t Validation Loss: 0.2288546199695422\n",
            "\t\t Validation Loss Decreased(0.240796--->0.228855) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003233126167214249 \t\t Validation Loss: 0.21906643690398106\n",
            "\t\t Validation Loss Decreased(0.228855--->0.219066) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00316801414954023 \t\t Validation Loss: 0.20713337209935373\n",
            "\t\t Validation Loss Decreased(0.219066--->0.207133) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00286836941361528 \t\t Validation Loss: 0.19581564931342235\n",
            "\t\t Validation Loss Decreased(0.207133--->0.195816) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029628896406172097 \t\t Validation Loss: 0.18557147309184074\n",
            "\t\t Validation Loss Decreased(0.195816--->0.185571) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027974011216035767 \t\t Validation Loss: 0.17665097669053537\n",
            "\t\t Validation Loss Decreased(0.185571--->0.176651) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002849697317877734 \t\t Validation Loss: 0.17280914799238628\n",
            "\t\t Validation Loss Decreased(0.176651--->0.172809) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028397655649412728 \t\t Validation Loss: 0.16721806617883536\n",
            "\t\t Validation Loss Decreased(0.172809--->0.167218) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003170847353160482 \t\t Validation Loss: 0.1615615440532565\n",
            "\t\t Validation Loss Decreased(0.167218--->0.161562) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002904114639071899 \t\t Validation Loss: 0.15703054424375296\n",
            "\t\t Validation Loss Decreased(0.161562--->0.157031) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031434445000077423 \t\t Validation Loss: 0.15101021241683227\n",
            "\t\t Validation Loss Decreased(0.157031--->0.151010) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027407119491112395 \t\t Validation Loss: 0.1460326099051879\n",
            "\t\t Validation Loss Decreased(0.151010--->0.146033) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002799483823174661 \t\t Validation Loss: 0.14263985875564125\n",
            "\t\t Validation Loss Decreased(0.146033--->0.142640) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003032208257512114 \t\t Validation Loss: 0.14011839909765583\n",
            "\t\t Validation Loss Decreased(0.142640--->0.140118) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029330121516879344 \t\t Validation Loss: 0.1355008318876991\n",
            "\t\t Validation Loss Decreased(0.140118--->0.135501) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003056350933400461 \t\t Validation Loss: 0.13228901801630855\n",
            "\t\t Validation Loss Decreased(0.135501--->0.132289) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00301380327909027 \t\t Validation Loss: 0.1305926376237319\n",
            "\t\t Validation Loss Decreased(0.132289--->0.130593) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002691195494293965 \t\t Validation Loss: 0.12603935327094334\n",
            "\t\t Validation Loss Decreased(0.130593--->0.126039) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027766023413278162 \t\t Validation Loss: 0.12208242936489674\n",
            "\t\t Validation Loss Decreased(0.126039--->0.122082) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002689725433073535 \t\t Validation Loss: 0.12275346418699393\n",
            "Epoch 42 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003050103449306728 \t\t Validation Loss: 0.11703565505404885\n",
            "\t\t Validation Loss Decreased(0.122082--->0.117036) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027577809382204875 \t\t Validation Loss: 0.11385740101552354\n",
            "\t\t Validation Loss Decreased(0.117036--->0.113857) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027835406819232615 \t\t Validation Loss: 0.11499800092469041\n",
            "Epoch 45 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027741683659621994 \t\t Validation Loss: 0.11360767322520797\n",
            "\t\t Validation Loss Decreased(0.113857--->0.113608) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00288905985055592 \t\t Validation Loss: 0.10947172884614421\n",
            "\t\t Validation Loss Decreased(0.113608--->0.109472) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002727879532235297 \t\t Validation Loss: 0.11100501715778731\n",
            "Epoch 48 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002826991703399029 \t\t Validation Loss: 0.10882473434321582\n",
            "\t\t Validation Loss Decreased(0.109472--->0.108825) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002806104247684817 \t\t Validation Loss: 0.10890296753495932\n",
            "Epoch 50 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002813087886425893 \t\t Validation Loss: 0.10416842022767434\n",
            "\t\t Validation Loss Decreased(0.108825--->0.104168) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002824895845323398 \t\t Validation Loss: 0.10374916552637632\n",
            "\t\t Validation Loss Decreased(0.104168--->0.103749) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027047023365928515 \t\t Validation Loss: 0.10606723395176232\n",
            "Epoch 53 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029013881906615317 \t\t Validation Loss: 0.09998271755802517\n",
            "\t\t Validation Loss Decreased(0.103749--->0.099983) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028905327199026942 \t\t Validation Loss: 0.103640233954558\n",
            "Epoch 55 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027103966457542737 \t\t Validation Loss: 0.09935995649832946\n",
            "\t\t Validation Loss Decreased(0.099983--->0.099360) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002698953726763107 \t\t Validation Loss: 0.10013629942057797\n",
            "Epoch 57 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027814696580675 \t\t Validation Loss: 0.09792450267391709\n",
            "\t\t Validation Loss Decreased(0.099360--->0.097925) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027521083892851667 \t\t Validation Loss: 0.09843596593978313\n",
            "Epoch 59 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028939120875784777 \t\t Validation Loss: 0.0946971492555279\n",
            "\t\t Validation Loss Decreased(0.097925--->0.094697) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002999865183310986 \t\t Validation Loss: 0.09718754291176222\n",
            "Epoch 61 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.0027871933479390637 \t\t Validation Loss: 0.0939216787676112\n",
            "\t\t Validation Loss Decreased(0.094697--->0.093922) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.00282694652009554 \t\t Validation Loss: 0.10064600273751868\n",
            "Epoch 63 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002793077025499598 \t\t Validation Loss: 0.09315225433629866\n",
            "\t\t Validation Loss Decreased(0.093922--->0.093152) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002953394185562895 \t\t Validation Loss: 0.09987402796888581\n",
            "Epoch 65 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028474806553080074 \t\t Validation Loss: 0.09375123326809934\n",
            "Epoch 66 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002702667746013282 \t\t Validation Loss: 0.09573797050576943\n",
            "Epoch 67 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003006793202272289 \t\t Validation Loss: 0.09302535242974184\n",
            "\t\t Validation Loss Decreased(0.093152--->0.093025) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002907322206836496 \t\t Validation Loss: 0.09810466210071284\n",
            "Epoch 69 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029903819172555027 \t\t Validation Loss: 0.09267247033018905\n",
            "\t\t Validation Loss Decreased(0.093025--->0.092672) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002713428457846513 \t\t Validation Loss: 0.0931061151294181\n",
            "Epoch 71 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002960391287578622 \t\t Validation Loss: 0.09104691026732326\n",
            "\t\t Validation Loss Decreased(0.092672--->0.091047) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002659309156729865 \t\t Validation Loss: 0.09618827231371632\n",
            "Epoch 73 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003410577360301505 \t\t Validation Loss: 0.08954634121619165\n",
            "\t\t Validation Loss Decreased(0.091047--->0.089546) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028091790311894306 \t\t Validation Loss: 0.09952700994192408\n",
            "Epoch 75 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033172064655888323 \t\t Validation Loss: 0.09072379791177809\n",
            "Epoch 76 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028314560091727086 \t\t Validation Loss: 0.0999787887916542\n",
            "Epoch 77 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031464653021995785 \t\t Validation Loss: 0.08743416034401609\n",
            "\t\t Validation Loss Decreased(0.089546--->0.087434) \t Saving The Model\n",
            "Epoch 78 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002860746151339766 \t\t Validation Loss: 0.09856106595207866\n",
            "Epoch 79 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031088474851630227 \t\t Validation Loss: 0.0895907376677944\n",
            "Epoch 80 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030118719850843016 \t\t Validation Loss: 0.0969302287647644\n",
            "Epoch 81 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032037916385593847 \t\t Validation Loss: 0.09058488654689147\n",
            "Epoch 82 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002891741965180011 \t\t Validation Loss: 0.0911037903995468\n",
            "Epoch 83 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002844677198162252 \t\t Validation Loss: 0.0893693536412544\n",
            "Epoch 84 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028194171798138603 \t\t Validation Loss: 0.09318492660084023\n",
            "Epoch 85 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029103001674033097 \t\t Validation Loss: 0.08853787871507499\n",
            "Epoch 86 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027824618296722905 \t\t Validation Loss: 0.08727352446518265\n",
            "\t\t Validation Loss Decreased(0.087434--->0.087274) \t Saving The Model\n",
            "Epoch 87 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027325999517169956 \t\t Validation Loss: 0.0902463981630997\n",
            "Epoch 88 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027948099367849128 \t\t Validation Loss: 0.09096969542308496\n",
            "Epoch 89 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002800308498302223 \t\t Validation Loss: 0.08649012736546305\n",
            "\t\t Validation Loss Decreased(0.087274--->0.086490) \t Saving The Model\n",
            "Epoch 90 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002755561484829397 \t\t Validation Loss: 0.0873453973685033\n",
            "Epoch 91 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003230370594522085 \t\t Validation Loss: 0.08605168167000207\n",
            "\t\t Validation Loss Decreased(0.086490--->0.086052) \t Saving The Model\n",
            "Epoch 92 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029969090246595442 \t\t Validation Loss: 0.08794112593078843\n",
            "Epoch 93 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003071445652371826 \t\t Validation Loss: 0.08719297186829723\n",
            "Epoch 94 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030194971461881054 \t\t Validation Loss: 0.08641721501659888\n",
            "Epoch 95 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002949645572870567 \t\t Validation Loss: 0.08475357297664651\n",
            "\t\t Validation Loss Decreased(0.086052--->0.084754) \t Saving The Model\n",
            "Epoch 96 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002914875832298217 \t\t Validation Loss: 0.088039573496924\n",
            "Epoch 97 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003168531438662998 \t\t Validation Loss: 0.0846495595939744\n",
            "\t\t Validation Loss Decreased(0.084754--->0.084650) \t Saving The Model\n",
            "Epoch 98 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029590323211851756 \t\t Validation Loss: 0.0874932248216982\n",
            "Epoch 99 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003328331609294322 \t\t Validation Loss: 0.08538533885103579\n",
            "Epoch 100 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003534302532333075 \t\t Validation Loss: 0.08534902640475103\n",
            "Epoch 101 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003318615526753805 \t\t Validation Loss: 0.08701352121380086\n",
            "Epoch 102 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004160105222458574 \t\t Validation Loss: 0.08961702283256902\n",
            "Epoch 103 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004341392368833358 \t\t Validation Loss: 0.09246936744938676\n",
            "Epoch 104 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004350094212772879 \t\t Validation Loss: 0.09276374685578048\n",
            "Epoch 105 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.005300555510345746 \t\t Validation Loss: 0.08982172425693044\n",
            "Epoch 106 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0037643930000076826 \t\t Validation Loss: 0.09275260008871555\n",
            "Epoch 107 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004419326870324644 \t\t Validation Loss: 0.0840284620734075\n",
            "\t\t Validation Loss Decreased(0.084650--->0.084028) \t Saving The Model\n",
            "Epoch 108 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003109612216625514 \t\t Validation Loss: 0.09078245439853233\n",
            "Epoch 109 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035833888325361987 \t\t Validation Loss: 0.07798685226589441\n",
            "\t\t Validation Loss Decreased(0.084028--->0.077987) \t Saving The Model\n",
            "Epoch 110 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030788283219334443 \t\t Validation Loss: 0.09336935685804257\n",
            "Epoch 111 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032764074118883423 \t\t Validation Loss: 0.07437005164459921\n",
            "\t\t Validation Loss Decreased(0.077987--->0.074370) \t Saving The Model\n",
            "Epoch 112 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003323016921058297 \t\t Validation Loss: 0.09749481210914943\n",
            "Epoch 113 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0037694395216169288 \t\t Validation Loss: 0.07224326247635943\n",
            "\t\t Validation Loss Decreased(0.074370--->0.072243) \t Saving The Model\n",
            "Epoch 114 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0037622664743606504 \t\t Validation Loss: 0.10146023515755168\n",
            "Epoch 115 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0038217595728063905 \t\t Validation Loss: 0.07163168531126128\n",
            "\t\t Validation Loss Decreased(0.072243--->0.071632) \t Saving The Model\n",
            "Epoch 116 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003502122869060652 \t\t Validation Loss: 0.1013324368530168\n",
            "Epoch 117 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035389269013083664 \t\t Validation Loss: 0.07501342081321546\n",
            "Epoch 118 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032532502428276114 \t\t Validation Loss: 0.0971883127752405\n",
            "Epoch 119 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029612592660595437 \t\t Validation Loss: 0.07414268353022635\n",
            "Epoch 120 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003719983612992675 \t\t Validation Loss: 0.09138649762966312\n",
            "Epoch 121 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030016370724911825 \t\t Validation Loss: 0.07314345718791279\n",
            "Epoch 122 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0035142415498911933 \t\t Validation Loss: 0.09207578863088901\n",
            "Epoch 123 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003094657009618508 \t\t Validation Loss: 0.0724226247626715\n",
            "Epoch 124 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031953282541016467 \t\t Validation Loss: 0.08890837627964523\n",
            "Epoch 125 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033997952390613186 \t\t Validation Loss: 0.07001499154676612\n",
            "\t\t Validation Loss Decreased(0.071632--->0.070015) \t Saving The Model\n",
            "Epoch 126 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033444098882830222 \t\t Validation Loss: 0.08699485294234294\n",
            "Epoch 127 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033949533449108334 \t\t Validation Loss: 0.07071596375093438\n",
            "Epoch 128 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031846252119017613 \t\t Validation Loss: 0.08338179052449189\n",
            "Epoch 129 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031290770212591096 \t\t Validation Loss: 0.06876058369660033\n",
            "\t\t Validation Loss Decreased(0.070015--->0.068761) \t Saving The Model\n",
            "Epoch 130 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002847326488385128 \t\t Validation Loss: 0.07564634022016364\n",
            "Epoch 131 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029912658485684645 \t\t Validation Loss: 0.06748479341443342\n",
            "\t\t Validation Loss Decreased(0.068761--->0.067485) \t Saving The Model\n",
            "Epoch 132 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029363966581562686 \t\t Validation Loss: 0.07219568260300618\n",
            "Epoch 133 \t\t Epoch time: 0m 11s\n",
            "\t\t Training Loss: 0.002921715168741758 \t\t Validation Loss: 0.06451429021902956\n",
            "\t\t Validation Loss Decreased(0.067485--->0.064514) \t Saving The Model\n",
            "Epoch 134 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003047847609048256 \t\t Validation Loss: 0.07122670026281132\n",
            "Epoch 135 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027559511835460325 \t\t Validation Loss: 0.06418514338572724\n",
            "\t\t Validation Loss Decreased(0.064514--->0.064185) \t Saving The Model\n",
            "Epoch 136 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027033135851819976 \t\t Validation Loss: 0.07004636477750655\n",
            "Epoch 137 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002943485300371272 \t\t Validation Loss: 0.06403718897714637\n",
            "\t\t Validation Loss Decreased(0.064185--->0.064037) \t Saving The Model\n",
            "Epoch 138 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027472854330758186 \t\t Validation Loss: 0.0657016306470793\n",
            "Epoch 139 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027507180426697675 \t\t Validation Loss: 0.06534586472508426\n",
            "Epoch 140 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027246028200622547 \t\t Validation Loss: 0.06556167863667585\n",
            "Epoch 141 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029218133949561395 \t\t Validation Loss: 0.06550800488688625\n",
            "Epoch 142 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002953138109611196 \t\t Validation Loss: 0.06159428976332912\n",
            "\t\t Validation Loss Decreased(0.064037--->0.061594) \t Saving The Model\n",
            "Epoch 143 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028855084642969275 \t\t Validation Loss: 0.06371053770327798\n",
            "Epoch 144 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002856293848929675 \t\t Validation Loss: 0.06074089870358316\n",
            "\t\t Validation Loss Decreased(0.061594--->0.060741) \t Saving The Model\n",
            "Epoch 145 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002855193831114652 \t\t Validation Loss: 0.06931255972729279\n",
            "Epoch 146 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0034196941084480167 \t\t Validation Loss: 0.060757803211275205\n",
            "Epoch 147 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028697181260213256 \t\t Validation Loss: 0.07121036790956098\n",
            "Epoch 148 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004221015496531854 \t\t Validation Loss: 0.06293065921188547\n",
            "Epoch 149 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031607933720020024 \t\t Validation Loss: 0.06896215833078784\n",
            "Epoch 150 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0042986901432930215 \t\t Validation Loss: 0.0629438422082995\n",
            "Epoch 151 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033341460464824295 \t\t Validation Loss: 0.0720319457602902\n",
            "Epoch 152 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004631338683523338 \t\t Validation Loss: 0.0635189653123514\n",
            "Epoch 153 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032437573992521378 \t\t Validation Loss: 0.0694017291892893\n",
            "Epoch 154 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003863282342806358 \t\t Validation Loss: 0.056798117190527804\n",
            "\t\t Validation Loss Decreased(0.060741--->0.056798) \t Saving The Model\n",
            "Epoch 155 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002870992674001467 \t\t Validation Loss: 0.07616485331135874\n",
            "Epoch 156 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003977430230824629 \t\t Validation Loss: 0.058140936540439725\n",
            "Epoch 157 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002940223678141027 \t\t Validation Loss: 0.07441290840506554\n",
            "Epoch 158 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003607434470747673 \t\t Validation Loss: 0.053975100826042205\n",
            "\t\t Validation Loss Decreased(0.056798--->0.053975) \t Saving The Model\n",
            "Epoch 159 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003123062580974924 \t\t Validation Loss: 0.08133871910663751\n",
            "Epoch 160 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003719221128817849 \t\t Validation Loss: 0.05608430617632201\n",
            "Epoch 161 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.004099632185455915 \t\t Validation Loss: 0.08023401199338528\n",
            "Epoch 162 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0036427145052978114 \t\t Validation Loss: 0.05459143709427176\n",
            "Epoch 163 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003751125916637279 \t\t Validation Loss: 0.07627713379378502\n",
            "Epoch 164 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003180638842939122 \t\t Validation Loss: 0.053275728660922214\n",
            "\t\t Validation Loss Decreased(0.053975--->0.053276) \t Saving The Model\n",
            "Epoch 165 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0036830220912658683 \t\t Validation Loss: 0.07226433950619629\n",
            "Epoch 166 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031271532580659195 \t\t Validation Loss: 0.05337483294379826\n",
            "Epoch 167 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003422119657861421 \t\t Validation Loss: 0.07223514909856021\n",
            "Epoch 168 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003374550840817392 \t\t Validation Loss: 0.053268024035227984\n",
            "\t\t Validation Loss Decreased(0.053276--->0.053268) \t Saving The Model\n",
            "Epoch 169 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003285360347945243 \t\t Validation Loss: 0.07904046211534968\n",
            "Epoch 170 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0033650164991126373 \t\t Validation Loss: 0.05528904784184236\n",
            "Epoch 171 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0036688081485949254 \t\t Validation Loss: 0.07303328876598524\n",
            "Epoch 172 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003244917732789307 \t\t Validation Loss: 0.05218904468123443\n",
            "\t\t Validation Loss Decreased(0.053268--->0.052189) \t Saving The Model\n",
            "Epoch 173 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0036301013724123303 \t\t Validation Loss: 0.07530850308159223\n",
            "Epoch 174 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003382131748367101 \t\t Validation Loss: 0.05602162180898281\n",
            "Epoch 175 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031348025832149025 \t\t Validation Loss: 0.06982204230287327\n",
            "Epoch 176 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032206544702922976 \t\t Validation Loss: 0.054060923983342946\n",
            "Epoch 177 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002834135394329457 \t\t Validation Loss: 0.06778944389393124\n",
            "Epoch 178 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003282963760776093 \t\t Validation Loss: 0.05479094018944754\n",
            "Epoch 179 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0030808228058614644 \t\t Validation Loss: 0.061587493401020765\n",
            "Epoch 180 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002872810643984357 \t\t Validation Loss: 0.05214453259339699\n",
            "\t\t Validation Loss Decreased(0.052189--->0.052145) \t Saving The Model\n",
            "Epoch 181 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003176853040271011 \t\t Validation Loss: 0.058180163524901636\n",
            "Epoch 182 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027850547150985614 \t\t Validation Loss: 0.05246488249395043\n",
            "Epoch 183 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0029201790473323214 \t\t Validation Loss: 0.06015911952664073\n",
            "Epoch 184 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028850398706020534 \t\t Validation Loss: 0.05331901798490435\n",
            "Epoch 185 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028040329579616316 \t\t Validation Loss: 0.05585596232245175\n",
            "Epoch 186 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028525813124016734 \t\t Validation Loss: 0.05259929206043195\n",
            "Epoch 187 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002674703725387116 \t\t Validation Loss: 0.06354382537448636\n",
            "Epoch 188 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003056253108346986 \t\t Validation Loss: 0.04920110995702159\n",
            "\t\t Validation Loss Decreased(0.052145--->0.049201) \t Saving The Model\n",
            "Epoch 189 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002751012082601822 \t\t Validation Loss: 0.05787922490316515\n",
            "Epoch 190 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0032968330181304467 \t\t Validation Loss: 0.04816694661545066\n",
            "\t\t Validation Loss Decreased(0.049201--->0.048167) \t Saving The Model\n",
            "Epoch 191 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002839899352890112 \t\t Validation Loss: 0.05646557937591122\n",
            "Epoch 192 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0031921057442731754 \t\t Validation Loss: 0.049051364740500085\n",
            "Epoch 193 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0028180793034718247 \t\t Validation Loss: 0.05435936178009097\n",
            "Epoch 194 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027671015012782772 \t\t Validation Loss: 0.04865224608399261\n",
            "Epoch 195 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0027981960368846114 \t\t Validation Loss: 0.05754286799436578\n",
            "Epoch 196 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.0034505337385485903 \t\t Validation Loss: 0.04402664085276998\n",
            "\t\t Validation Loss Decreased(0.048167--->0.044027) \t Saving The Model\n",
            "Epoch 197 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002667478666439451 \t\t Validation Loss: 0.05309691258634512\n",
            "Epoch 198 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003795759396854083 \t\t Validation Loss: 0.04870937446741244\n",
            "Epoch 199 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.002715953128613733 \t\t Validation Loss: 0.051606819194813185\n",
            "Epoch 200 \t\t Epoch time: 0m 8s\n",
            "\t\t Training Loss: 0.003172592639079871 \t\t Validation Loss: 0.049210175227087274\n",
            "\n",
            "Score: 4397.1202411167515\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjyBigAi0o9R",
        "outputId": "7493625a-34fb-4914-895e-1a1d0f102ae8"
      },
      "source": [
        "# check the complex results epoch to epoch\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "e = [100,199,101,198,139,199]\n",
        "for j in range(len(model_name_tmp)):\n",
        "  for k in range(e[j]):\n",
        "    print(f'{k}. epoch')\n",
        "\n",
        "    name = model_name_tmp[j]\n",
        "    model = model_complex[j]\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(f\"drive/MyDrive/complex/complex_models/{k}_model_{name}.pt\",map_location=torch.device('cpu')))\n",
        "\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_X_num)):\n",
        "            model.eval()\n",
        "            predict = model(test_X_num[i].reshape(1,-1).to(\"cpu\"), test_X_news[i].reshape(1,-1).to(\"cpu\"))        \n",
        "            predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "    real_test_array = []\n",
        "    real_trend_array = test[\"Trend target\"]  \n",
        "    for y in test_Y_num:\n",
        "      real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "    # calculate score\n",
        "    score = calc_score(predict_test_array,real_test_array)    \n",
        "    print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "    #check the trend -> create an array with the real and with the predicted trend\n",
        "    #if the current value is bigger than before -> 1\n",
        "    #otherwise (same or smaller) -> 0\n",
        "    predicted_trend_array = []\n",
        "    for element in range(len(predict_test_array)):\n",
        "        real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "        if real_today_close > predict_test_array[element].values:\n",
        "            predicted_trend_array.append(0)\n",
        "        else:\n",
        "            predicted_trend_array.append(1)\n",
        "\n",
        "    #check the number of differences\n",
        "    trend_diff_array = []\n",
        "    for element in range(len(real_trend_array)):\n",
        "        if real_trend_array[element] != predicted_trend_array[element]:\n",
        "          trend_diff_array.append(element)\n",
        "\n",
        "    #percentage of good predict\n",
        "    acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "    print(f\"Accuracy: {acc}\\n\")  \n",
        "    #visualize it\n",
        "    f2 = plt.figure(figsize=(24,12))\n",
        "    plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "    plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "            label = \"Real\")\n",
        "    plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "            label = \"Predicted\")\n",
        "    plt.xlabel(\"Date\",fontsize = 18)\n",
        "    plt.legend(fontsize = 18)\n",
        "    f2.set_size_inches(12,6)\n",
        "    plt_pred = f2\n",
        "    plt.close()\n",
        "\n",
        "    plt_pred.savefig('drive/MyDrive/complex/complex_models/'+str(k)+\"_log_pred_\" + str(name) + \"_.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0. epoch\n",
            "\n",
            "Score: 47233.82233502538\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 53255.97969543147\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 58794.72588832487\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 61972.26395939086\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 61940.46192893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 59525.17258883249\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 55945.47715736041\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 51849.857868020306\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 47470.137055837564\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 42965.3807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 38322.71319796954\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 33574.37055837563\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 28773.327411167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 23991.53299492386\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 19337.1230964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 14980.472081218275\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 11049.639593908629\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 7681.321065989848\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 5002.021256345178\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 3032.833121827411\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 1717.7630076142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 923.3834073604061\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 490.3080187182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 274.8184089467005\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 174.48320510786803\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 129.69118813451777\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 109.74204869923858\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 100.88972279505076\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 96.74749167195432\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 94.55602593591371\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 93.64735683692894\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 93.0670804251269\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 92.78373849936548\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 92.97569003807106\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 92.78105171319797\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 92.92569202093908\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 92.60646018401015\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 92.49361516497461\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 92.52685794733503\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 92.84267925126903\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 92.96959271890863\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 92.97110961294416\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 92.97251744923858\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 92.9941009676396\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 92.77552942576142\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 92.3372263642132\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 92.06638642131979\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 92.15579394035532\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 91.98840022208122\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 91.48193607233503\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 91.77831535532995\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 91.60146335659898\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 91.1966013642132\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 91.08280456852792\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 90.83389514593908\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 90.9492088356599\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 91.10077926713198\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 90.69686905139594\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 90.01088594543147\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 89.62170843908629\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 90.1243555678934\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 90.44288348667513\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 89.73222358819797\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 89.51183772208122\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 89.1100987468274\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 89.2402740323604\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 89.35978148794416\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 88.80746351522842\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 88.56361040609137\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 88.31916243654823\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 88.29140228426395\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 87.3710441782995\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 86.94055361675127\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 87.50202252538071\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 87.13388324873097\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 86.75568091687818\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 86.96584509835026\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 86.49450745558376\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 85.84131107233503\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 86.17197414340102\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 85.74216767131979\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 85.73156924175127\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 85.86813927664974\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 85.60992028870558\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 85.26576380076142\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 84.9029782677665\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 84.85791759200508\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 84.60982114530456\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 84.84726959073605\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 84.53390704314721\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 84.44713673857868\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 84.21126467322335\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 83.83648278870558\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 83.8248235247462\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 83.80755274428934\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 83.91263483502539\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 83.50116989213198\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 83.63114689086295\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 83.8912991751269\n",
            "\n",
            "Accuracy: 51.52284263959391\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 83.43130353743655\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 45205.84771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 47511.59390862944\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 49280.31979695432\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 49623.248730964464\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 47690.142131979694\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 43432.12182741117\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 37762.46446700508\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 31691.548223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 25792.39086294416\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 20317.76649746193\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 15407.568527918782\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 11166.997461928933\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 7669.4295685279185\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 4931.215418781726\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 2948.32614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 1621.4876269035533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 807.7983819796955\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 382.479100571066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 187.00686072335026\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 114.65503053616752\n",
            "\n",
            "Accuracy: 53.80710659898477\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 100.89007971129442\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 108.1686429251269\n",
            "\n",
            "Accuracy: 52.53807106598985\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 119.79876863895939\n",
            "\n",
            "Accuracy: 52.53807106598985\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 132.1574397208122\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 143.56328323286803\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 148.70090418781726\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 147.35708478743655\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 154.8424710501269\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 154.43176951142132\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 162.92205345812184\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 161.94959549492387\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 156.64033748413706\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 155.2414934961929\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 156.97237864847716\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 163.0287714149746\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 158.02839467005077\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 156.77363578680203\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 159.23271930520303\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 162.42827966370558\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 165.75776292829948\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 160.0943746034264\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 161.6269332963198\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 169.4680559961929\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 166.1049532043147\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 161.81171676713197\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 159.34581218274113\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 152.75853624682742\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 162.3301376110406\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 185.1652522208122\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 169.55550047588832\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 159.4088773001269\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 157.02301118337564\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 166.06526610088832\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 172.5703719860406\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 170.57590418781726\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 168.0944043464467\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 158.97414340101523\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 174.5749920685279\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 167.60576618020303\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 173.95090418781726\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 173.3541402284264\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 166.2342857709391\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 170.98423619923858\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 179.41977315989848\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 173.4757891814721\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 175.79899666878174\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 174.8750793147208\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 180.6255750317259\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 184.27595574238578\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 168.79435675761422\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 187.6667195431472\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 194.25337087563452\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 168.73090498096445\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 177.65254203680203\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 185.70982709390864\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 170.0520701142132\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 183.70471129441626\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 193.8339744606599\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 175.44255631345177\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 187.11433217005077\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 188.53547350888326\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 191.74716449873097\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 185.15902601522842\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 192.2480567893401\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 172.94907994923858\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 204.31541481598984\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 193.58688927664974\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 193.0019828680203\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 198.6125872461929\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 202.10961294416245\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 190.28178537436548\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 197.50426316624365\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 198.26132217639594\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 192.93286008883248\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 195.64415450507613\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 196.7317972715736\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 193.8749206852792\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 194.12571383248732\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 211.03731757614213\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 190.95998572335026\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "100. epoch\n",
            "\n",
            "Score: 202.55843512055839\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "101. epoch\n",
            "\n",
            "Score: 185.70217322335026\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "102. epoch\n",
            "\n",
            "Score: 195.51171875\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "103. epoch\n",
            "\n",
            "Score: 207.2057027284264\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "104. epoch\n",
            "\n",
            "Score: 195.4297668147208\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "105. epoch\n",
            "\n",
            "Score: 203.66437975888326\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "106. epoch\n",
            "\n",
            "Score: 204.2396097715736\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "107. epoch\n",
            "\n",
            "Score: 207.21986040609136\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "108. epoch\n",
            "\n",
            "Score: 186.1047747461929\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "109. epoch\n",
            "\n",
            "Score: 215.00140783629442\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "110. epoch\n",
            "\n",
            "Score: 179.67829949238578\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "111. epoch\n",
            "\n",
            "Score: 204.54871906725887\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "112. epoch\n",
            "\n",
            "Score: 175.86078283629442\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "113. epoch\n",
            "\n",
            "Score: 185.4895701142132\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "114. epoch\n",
            "\n",
            "Score: 186.86044574873097\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "115. epoch\n",
            "\n",
            "Score: 195.64865561548223\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "116. epoch\n",
            "\n",
            "Score: 164.17935041243655\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "117. epoch\n",
            "\n",
            "Score: 169.2362388959391\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "118. epoch\n",
            "\n",
            "Score: 177.1015823286802\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "119. epoch\n",
            "\n",
            "Score: 184.82645939086294\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "120. epoch\n",
            "\n",
            "Score: 176.81012055837564\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "121. epoch\n",
            "\n",
            "Score: 184.2779386104061\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "122. epoch\n",
            "\n",
            "Score: 161.14359930203045\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "123. epoch\n",
            "\n",
            "Score: 170.92258883248732\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "124. epoch\n",
            "\n",
            "Score: 154.46780813769035\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "125. epoch\n",
            "\n",
            "Score: 185.30032519035532\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "126. epoch\n",
            "\n",
            "Score: 160.6237904505076\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "127. epoch\n",
            "\n",
            "Score: 159.09581218274113\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "128. epoch\n",
            "\n",
            "Score: 162.52708597715736\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "129. epoch\n",
            "\n",
            "Score: 158.26672549175126\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "130. epoch\n",
            "\n",
            "Score: 185.27107788705584\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "131. epoch\n",
            "\n",
            "Score: 159.54773754758884\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "132. epoch\n",
            "\n",
            "Score: 150.17887452411168\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "133. epoch\n",
            "\n",
            "Score: 152.13252498413706\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "134. epoch\n",
            "\n",
            "Score: 152.92339189403555\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "135. epoch\n",
            "\n",
            "Score: 136.3833875317259\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "136. epoch\n",
            "\n",
            "Score: 145.64016894035532\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "137. epoch\n",
            "\n",
            "Score: 133.85642052664974\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "138. epoch\n",
            "\n",
            "Score: 137.6987131186548\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "139. epoch\n",
            "\n",
            "Score: 142.28483899111674\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "140. epoch\n",
            "\n",
            "Score: 138.0267290609137\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "141. epoch\n",
            "\n",
            "Score: 140.47896177030458\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "142. epoch\n",
            "\n",
            "Score: 140.65384081535532\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "143. epoch\n",
            "\n",
            "Score: 147.43178934010152\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "144. epoch\n",
            "\n",
            "Score: 125.8235049175127\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "145. epoch\n",
            "\n",
            "Score: 132.3443547747462\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "146. epoch\n",
            "\n",
            "Score: 124.38836453045685\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "147. epoch\n",
            "\n",
            "Score: 143.67130988261422\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "148. epoch\n",
            "\n",
            "Score: 125.65225452093908\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "149. epoch\n",
            "\n",
            "Score: 147.55158431154823\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "150. epoch\n",
            "\n",
            "Score: 132.00501665609136\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "151. epoch\n",
            "\n",
            "Score: 130.45680322017768\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "152. epoch\n",
            "\n",
            "Score: 118.68262214467005\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "153. epoch\n",
            "\n",
            "Score: 124.25743575507614\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "154. epoch\n",
            "\n",
            "Score: 133.21371351522842\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "155. epoch\n",
            "\n",
            "Score: 111.41038427982234\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "156. epoch\n",
            "\n",
            "Score: 117.12801395939087\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "157. epoch\n",
            "\n",
            "Score: 128.25640466370558\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "158. epoch\n",
            "\n",
            "Score: 116.87770661484771\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "159. epoch\n",
            "\n",
            "Score: 116.16260509200508\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "160. epoch\n",
            "\n",
            "Score: 128.75407479378174\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "161. epoch\n",
            "\n",
            "Score: 117.52495439403553\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "162. epoch\n",
            "\n",
            "Score: 133.1042790291878\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "163. epoch\n",
            "\n",
            "Score: 112.26605131662437\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "164. epoch\n",
            "\n",
            "Score: 120.9121490323604\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "165. epoch\n",
            "\n",
            "Score: 119.23884636738579\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "166. epoch\n",
            "\n",
            "Score: 124.22994328997461\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "167. epoch\n",
            "\n",
            "Score: 122.37033034581218\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "168. epoch\n",
            "\n",
            "Score: 118.46690593274111\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "169. epoch\n",
            "\n",
            "Score: 118.73596129441624\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "170. epoch\n",
            "\n",
            "Score: 113.24616315038071\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "171. epoch\n",
            "\n",
            "Score: 105.65088634200508\n",
            "\n",
            "Accuracy: 52.53807106598985\n",
            "\n",
            "172. epoch\n",
            "\n",
            "Score: 120.77752220812182\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "173. epoch\n",
            "\n",
            "Score: 109.30813769035532\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "174. epoch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN7SkH4k4v5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15db02ea-f851-4d7b-956b-e86e564eb80c"
      },
      "source": [
        "# check the complex results epoch to epoch\n",
        "complex_name_tmp = [\"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"LSTM_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "PAD_ID = vocab.lookup_indices([\"<pad>\"])[0]\n",
        "model_news = LSTM_news(len(vocab),PAD_ID)\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/26_model_LSTM_pilot.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "e = [100,199,101,198,139,199]\n",
        "for j in range(1,len(model_name_tmp)):\n",
        "  if j == 1:\n",
        "      for k in range(174,e[j]):\n",
        "        print(f'{k}. epoch')\n",
        "\n",
        "        name = model_name_tmp[j]\n",
        "        model = model_complex[j]\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.load_state_dict(torch.load(f\"drive/MyDrive/complex/complex_models/{k}_model_{name}.pt\",map_location=torch.device('cpu')))\n",
        "\n",
        "        predict_test_array = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(test_X_num)):\n",
        "                model.eval()\n",
        "                predict = model(test_X_num[i].reshape(1,-1).to(\"cpu\"), test_X_news[i].reshape(1,-1).to(\"cpu\"))        \n",
        "                predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "        real_test_array = []\n",
        "        real_trend_array = test[\"Trend target\"]  \n",
        "        for y in test_Y_num:\n",
        "          real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "        # calculate score\n",
        "        score = calc_score(predict_test_array,real_test_array)    \n",
        "        print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "        #check the trend -> create an array with the real and with the predicted trend\n",
        "        #if the current value is bigger than before -> 1\n",
        "        #otherwise (same or smaller) -> 0\n",
        "        predicted_trend_array = []\n",
        "        for element in range(len(predict_test_array)):\n",
        "            real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "            if real_today_close > predict_test_array[element].values:\n",
        "                predicted_trend_array.append(0)\n",
        "            else:\n",
        "                predicted_trend_array.append(1)\n",
        "\n",
        "        #check the number of differences\n",
        "        trend_diff_array = []\n",
        "        for element in range(len(real_trend_array)):\n",
        "            if real_trend_array[element] != predicted_trend_array[element]:\n",
        "              trend_diff_array.append(element)\n",
        "\n",
        "        #percentage of good predict\n",
        "        acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "        print(f\"Accuracy: {acc}\\n\")  \n",
        "        #visualize it\n",
        "        f2 = plt.figure(figsize=(24,12))\n",
        "        plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "        plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "                label = \"Real\")\n",
        "        plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "                label = \"Predicted\")\n",
        "        plt.xlabel(\"Date\",fontsize = 18)\n",
        "        plt.legend(fontsize = 18)\n",
        "        f2.set_size_inches(12,6)\n",
        "        plt_pred = f2\n",
        "        plt.close()\n",
        "\n",
        "        plt_pred.savefig('drive/MyDrive/complex/complex_models/'+str(k)+\"_log_pred_\" + str(name) + \"_.png\")\n",
        "  else:\n",
        "      for k in range(e[j]):\n",
        "        print(f'{k}. epoch')\n",
        "\n",
        "        name = model_name_tmp[j]\n",
        "        model = model_complex[j]\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.load_state_dict(torch.load(f\"drive/MyDrive/complex/complex_models/{k}_model_{name}.pt\",map_location=torch.device('cpu')))\n",
        "\n",
        "        predict_test_array = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(test_X_num)):\n",
        "                model.eval()\n",
        "                predict = model(test_X_num[i].reshape(1,-1).to(\"cpu\"), test_X_news[i].reshape(1,-1).to(\"cpu\"))        \n",
        "                predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "        real_test_array = []\n",
        "        real_trend_array = test[\"Trend target\"]  \n",
        "        for y in test_Y_num:\n",
        "          real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "        # calculate score\n",
        "        score = calc_score(predict_test_array,real_test_array)    \n",
        "        print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "        #check the trend -> create an array with the real and with the predicted trend\n",
        "        #if the current value is bigger than before -> 1\n",
        "        #otherwise (same or smaller) -> 0\n",
        "        predicted_trend_array = []\n",
        "        for element in range(len(predict_test_array)):\n",
        "            real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "            if real_today_close > predict_test_array[element].values:\n",
        "                predicted_trend_array.append(0)\n",
        "            else:\n",
        "                predicted_trend_array.append(1)\n",
        "\n",
        "        #check the number of differences\n",
        "        trend_diff_array = []\n",
        "        for element in range(len(real_trend_array)):\n",
        "            if real_trend_array[element] != predicted_trend_array[element]:\n",
        "              trend_diff_array.append(element)\n",
        "\n",
        "        #percentage of good predict\n",
        "        acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "        print(f\"Accuracy: {acc}\\n\")  \n",
        "        #visualize it\n",
        "        f2 = plt.figure(figsize=(24,12))\n",
        "        plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "        plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "                label = \"Real\")\n",
        "        plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "                label = \"Predicted\")\n",
        "        plt.xlabel(\"Date\",fontsize = 18)\n",
        "        plt.legend(fontsize = 18)\n",
        "        f2.set_size_inches(12,6)\n",
        "        plt_pred = f2\n",
        "        plt.close()\n",
        "\n",
        "        plt_pred.savefig('drive/MyDrive/complex/complex_models/'+str(k)+\"_log_pred_\" + str(name) + \"_.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "174. epoch\n",
            "\n",
            "Score: 126.62183732550761\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "175. epoch\n",
            "\n",
            "Score: 108.59615918464468\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "176. epoch\n",
            "\n",
            "Score: 129.5793742068528\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "177. epoch\n",
            "\n",
            "Score: 108.50812975888324\n",
            "\n",
            "Accuracy: 52.53807106598985\n",
            "\n",
            "178. epoch\n",
            "\n",
            "Score: 114.12445471129442\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "179. epoch\n",
            "\n",
            "Score: 109.57637016180203\n",
            "\n",
            "Accuracy: 52.28426395939086\n",
            "\n",
            "180. epoch\n",
            "\n",
            "Score: 110.03932027284264\n",
            "\n",
            "Accuracy: 52.03045685279187\n",
            "\n",
            "181. epoch\n",
            "\n",
            "Score: 107.94359731916244\n",
            "\n",
            "Accuracy: 53.045685279187815\n",
            "\n",
            "182. epoch\n",
            "\n",
            "Score: 115.9270800285533\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "183. epoch\n",
            "\n",
            "Score: 116.12994725571066\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "184. epoch\n",
            "\n",
            "Score: 115.70664459073605\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "185. epoch\n",
            "\n",
            "Score: 120.6816505393401\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "186. epoch\n",
            "\n",
            "Score: 116.84865759835026\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "187. epoch\n",
            "\n",
            "Score: 119.95931154822335\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "188. epoch\n",
            "\n",
            "Score: 127.63180123730965\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "189. epoch\n",
            "\n",
            "Score: 126.53600888324873\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "190. epoch\n",
            "\n",
            "Score: 131.57898754758884\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "191. epoch\n",
            "\n",
            "Score: 123.86280536167513\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "192. epoch\n",
            "\n",
            "Score: 141.25640466370558\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "193. epoch\n",
            "\n",
            "Score: 117.95720970812182\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "194. epoch\n",
            "\n",
            "Score: 118.53855686865482\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "195. epoch\n",
            "\n",
            "Score: 117.78471010469544\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "196. epoch\n",
            "\n",
            "Score: 115.48927268401015\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "197. epoch\n",
            "\n",
            "Score: 120.600878410533\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "198. epoch\n",
            "\n",
            "Score: 121.7134854854061\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 43979.43147208122\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 45697.91370558376\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 46916.63959390863\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 47434.56345177665\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 46631.93908629441\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 44086.68020304568\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 39884.91370558376\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 34726.99492385787\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 29420.04314720812\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 24391.256345177666\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 19841.58375634518\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 15851.732233502538\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 12329.248730964468\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 9366.151015228426\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 6900.223984771574\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 4982.2017766497465\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 3493.5437817258885\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 2399.1611675126906\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 1660.3156725888325\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 1119.7659422588833\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 762.6426078680203\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 555.800682106599\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 406.92346129441626\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 332.37002300126903\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 277.5240521890863\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 241.31366989213197\n",
            "\n",
            "Accuracy: 48.47715736040609\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 215.67800206218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 205.37737944162436\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 198.70639673223351\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 191.45665450507613\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 188.27153394670052\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 190.33088118654823\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 184.32965180837564\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 179.06799254441626\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 175.82404029187816\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 177.64425364847716\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 177.6256741751269\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 184.84265942258884\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 183.69247699873097\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 172.117088356599\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 186.17883486675126\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 183.83452966370558\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 177.74700586928935\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 177.81945986675126\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 175.1453045685279\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 183.30577807741116\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 180.43660770939087\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 184.38525142766497\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 184.32856123096445\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 187.5331337246193\n",
            "\n",
            "Accuracy: 48.47715736040609\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 186.4622858502538\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 178.4491989213198\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 174.1237507931472\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 171.50354933375635\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 180.36211135786803\n",
            "\n",
            "Accuracy: 48.47715736040609\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 176.06331297588832\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 168.23500951776649\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 180.33887214467006\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 181.50922033629442\n",
            "\n",
            "Accuracy: 48.47715736040609\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 176.82330663071065\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 168.48590180837564\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 178.71512135152284\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 175.78610802664974\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 177.26846050126903\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 169.85152284263958\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 176.44172350888326\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 175.3131741751269\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 170.86300364847716\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 179.11280536167513\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 174.8039340101523\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 178.16870241116752\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 178.5775499682741\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 162.9028493813452\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 166.39353981598984\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 169.4940712246193\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 176.70366037436548\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 164.45917274746193\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 173.39292512690355\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 176.02359612944161\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 168.72642369923858\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 175.4076974936548\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 168.38253489847716\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 182.14625634517768\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 164.32733185279187\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 162.48357193845177\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 167.38172192258884\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 157.07959232233503\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 171.08690910532994\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 163.7726641814721\n",
            "\n",
            "Accuracy: 51.01522842639594\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 156.2720891497462\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 165.98402799809645\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 179.28789260786803\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 158.06792314403555\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 160.5590993813452\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 166.16484573286803\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 159.24207844225887\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 159.29673619923858\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 170.6920209390863\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 165.4360723350254\n",
            "\n",
            "Accuracy: 50.76142131979695\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 161.0573346288071\n",
            "\n",
            "Accuracy: 50.25380710659898\n",
            "\n",
            "100. epoch\n",
            "\n",
            "Score: 160.72486714784264\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 53275.48223350254\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 59564.03553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 63861.00507614213\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 58376.22335025381\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 50462.52284263959\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 42766.02538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 34964.210659898476\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 27853.862944162436\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 24368.261421319796\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 24108.05583756345\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 23464.093908629442\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 22420.195431472082\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 21479.60913705584\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 20601.08883248731\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 19734.73350253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 18911.819796954314\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 18188.548223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 17530.25\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 16912.988578680204\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 16339.194162436548\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 15808.808375634519\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 15304.242385786802\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 14853.857868020305\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 14457.876903553299\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 14056.860406091371\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 13693.357868020305\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 13373.267766497462\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 13085.50888324873\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 12791.709390862945\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 12540.001269035532\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 12279.24111675127\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 12034.4961928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 11850.925126903554\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 11677.668781725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 11457.413705583756\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 11289.151015228426\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 11117.365482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 10926.234771573603\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 10754.52918781726\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 10570.290609137055\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 10433.043781725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 10224.661802030458\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 10083.431472081218\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 9934.988578680202\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 9773.47081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 9626.10850253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 9509.95812182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 9364.274746192894\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 9294.723350253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 9159.331218274112\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 9078.550126903554\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 8941.342005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 8866.505076142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 8766.030456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 8695.100888324872\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 8603.732868020305\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 8562.159263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 8468.173223350253\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 8428.268401015228\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 8335.52538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 8296.448604060914\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 8231.835025380711\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 8161.883248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 8142.598984771574\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 8061.08692893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 7998.927030456853\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 7940.780456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 7894.033629441625\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 7850.007614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 7825.115482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 7755.884517766497\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 7711.495558375635\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 7715.083121827412\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 7679.786167512691\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 7592.653553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 7601.355329949239\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 7603.4232233502535\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 7498.9796954314725\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 7506.6548223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 7549.953680203046\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 7524.347081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 7518.579949238579\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 7597.284898477157\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 7613.730964467005\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 7709.52538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 7897.912436548223\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 8110.6548223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 8243.16687817259\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 8449.624365482234\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 8088.907994923858\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 8187.7328680203045\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 7896.416878172588\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 7803.638324873096\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 7526.633248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 7578.583756345178\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 7397.881345177665\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 7370.842005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 7219.9765228426395\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 7198.8578680203045\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 7065.170050761421\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "100. epoch\n",
            "\n",
            "Score: 7029.968274111675\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "101. epoch\n",
            "\n",
            "Score: 6883.355964467005\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "102. epoch\n",
            "\n",
            "Score: 6899.38578680203\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "103. epoch\n",
            "\n",
            "Score: 6733.934010152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "104. epoch\n",
            "\n",
            "Score: 6721.506979695431\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "105. epoch\n",
            "\n",
            "Score: 6623.26269035533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "106. epoch\n",
            "\n",
            "Score: 6585.997461928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "107. epoch\n",
            "\n",
            "Score: 6556.906091370558\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "108. epoch\n",
            "\n",
            "Score: 6472.637055837564\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "109. epoch\n",
            "\n",
            "Score: 6472.71002538071\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "110. epoch\n",
            "\n",
            "Score: 6446.0767766497465\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "111. epoch\n",
            "\n",
            "Score: 6391.044416243655\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "112. epoch\n",
            "\n",
            "Score: 6344.004441624365\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "113. epoch\n",
            "\n",
            "Score: 6363.7201776649745\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "114. epoch\n",
            "\n",
            "Score: 6319.366116751269\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "115. epoch\n",
            "\n",
            "Score: 6295.247461928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "116. epoch\n",
            "\n",
            "Score: 6268.430837563452\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "117. epoch\n",
            "\n",
            "Score: 6337.302664974619\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "118. epoch\n",
            "\n",
            "Score: 6299.61230964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "119. epoch\n",
            "\n",
            "Score: 6305.904187817259\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "120. epoch\n",
            "\n",
            "Score: 6346.642766497462\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "121. epoch\n",
            "\n",
            "Score: 6333.805203045686\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "122. epoch\n",
            "\n",
            "Score: 6447.909263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "123. epoch\n",
            "\n",
            "Score: 6387.673857868021\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "124. epoch\n",
            "\n",
            "Score: 6544.418147208122\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "125. epoch\n",
            "\n",
            "Score: 6316.467005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "126. epoch\n",
            "\n",
            "Score: 6482.569162436548\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "127. epoch\n",
            "\n",
            "Score: 6092.357233502538\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "128. epoch\n",
            "\n",
            "Score: 6377.712563451777\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "129. epoch\n",
            "\n",
            "Score: 5708.807106598984\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "130. epoch\n",
            "\n",
            "Score: 6320.128807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "131. epoch\n",
            "\n",
            "Score: 5494.897208121827\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "132. epoch\n",
            "\n",
            "Score: 6344.717639593909\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "133. epoch\n",
            "\n",
            "Score: 5438.66307106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "134. epoch\n",
            "\n",
            "Score: 6430.8769035533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "135. epoch\n",
            "\n",
            "Score: 5344.464467005077\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "136. epoch\n",
            "\n",
            "Score: 6422.4232233502535\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "137. epoch\n",
            "\n",
            "Score: 5320.841370558375\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "138. epoch\n",
            "\n",
            "Score: 6432.861675126904\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "139. epoch\n",
            "\n",
            "Score: 5279.776967005077\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "140. epoch\n",
            "\n",
            "Score: 6459.008883248731\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "141. epoch\n",
            "\n",
            "Score: 5265.3515228426395\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "142. epoch\n",
            "\n",
            "Score: 6385.946065989848\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "143. epoch\n",
            "\n",
            "Score: 5269.139911167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "144. epoch\n",
            "\n",
            "Score: 6322.960659898477\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "145. epoch\n",
            "\n",
            "Score: 5248.497144670051\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "146. epoch\n",
            "\n",
            "Score: 6241.28807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "147. epoch\n",
            "\n",
            "Score: 5235.5298223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "148. epoch\n",
            "\n",
            "Score: 6160.133248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "149. epoch\n",
            "\n",
            "Score: 5224.065038071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "150. epoch\n",
            "\n",
            "Score: 6095.5298223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "151. epoch\n",
            "\n",
            "Score: 5227.994606598984\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "152. epoch\n",
            "\n",
            "Score: 6048.258248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "153. epoch\n",
            "\n",
            "Score: 5217.306154822335\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "154. epoch\n",
            "\n",
            "Score: 5965.52728426396\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "155. epoch\n",
            "\n",
            "Score: 5193.972081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "156. epoch\n",
            "\n",
            "Score: 5855.291243654822\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "157. epoch\n",
            "\n",
            "Score: 5177.018401015229\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "158. epoch\n",
            "\n",
            "Score: 5781.83692893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "159. epoch\n",
            "\n",
            "Score: 5213.4578045685275\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "160. epoch\n",
            "\n",
            "Score: 5689.602157360406\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "161. epoch\n",
            "\n",
            "Score: 5228.319479695431\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "162. epoch\n",
            "\n",
            "Score: 5578.51078680203\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "163. epoch\n",
            "\n",
            "Score: 5223.778553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "164. epoch\n",
            "\n",
            "Score: 5436.302664974619\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "165. epoch\n",
            "\n",
            "Score: 5192.003172588832\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "166. epoch\n",
            "\n",
            "Score: 5373.0\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "167. epoch\n",
            "\n",
            "Score: 5210.981281725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "168. epoch\n",
            "\n",
            "Score: 5243.85247461929\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "169. epoch\n",
            "\n",
            "Score: 5224.3797588832485\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "170. epoch\n",
            "\n",
            "Score: 5122.16211928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "171. epoch\n",
            "\n",
            "Score: 5154.7452411167515\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "172. epoch\n",
            "\n",
            "Score: 5093.442576142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "173. epoch\n",
            "\n",
            "Score: 5104.515862944162\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "174. epoch\n",
            "\n",
            "Score: 5043.3359771573605\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "175. epoch\n",
            "\n",
            "Score: 5034.384517766497\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "176. epoch\n",
            "\n",
            "Score: 4969.479060913705\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "177. epoch\n",
            "\n",
            "Score: 4954.771573604061\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "178. epoch\n",
            "\n",
            "Score: 4990.835659898477\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "179. epoch\n",
            "\n",
            "Score: 4940.1671954314725\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "180. epoch\n",
            "\n",
            "Score: 4932.500317258883\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "181. epoch\n",
            "\n",
            "Score: 4927.592005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "182. epoch\n",
            "\n",
            "Score: 4899.153553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "183. epoch\n",
            "\n",
            "Score: 4909.691941624365\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "184. epoch\n",
            "\n",
            "Score: 4951.213515228426\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "185. epoch\n",
            "\n",
            "Score: 4978.777601522843\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "186. epoch\n",
            "\n",
            "Score: 4943.632614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "187. epoch\n",
            "\n",
            "Score: 4976.500951776649\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "188. epoch\n",
            "\n",
            "Score: 5049.220812182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "189. epoch\n",
            "\n",
            "Score: 5114.3296319796955\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "190. epoch\n",
            "\n",
            "Score: 5200.317576142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "191. epoch\n",
            "\n",
            "Score: 5215.802982233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "192. epoch\n",
            "\n",
            "Score: 5316.115482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "193. epoch\n",
            "\n",
            "Score: 5110.23730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "194. epoch\n",
            "\n",
            "Score: 5097.9454314720815\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "195. epoch\n",
            "\n",
            "Score: 4721.4375\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "196. epoch\n",
            "\n",
            "Score: 4859.768083756345\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "197. epoch\n",
            "\n",
            "Score: 4341.8578680203045\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 62341.48730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 65202.20812182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 63242.43654822335\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 55505.37563451777\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 46885.43147208122\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 36572.37563451777\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 26539.761421319796\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 22697.931472081218\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 22000.31725888325\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 20238.22461928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 18673.475888324872\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 17374.450507614212\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 16114.794416243654\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 14873.578680203045\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 13895.294416243654\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 13133.125634517766\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 12303.89847715736\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 11741.614213197969\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 11206.469543147208\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 10712.247461928933\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 10347.461294416244\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 10026.827411167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 9674.443527918782\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 9385.297588832487\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 9135.690355329949\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 8848.205583756346\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 8751.895939086295\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 8462.479695431472\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 8171.756979695431\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 8101.781091370558\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 7834.86230964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 7835.127538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 7581.880076142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 7427.197335025381\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 7381.511421319797\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 7111.994923857868\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 7073.4232233502535\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 7110.276649746193\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 6776.9390862944165\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 6753.597081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 6698.111675126904\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 6631.006345177665\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 6616.935279187817\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 6508.4422588832485\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 6371.314720812183\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 6383.285532994923\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 6330.766497461929\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 6311.4422588832485\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 6014.852791878173\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 6188.059010152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 6018.308375634518\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 5956.423857868021\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 6134.0\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 5946.293147208122\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 5978.61040609137\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 5892.3267766497465\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 5906.727791878173\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 5720.821700507614\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 5987.984771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 5671.184010152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 5972.704314720812\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 5738.958121827412\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 5906.112944162436\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 5643.418781725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 5760.670050761421\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 5593.194162436548\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 5643.463197969543\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 5586.364847715736\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 5607.621192893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 5458.83502538071\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 5493.703680203046\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 5687.310279187817\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 5369.187817258883\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 5401.723984771574\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 5463.375\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 5149.95209390863\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 5511.119289340101\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 5398.53997461929\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 5419.321700507614\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 5079.611992385787\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 5695.684010152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 5390.463197969543\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 5302.1827411167515\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 5323.92576142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 5495.715101522843\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 5365.88578680203\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 5404.156725888325\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 5381.2798223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 5397.803299492386\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 5376.5019035533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 5415.868020304569\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 5299.059644670051\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 5254.098667512691\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 5357.285532994923\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 5168.693527918782\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 5348.659898477157\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 5218.258565989848\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 5473.307106598984\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 5148.871510152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 5709.138324873096\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "100. epoch\n",
            "\n",
            "Score: 4956.299492385787\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "101. epoch\n",
            "\n",
            "Score: 5934.611675126904\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "102. epoch\n",
            "\n",
            "Score: 4903.258248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "103. epoch\n",
            "\n",
            "Score: 6117.615482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "104. epoch\n",
            "\n",
            "Score: 4798.430203045686\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "105. epoch\n",
            "\n",
            "Score: 6612.122461928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "106. epoch\n",
            "\n",
            "Score: 4906.566941624365\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "107. epoch\n",
            "\n",
            "Score: 6935.331218274112\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "108. epoch\n",
            "\n",
            "Score: 4912.300444162436\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "109. epoch\n",
            "\n",
            "Score: 6783.633248730965\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "110. epoch\n",
            "\n",
            "Score: 5043.116751269035\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "111. epoch\n",
            "\n",
            "Score: 6124.053934010152\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "112. epoch\n",
            "\n",
            "Score: 5130.119289340101\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "113. epoch\n",
            "\n",
            "Score: 5447.365482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "114. epoch\n",
            "\n",
            "Score: 4933.63959390863\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "115. epoch\n",
            "\n",
            "Score: 5324.965101522843\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "116. epoch\n",
            "\n",
            "Score: 4713.711611675127\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "117. epoch\n",
            "\n",
            "Score: 5155.25\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "118. epoch\n",
            "\n",
            "Score: 4678.671002538071\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "119. epoch\n",
            "\n",
            "Score: 5000.151015228426\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "120. epoch\n",
            "\n",
            "Score: 4684.494923857868\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "121. epoch\n",
            "\n",
            "Score: 5022.875\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "122. epoch\n",
            "\n",
            "Score: 4398.112626903553\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "123. epoch\n",
            "\n",
            "Score: 5186.290609137056\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "124. epoch\n",
            "\n",
            "Score: 4325.263007614213\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "125. epoch\n",
            "\n",
            "Score: 5149.234771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "126. epoch\n",
            "\n",
            "Score: 4274.574555837564\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "127. epoch\n",
            "\n",
            "Score: 5204.971763959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "128. epoch\n",
            "\n",
            "Score: 4217.106598984771\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "129. epoch\n",
            "\n",
            "Score: 5485.394670050761\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "130. epoch\n",
            "\n",
            "Score: 4231.213197969543\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "131. epoch\n",
            "\n",
            "Score: 5606.208756345178\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "132. epoch\n",
            "\n",
            "Score: 4047.4571700507613\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "133. epoch\n",
            "\n",
            "Score: 5842.869923857868\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "134. epoch\n",
            "\n",
            "Score: 4129.348032994923\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "135. epoch\n",
            "\n",
            "Score: 5475.334390862944\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "136. epoch\n",
            "\n",
            "Score: 4066.416878172589\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "137. epoch\n",
            "\n",
            "Score: 5066.809010152284\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "138. epoch\n",
            "\n",
            "Score: 4137.114530456853\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "0. epoch\n",
            "\n",
            "Score: 47033.8883248731\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "1. epoch\n",
            "\n",
            "Score: 56947.15228426396\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "2. epoch\n",
            "\n",
            "Score: 62633.548223350255\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "3. epoch\n",
            "\n",
            "Score: 55660.09644670051\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "4. epoch\n",
            "\n",
            "Score: 47527.289340101524\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "5. epoch\n",
            "\n",
            "Score: 41964.802030456856\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "6. epoch\n",
            "\n",
            "Score: 37356.60659898477\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "7. epoch\n",
            "\n",
            "Score: 33192.10659898477\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "8. epoch\n",
            "\n",
            "Score: 29847.413705583756\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "9. epoch\n",
            "\n",
            "Score: 28416.723350253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "10. epoch\n",
            "\n",
            "Score: 27236.289340101524\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "11. epoch\n",
            "\n",
            "Score: 25763.687817258884\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "12. epoch\n",
            "\n",
            "Score: 24520.15228426396\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "13. epoch\n",
            "\n",
            "Score: 23374.352791878173\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "14. epoch\n",
            "\n",
            "Score: 22271.692893401014\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "15. epoch\n",
            "\n",
            "Score: 21196.002538071065\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "16. epoch\n",
            "\n",
            "Score: 20248.501269035532\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "17. epoch\n",
            "\n",
            "Score: 19319.450507614212\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "18. epoch\n",
            "\n",
            "Score: 18481.095177664974\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "19. epoch\n",
            "\n",
            "Score: 17688.26776649746\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "20. epoch\n",
            "\n",
            "Score: 16918.824873096448\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "21. epoch\n",
            "\n",
            "Score: 16287.39847715736\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "22. epoch\n",
            "\n",
            "Score: 15764.092639593908\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "23. epoch\n",
            "\n",
            "Score: 15129.307106598984\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "24. epoch\n",
            "\n",
            "Score: 14518.615482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "25. epoch\n",
            "\n",
            "Score: 13951.696700507615\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "26. epoch\n",
            "\n",
            "Score: 13454.827411167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "27. epoch\n",
            "\n",
            "Score: 13226.52918781726\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "28. epoch\n",
            "\n",
            "Score: 12915.798223350253\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "29. epoch\n",
            "\n",
            "Score: 12580.24111675127\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "30. epoch\n",
            "\n",
            "Score: 12330.821065989847\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "31. epoch\n",
            "\n",
            "Score: 11980.47461928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "32. epoch\n",
            "\n",
            "Score: 11690.467005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "33. epoch\n",
            "\n",
            "Score: 11496.284263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "34. epoch\n",
            "\n",
            "Score: 11345.72081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "35. epoch\n",
            "\n",
            "Score: 11074.314720812183\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "36. epoch\n",
            "\n",
            "Score: 10884.60152284264\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "37. epoch\n",
            "\n",
            "Score: 10773.652284263959\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "38. epoch\n",
            "\n",
            "Score: 10501.584390862945\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "39. epoch\n",
            "\n",
            "Score: 10269.217005076142\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "40. epoch\n",
            "\n",
            "Score: 10299.440355329949\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "41. epoch\n",
            "\n",
            "Score: 9961.836294416244\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "42. epoch\n",
            "\n",
            "Score: 9758.948604060914\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "43. epoch\n",
            "\n",
            "Score: 9828.58312182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "44. epoch\n",
            "\n",
            "Score: 9744.647842639593\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "45. epoch\n",
            "\n",
            "Score: 9489.70812182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "46. epoch\n",
            "\n",
            "Score: 9574.376269035532\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "47. epoch\n",
            "\n",
            "Score: 9434.262055837564\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "48. epoch\n",
            "\n",
            "Score: 9440.538705583756\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "49. epoch\n",
            "\n",
            "Score: 9150.831852791878\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "50. epoch\n",
            "\n",
            "Score: 9104.981598984772\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "51. epoch\n",
            "\n",
            "Score: 9262.754441624365\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "52. epoch\n",
            "\n",
            "Score: 8863.03616751269\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "53. epoch\n",
            "\n",
            "Score: 9099.189086294416\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "54. epoch\n",
            "\n",
            "Score: 8825.10850253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "55. epoch\n",
            "\n",
            "Score: 8876.996827411167\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "56. epoch\n",
            "\n",
            "Score: 8731.421319796955\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "57. epoch\n",
            "\n",
            "Score: 8771.543781725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "58. epoch\n",
            "\n",
            "Score: 8519.113578680202\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "59. epoch\n",
            "\n",
            "Score: 8686.678934010153\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "60. epoch\n",
            "\n",
            "Score: 8466.303934010153\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "61. epoch\n",
            "\n",
            "Score: 8903.780456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "62. epoch\n",
            "\n",
            "Score: 8399.260786802031\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "63. epoch\n",
            "\n",
            "Score: 8845.256345177664\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "64. epoch\n",
            "\n",
            "Score: 8425.83692893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "65. epoch\n",
            "\n",
            "Score: 8568.159263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "66. epoch\n",
            "\n",
            "Score: 8383.194162436548\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "67. epoch\n",
            "\n",
            "Score: 8721.212563451776\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "68. epoch\n",
            "\n",
            "Score: 8352.363578680202\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "69. epoch\n",
            "\n",
            "Score: 8390.185913705584\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "70. epoch\n",
            "\n",
            "Score: 8245.480329949238\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "71. epoch\n",
            "\n",
            "Score: 8579.114847715737\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "72. epoch\n",
            "\n",
            "Score: 8131.371827411168\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "73. epoch\n",
            "\n",
            "Score: 8781.412436548224\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "74. epoch\n",
            "\n",
            "Score: 8192.414974619289\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "75. epoch\n",
            "\n",
            "Score: 8793.359137055837\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "76. epoch\n",
            "\n",
            "Score: 7967.870558375635\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "77. epoch\n",
            "\n",
            "Score: 8694.055837563452\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "78. epoch\n",
            "\n",
            "Score: 8101.28807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "79. epoch\n",
            "\n",
            "Score: 8592.822335025381\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "80. epoch\n",
            "\n",
            "Score: 8160.259517766497\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "81. epoch\n",
            "\n",
            "Score: 8218.540609137055\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "82. epoch\n",
            "\n",
            "Score: 8096.484771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "83. epoch\n",
            "\n",
            "Score: 8343.65038071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "84. epoch\n",
            "\n",
            "Score: 8041.220812182741\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "85. epoch\n",
            "\n",
            "Score: 7961.248730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "86. epoch\n",
            "\n",
            "Score: 8150.412436548223\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "87. epoch\n",
            "\n",
            "Score: 8191.208121827412\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "88. epoch\n",
            "\n",
            "Score: 7889.396573604061\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "89. epoch\n",
            "\n",
            "Score: 7955.348350253807\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "90. epoch\n",
            "\n",
            "Score: 7849.765228426396\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "91. epoch\n",
            "\n",
            "Score: 7975.424492385787\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "92. epoch\n",
            "\n",
            "Score: 7917.044416243655\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "93. epoch\n",
            "\n",
            "Score: 7859.814720812183\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "94. epoch\n",
            "\n",
            "Score: 7742.546954314721\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "95. epoch\n",
            "\n",
            "Score: 7954.153553299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "96. epoch\n",
            "\n",
            "Score: 7721.259517766497\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "97. epoch\n",
            "\n",
            "Score: 7908.416243654822\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "98. epoch\n",
            "\n",
            "Score: 7763.967639593909\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "99. epoch\n",
            "\n",
            "Score: 7732.703045685279\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "100. epoch\n",
            "\n",
            "Score: 7829.659263959391\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "101. epoch\n",
            "\n",
            "Score: 7978.641497461929\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "102. epoch\n",
            "\n",
            "Score: 8146.609137055838\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "103. epoch\n",
            "\n",
            "Score: 8161.083756345178\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "104. epoch\n",
            "\n",
            "Score: 7941.936548223351\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "105. epoch\n",
            "\n",
            "Score: 8113.955583756345\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "106. epoch\n",
            "\n",
            "Score: 7543.172588832487\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "107. epoch\n",
            "\n",
            "Score: 7958.422588832487\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "108. epoch\n",
            "\n",
            "Score: 7127.86230964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "109. epoch\n",
            "\n",
            "Score: 8110.3515228426395\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "110. epoch\n",
            "\n",
            "Score: 6874.477157360406\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "111. epoch\n",
            "\n",
            "Score: 8339.610406091371\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "112. epoch\n",
            "\n",
            "Score: 6697.458121827412\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "113. epoch\n",
            "\n",
            "Score: 8537.637055837564\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "114. epoch\n",
            "\n",
            "Score: 6647.171319796954\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "115. epoch\n",
            "\n",
            "Score: 8541.984771573603\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "116. epoch\n",
            "\n",
            "Score: 6862.000634517766\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "117. epoch\n",
            "\n",
            "Score: 8276.020939086295\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "118. epoch\n",
            "\n",
            "Score: 6804.993654822335\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "119. epoch\n",
            "\n",
            "Score: 7904.283629441625\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "120. epoch\n",
            "\n",
            "Score: 6736.828045685279\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "121. epoch\n",
            "\n",
            "Score: 7946.856598984771\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "122. epoch\n",
            "\n",
            "Score: 6685.015862944162\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "123. epoch\n",
            "\n",
            "Score: 7747.850888324873\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "124. epoch\n",
            "\n",
            "Score: 6515.632614213198\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "125. epoch\n",
            "\n",
            "Score: 7614.5767766497465\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "126. epoch\n",
            "\n",
            "Score: 6546.1922588832485\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "127. epoch\n",
            "\n",
            "Score: 7364.837563451777\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "128. epoch\n",
            "\n",
            "Score: 6391.094543147208\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "129. epoch\n",
            "\n",
            "Score: 6853.529187817259\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "130. epoch\n",
            "\n",
            "Score: 6316.390228426396\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "131. epoch\n",
            "\n",
            "Score: 6624.33692893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "132. epoch\n",
            "\n",
            "Score: 6114.36421319797\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "133. epoch\n",
            "\n",
            "Score: 6568.692893401016\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "134. epoch\n",
            "\n",
            "Score: 6075.315989847716\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "135. epoch\n",
            "\n",
            "Score: 6469.234771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "136. epoch\n",
            "\n",
            "Score: 6072.002538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "137. epoch\n",
            "\n",
            "Score: 6161.030456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "138. epoch\n",
            "\n",
            "Score: 6137.428299492386\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "139. epoch\n",
            "\n",
            "Score: 6142.25\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "140. epoch\n",
            "\n",
            "Score: 6142.23730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "141. epoch\n",
            "\n",
            "Score: 5869.154187817259\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "142. epoch\n",
            "\n",
            "Score: 6007.144035532995\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "143. epoch\n",
            "\n",
            "Score: 5804.952411167513\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "144. epoch\n",
            "\n",
            "Score: 6372.998730964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "145. epoch\n",
            "\n",
            "Score: 5782.905456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "146. epoch\n",
            "\n",
            "Score: 6482.4295685279185\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "147. epoch\n",
            "\n",
            "Score: 5913.84771573604\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "148. epoch\n",
            "\n",
            "Score: 6321.4295685279185\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "149. epoch\n",
            "\n",
            "Score: 5898.847081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "150. epoch\n",
            "\n",
            "Score: 6487.8610406091375\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "151. epoch\n",
            "\n",
            "Score: 5912.491116751269\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "152. epoch\n",
            "\n",
            "Score: 6299.7734771573605\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "153. epoch\n",
            "\n",
            "Score: 5452.017766497462\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "154. epoch\n",
            "\n",
            "Score: 6711.177030456853\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "155. epoch\n",
            "\n",
            "Score: 5528.715736040609\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "156. epoch\n",
            "\n",
            "Score: 6585.823604060914\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "157. epoch\n",
            "\n",
            "Score: 5224.365799492386\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "158. epoch\n",
            "\n",
            "Score: 7001.597081218274\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "159. epoch\n",
            "\n",
            "Score: 5341.005076142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "160. epoch\n",
            "\n",
            "Score: 6911.161802030457\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "161. epoch\n",
            "\n",
            "Score: 5229.877220812183\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "162. epoch\n",
            "\n",
            "Score: 6651.661167512691\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "163. epoch\n",
            "\n",
            "Score: 5141.77442893401\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "164. epoch\n",
            "\n",
            "Score: 6403.69923857868\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "165. epoch\n",
            "\n",
            "Score: 5147.096446700508\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "166. epoch\n",
            "\n",
            "Score: 6401.334390862944\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "167. epoch\n",
            "\n",
            "Score: 5134.86326142132\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "168. epoch\n",
            "\n",
            "Score: 6818.862944162436\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "169. epoch\n",
            "\n",
            "Score: 5252.706535532995\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "170. epoch\n",
            "\n",
            "Score: 6423.126269035533\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "171. epoch\n",
            "\n",
            "Score: 5048.985088832487\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "172. epoch\n",
            "\n",
            "Score: 6567.026015228426\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "173. epoch\n",
            "\n",
            "Score: 5298.26078680203\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "174. epoch\n",
            "\n",
            "Score: 6211.908629441625\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "175. epoch\n",
            "\n",
            "Score: 5172.709073604061\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "176. epoch\n",
            "\n",
            "Score: 6081.479060913705\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "177. epoch\n",
            "\n",
            "Score: 5218.655456852792\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "178. epoch\n",
            "\n",
            "Score: 5685.186548223351\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "179. epoch\n",
            "\n",
            "Score: 5047.634200507614\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "180. epoch\n",
            "\n",
            "Score: 5463.125634517766\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "181. epoch\n",
            "\n",
            "Score: 5071.04980964467\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "182. epoch\n",
            "\n",
            "Score: 5588.628807106599\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "183. epoch\n",
            "\n",
            "Score: 5123.631345177665\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "184. epoch\n",
            "\n",
            "Score: 5300.34961928934\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "185. epoch\n",
            "\n",
            "Score: 5061.9796954314725\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "186. epoch\n",
            "\n",
            "Score: 5801.7201776649745\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "187. epoch\n",
            "\n",
            "Score: 4809.02538071066\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "188. epoch\n",
            "\n",
            "Score: 5407.769670050761\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "189. epoch\n",
            "\n",
            "Score: 4717.819479695431\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "190. epoch\n",
            "\n",
            "Score: 5289.565989847716\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "191. epoch\n",
            "\n",
            "Score: 4772.987626903553\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "192. epoch\n",
            "\n",
            "Score: 5143.133565989848\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "193. epoch\n",
            "\n",
            "Score: 4741.543781725888\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "194. epoch\n",
            "\n",
            "Score: 5359.174492385787\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "195. epoch\n",
            "\n",
            "Score: 4397.1202411167515\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "196. epoch\n",
            "\n",
            "Score: 5032.990482233503\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "197. epoch\n",
            "\n",
            "Score: 4713.591053299492\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "198. epoch\n",
            "\n",
            "Score: 4923.799175126904\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xdSjEfduJV5",
        "outputId": "573bfcf3-d3bf-4242-8f05-827a3444f7c4"
      },
      "source": [
        "# train all complex with logreg -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"LogReg\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = pickle.load(open('drive/MyDrive/complex/news_models/best_model_LogReg.pt', 'rb'))\n",
        "\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"\\n{model_name[0]}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"001\")\n",
        "    optimizer_end_res.append(\"001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Complex_1_Linear_108_LogReg\n",
            "\n",
            "The model has 1,398,266 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.5637512532742442 \t\t Validation Loss: 1.3721302976975074\n",
            "\t\t Validation Loss Decreased(inf--->1.372130) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.2229420849550012 \t\t Validation Loss: 0.8375803255117856\n",
            "\t\t Validation Loss Decreased(1.372130--->0.837580) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.14262602732491655 \t\t Validation Loss: 0.3590233429120137\n",
            "\t\t Validation Loss Decreased(0.837580--->0.359023) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.05740112837127133 \t\t Validation Loss: 0.05410323435297379\n",
            "\t\t Validation Loss Decreased(0.359023--->0.054103) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010826008652362067 \t\t Validation Loss: 0.0022393836071177456\n",
            "\t\t Validation Loss Decreased(0.054103--->0.002239) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018522445249333475 \t\t Validation Loss: 0.0017895744822453707\n",
            "\t\t Validation Loss Decreased(0.002239--->0.001790) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001904994051909779 \t\t Validation Loss: 0.0017524334252811968\n",
            "\t\t Validation Loss Decreased(0.001790--->0.001752) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001593540097756714 \t\t Validation Loss: 0.0016700638001426482\n",
            "\t\t Validation Loss Decreased(0.001752--->0.001670) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016139740269077388 \t\t Validation Loss: 0.0016597161794869373\n",
            "\t\t Validation Loss Decreased(0.001670--->0.001660) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016491173206501313 \t\t Validation Loss: 0.0016569592542229937\n",
            "\t\t Validation Loss Decreased(0.001660--->0.001657) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016613571347693938 \t\t Validation Loss: 0.0016656479531845364\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016713413335949284 \t\t Validation Loss: 0.001677397515427751\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016765865824512533 \t\t Validation Loss: 0.001690889827351874\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016775908478774836 \t\t Validation Loss: 0.001703099457350058\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016739203899821921 \t\t Validation Loss: 0.0017121671833312856\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016661003189526397 \t\t Validation Loss: 0.0017166326422459232\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001654776110552956 \t\t Validation Loss: 0.0017163967198799723\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016410048387164402 \t\t Validation Loss: 0.0017124104656082077\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016259002920894607 \t\t Validation Loss: 0.0017065430953292749\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016105805588193943 \t\t Validation Loss: 0.001701015090265383\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001595949287671042 \t\t Validation Loss: 0.001697884082274798\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015826734377549507 \t\t Validation Loss: 0.0016986191588400218\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015711210584708464 \t\t Validation Loss: 0.0017039091527784388\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001561416546669411 \t\t Validation Loss: 0.0017136948545857405\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015534984374204902 \t\t Validation Loss: 0.0017273703885551256\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015471897228004259 \t\t Validation Loss: 0.0017440284114295186\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015422697548716758 \t\t Validation Loss: 0.0017627009992093707\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00153851109427853 \t\t Validation Loss: 0.0017824830742588697\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015357169747428113 \t\t Validation Loss: 0.0018026524586065744\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015337143149982031 \t\t Validation Loss: 0.0018226418133753424\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015323737028961045 \t\t Validation Loss: 0.0018420521026620497\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001531594296810343 \t\t Validation Loss: 0.0018606074982716774\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015312975234779958 \t\t Validation Loss: 0.0018781169940036936\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015314311965846934 \t\t Validation Loss: 0.0018944834099294473\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015319517476962425 \t\t Validation Loss: 0.0019096269702109008\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015328324901732943 \t\t Validation Loss: 0.0019235154300426634\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001534053314055593 \t\t Validation Loss: 0.001936129128667884\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015355996359643098 \t\t Validation Loss: 0.0019474754987571102\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001537463381552968 \t\t Validation Loss: 0.0019575222440135595\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015396452343682885 \t\t Validation Loss: 0.0019662859580981042\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015421460077443437 \t\t Validation Loss: 0.001973747280247223\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015449711330851692 \t\t Validation Loss: 0.0019798966912695994\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015481300927333635 \t\t Validation Loss: 0.001984711445402354\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015516328126682925 \t\t Validation Loss: 0.0019881622373269727\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015554985415341485 \t\t Validation Loss: 0.001990204076999082\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001559743461847255 \t\t Validation Loss: 0.001990798986158692\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015643879604475522 \t\t Validation Loss: 0.001989902064311676\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015694552952285252 \t\t Validation Loss: 0.001987448875577404\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015749701234272907 \t\t Validation Loss: 0.0019834184741529706\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015809528169385787 \t\t Validation Loss: 0.001977760998460536\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015874271134757815 \t\t Validation Loss: 0.0019704427955170665\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015944177717461276 \t\t Validation Loss: 0.001961458366937362\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 93.17927109771574\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "\n",
            "Complex_1_LSTM_92_LogReg\n",
            "\n",
            "The model has 2,164,138 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.67916992884381 \t\t Validation Loss: 1.264389386543861\n",
            "\t\t Validation Loss Decreased(inf--->1.264389) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.22099748978743683 \t\t Validation Loss: 0.6879880405389346\n",
            "\t\t Validation Loss Decreased(1.264389--->0.687988) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.08988952750302348 \t\t Validation Loss: 0.17517501803544852\n",
            "\t\t Validation Loss Decreased(0.687988--->0.175175) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.024631811309026908 \t\t Validation Loss: 0.004090683731751947\n",
            "\t\t Validation Loss Decreased(0.175175--->0.004091) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002807459992205573 \t\t Validation Loss: 0.007754836106099761\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002116915754929839 \t\t Validation Loss: 0.0036303738955981457\n",
            "\t\t Validation Loss Decreased(0.004091--->0.003630) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017630646856363259 \t\t Validation Loss: 0.003919307478309537\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017052314384223741 \t\t Validation Loss: 0.004187656593365738\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019947609036682626 \t\t Validation Loss: 0.00494453189178155\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018771041414348056 \t\t Validation Loss: 0.004623295578102653\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001945775666909695 \t\t Validation Loss: 0.004809308859806221\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017460411205271107 \t\t Validation Loss: 0.004584199247451929\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00201391864558556 \t\t Validation Loss: 0.005294430177085674\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001839562364841333 \t\t Validation Loss: 0.005145549183138288\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017098273564369192 \t\t Validation Loss: 0.004475244867185561\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018588275148382258 \t\t Validation Loss: 0.004968087403820111\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019643860011726516 \t\t Validation Loss: 0.00491454721822475\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017970803837178628 \t\t Validation Loss: 0.004610319174109743\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018450841252578775 \t\t Validation Loss: 0.0043062126138605755\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019976880764777495 \t\t Validation Loss: 0.004093149412745753\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018255318305769783 \t\t Validation Loss: 0.0041055686485308865\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00191945128923131 \t\t Validation Loss: 0.003968432263578647\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001775274791824002 \t\t Validation Loss: 0.0032044589170254767\n",
            "\t\t Validation Loss Decreased(0.003630--->0.003204) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002092052428869883 \t\t Validation Loss: 0.0032589620274778167\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018709309114382376 \t\t Validation Loss: 0.0027712396202752222\n",
            "\t\t Validation Loss Decreased(0.003204--->0.002771) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018716966638945647 \t\t Validation Loss: 0.002644799036296228\n",
            "\t\t Validation Loss Decreased(0.002771--->0.002645) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019135115010588354 \t\t Validation Loss: 0.0028915922426117156\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001835621239749614 \t\t Validation Loss: 0.0023276098598636543\n",
            "\t\t Validation Loss Decreased(0.002645--->0.002328) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019401032721460168 \t\t Validation Loss: 0.0025292291046263506\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016884079857452496 \t\t Validation Loss: 0.0022534323148787594\n",
            "\t\t Validation Loss Decreased(0.002328--->0.002253) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021301450324244797 \t\t Validation Loss: 0.002271042760604849\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016792341516151824 \t\t Validation Loss: 0.0019710787892108783\n",
            "\t\t Validation Loss Decreased(0.002253--->0.001971) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001991691013540116 \t\t Validation Loss: 0.002188729919278278\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018127118119328108 \t\t Validation Loss: 0.0018430328110788162\n",
            "\t\t Validation Loss Decreased(0.001971--->0.001843) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019774309350051793 \t\t Validation Loss: 0.0024116904913591077\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018045471381786203 \t\t Validation Loss: 0.002030503839845411\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018159238333700577 \t\t Validation Loss: 0.0019158421200700104\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019124842200088441 \t\t Validation Loss: 0.0019148688700694877\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018675302515574102 \t\t Validation Loss: 0.0018541138919177824\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018753193962274472 \t\t Validation Loss: 0.0018806530861184\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016937871869477268 \t\t Validation Loss: 0.0017844541143858805\n",
            "\t\t Validation Loss Decreased(0.001843--->0.001784) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00195249033984854 \t\t Validation Loss: 0.0018651002386692339\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018446157588875173 \t\t Validation Loss: 0.0020216546007969347\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001846081206757525 \t\t Validation Loss: 0.001964013402287562\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017675514312498774 \t\t Validation Loss: 0.0018315089288919878\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001825086195084795 \t\t Validation Loss: 0.0017323837539431853\n",
            "\t\t Validation Loss Decreased(0.001784--->0.001732) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017037902670799176 \t\t Validation Loss: 0.0018098283718035629\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018538865512488661 \t\t Validation Loss: 0.0019024925198979103\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017745495636985209 \t\t Validation Loss: 0.0019251468945115518\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018880806510915627 \t\t Validation Loss: 0.0018002828784609358\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018769602505238475 \t\t Validation Loss: 0.0017790908190923242\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0018618375604116434 \t\t Validation Loss: 0.0021008061213741223\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 103.42843829314721\n",
            "\n",
            "Accuracy: 49.746192893401016\n",
            "\n",
            "\n",
            "Complex_1_LSTM_36_LogReg\n",
            "\n",
            "The model has 545,658 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.5324720008349096 \t\t Validation Loss: 1.8378789883393507\n",
            "\t\t Validation Loss Decreased(inf--->1.837879) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.3402557423703272 \t\t Validation Loss: 1.4945588570374708\n",
            "\t\t Validation Loss Decreased(1.837879--->1.494559) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.29599664401816755 \t\t Validation Loss: 1.154781236098363\n",
            "\t\t Validation Loss Decreased(1.494559--->1.154781) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.22645807842648513 \t\t Validation Loss: 0.7264443911038913\n",
            "\t\t Validation Loss Decreased(1.154781--->0.726444) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.1504945528718668 \t\t Validation Loss: 0.31884794051830584\n",
            "\t\t Validation Loss Decreased(0.726444--->0.318848) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.07977801712380873 \t\t Validation Loss: 0.07581103593111038\n",
            "\t\t Validation Loss Decreased(0.318848--->0.075811) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.032967570590207704 \t\t Validation Loss: 0.012514426193844814\n",
            "\t\t Validation Loss Decreased(0.075811--->0.012514) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011806803314971763 \t\t Validation Loss: 0.0056560852815612005\n",
            "\t\t Validation Loss Decreased(0.012514--->0.005656) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005093975345025191 \t\t Validation Loss: 0.0036197350575373722\n",
            "\t\t Validation Loss Decreased(0.005656--->0.003620) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0032936780260422746 \t\t Validation Loss: 0.0029354461629946646\n",
            "\t\t Validation Loss Decreased(0.003620--->0.002935) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0024549248250755103 \t\t Validation Loss: 0.0030692779400851578\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0023338367259507446 \t\t Validation Loss: 0.0025540268711315896\n",
            "\t\t Validation Loss Decreased(0.002935--->0.002554) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021697208741553935 \t\t Validation Loss: 0.0027266792872419152\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0024178649119504197 \t\t Validation Loss: 0.0027361197146372153\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022145601695818776 \t\t Validation Loss: 0.0027295717199404654\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002090497634600143 \t\t Validation Loss: 0.002475913228968588\n",
            "\t\t Validation Loss Decreased(0.002554--->0.002476) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002258466327770588 \t\t Validation Loss: 0.0025837235622860203\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020805168936243027 \t\t Validation Loss: 0.002698886579869745\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002182028897427576 \t\t Validation Loss: 0.002433893579738931\n",
            "\t\t Validation Loss Decreased(0.002476--->0.002434) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002272760376768388 \t\t Validation Loss: 0.0024915304876720677\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021919634372524516 \t\t Validation Loss: 0.002278383619760951\n",
            "\t\t Validation Loss Decreased(0.002434--->0.002278) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021470260343866773 \t\t Validation Loss: 0.002323926574228188\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020668913242155433 \t\t Validation Loss: 0.0021440563096593204\n",
            "\t\t Validation Loss Decreased(0.002278--->0.002144) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002172301981969409 \t\t Validation Loss: 0.0020541482282659183\n",
            "\t\t Validation Loss Decreased(0.002144--->0.002054) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002089799559561888 \t\t Validation Loss: 0.002064058319844592\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002119774087741216 \t\t Validation Loss: 0.002052844273678672\n",
            "\t\t Validation Loss Decreased(0.002054--->0.002053) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020380764711975447 \t\t Validation Loss: 0.002023126249416516\n",
            "\t\t Validation Loss Decreased(0.002053--->0.002023) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002213149461297419 \t\t Validation Loss: 0.002058263406909716\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020016873242786606 \t\t Validation Loss: 0.0020311456084108124\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022438617275297845 \t\t Validation Loss: 0.002069820090232847\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002114403373058382 \t\t Validation Loss: 0.0020415059362466517\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020757201638991465 \t\t Validation Loss: 0.0020575660836094846\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020781712224266515 \t\t Validation Loss: 0.002017575674332105\n",
            "\t\t Validation Loss Decreased(0.002023--->0.002018) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00210987647517106 \t\t Validation Loss: 0.0020786869124724315\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021464529846845245 \t\t Validation Loss: 0.002123922611085268\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002093759874303548 \t\t Validation Loss: 0.002151469310494856\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002008148288901744 \t\t Validation Loss: 0.002012584277858528\n",
            "\t\t Validation Loss Decreased(0.002018--->0.002013) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002073599595098278 \t\t Validation Loss: 0.0020409385011029933\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021569704876054784 \t\t Validation Loss: 0.0021255835040042605\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001941379058218838 \t\t Validation Loss: 0.002061500052849834\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022403042145211854 \t\t Validation Loss: 0.001992536757965214\n",
            "\t\t Validation Loss Decreased(0.002013--->0.001993) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0020852580649237075 \t\t Validation Loss: 0.0019547897435796377\n",
            "\t\t Validation Loss Decreased(0.001993--->0.001955) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001996813911466381 \t\t Validation Loss: 0.0020883370665475153\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002204569545102532 \t\t Validation Loss: 0.002195773623847904\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002088444005395915 \t\t Validation Loss: 0.001985999388405337\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002065570667115468 \t\t Validation Loss: 0.001953768302113391\n",
            "\t\t Validation Loss Decreased(0.001955--->0.001954) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022395251537484393 \t\t Validation Loss: 0.002184951570458137\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002108304958784248 \t\t Validation Loss: 0.0022740257957663676\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0023297619741611385 \t\t Validation Loss: 0.0019834609567904128\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002227238248844908 \t\t Validation Loss: 0.0019625189642493543\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002264371426536928 \t\t Validation Loss: 0.002027597963415946\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022426430219343887 \t\t Validation Loss: 0.001988013919729453\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 192.73356202411168\n",
            "\n",
            "Accuracy: 50.50761421319797\n",
            "\n",
            "\n",
            "Complex_2_Linear_108_LogReg\n",
            "\n",
            "The model has 1,401,290 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.5540723893791437 \t\t Validation Loss: 0.8218680207545941\n",
            "\t\t Validation Loss Decreased(inf--->0.821868) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.20200991721798642 \t\t Validation Loss: 0.2630057678772853\n",
            "\t\t Validation Loss Decreased(0.821868--->0.263006) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.07058975605508061 \t\t Validation Loss: 0.0033997952078397456\n",
            "\t\t Validation Loss Decreased(0.263006--->0.003400) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0046893914066557145 \t\t Validation Loss: 0.004229434237528879\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0022543742120064592 \t\t Validation Loss: 0.0024391677266416643\n",
            "\t\t Validation Loss Decreased(0.003400--->0.002439) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017274699608065389 \t\t Validation Loss: 0.002111615982497684\n",
            "\t\t Validation Loss Decreased(0.002439--->0.002112) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0019235677546758608 \t\t Validation Loss: 0.0018613311515834469\n",
            "\t\t Validation Loss Decreased(0.002112--->0.001861) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001606219112275269 \t\t Validation Loss: 0.001722325989528774\n",
            "\t\t Validation Loss Decreased(0.001861--->0.001722) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016850285165993547 \t\t Validation Loss: 0.001868236827878998\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015538045845460147 \t\t Validation Loss: 0.0017133156831662816\n",
            "\t\t Validation Loss Decreased(0.001722--->0.001713) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015368724738977649 \t\t Validation Loss: 0.0018930862603995663\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001504696364439017 \t\t Validation Loss: 0.001864986503138565\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0014953160071682588 \t\t Validation Loss: 0.0019026400360207146\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0014924500932023432 \t\t Validation Loss: 0.001946786642432786\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0014937897228535163 \t\t Validation Loss: 0.0019432484405115247\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0014950667265676767 \t\t Validation Loss: 0.0019624899589241697\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001496405280163713 \t\t Validation Loss: 0.0019756231206254317\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001498471903834945 \t\t Validation Loss: 0.0019781174800860193\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015007595781778061 \t\t Validation Loss: 0.0019849962554872036\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015031614606709194 \t\t Validation Loss: 0.0019905277429363476\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015058064104901073 \t\t Validation Loss: 0.00199424796916831\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001508609630085679 \t\t Validation Loss: 0.0019991626121247043\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015115310922516761 \t\t Validation Loss: 0.002005075973172027\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015145602999443843 \t\t Validation Loss: 0.002012892044149339\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015176926114961404 \t\t Validation Loss: 0.0020236769520964185\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015209476240105122 \t\t Validation Loss: 0.0020385242398613347\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015243819003886972 \t\t Validation Loss: 0.0020587000136192027\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015281013038475066 \t\t Validation Loss: 0.002085538849664422\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015322652478652024 \t\t Validation Loss: 0.002120230700641584\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015370726728547565 \t\t Validation Loss: 0.0021636948759595933\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015427323775617657 \t\t Validation Loss: 0.0022161032986612273\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015493823928015961 \t\t Validation Loss: 0.002276586982099196\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015570215520612593 \t\t Validation Loss: 0.00234281232741733\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015654142031987274 \t\t Validation Loss: 0.0024109211875698888\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015741065545699785 \t\t Validation Loss: 0.0024757906794548035\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015824763285235276 \t\t Validation Loss: 0.00253187189809978\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0015899275619901615 \t\t Validation Loss: 0.0025745121439775596\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001596087172180666 \t\t Validation Loss: 0.0026011689780996395\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016009492329258875 \t\t Validation Loss: 0.00261222947245607\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001604861646695566 \t\t Validation Loss: 0.002610758376809267\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016083859420079436 \t\t Validation Loss: 0.002601457446312102\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001612077369606374 \t\t Validation Loss: 0.0025890213885129644\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016163414622644417 \t\t Validation Loss: 0.002576917893468187\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016213934286497533 \t\t Validation Loss: 0.002566989824677316\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016273245155635115 \t\t Validation Loss: 0.0025598674546927214\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016341777667843711 \t\t Validation Loss: 0.002555529363095187\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016420090624533997 \t\t Validation Loss: 0.00255386066587212\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016509246299785839 \t\t Validation Loss: 0.002554883947595954\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016611171886324883 \t\t Validation Loss: 0.0025587221428465387\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.001672876367150378 \t\t Validation Loss: 0.002565580330645809\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0016866127340550963 \t\t Validation Loss: 0.0025757671906971014\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0017028882127330715 \t\t Validation Loss: 0.002589646252230383\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 99.22798025063452\n",
            "\n",
            "Accuracy: 47.96954314720812\n",
            "\n",
            "\n",
            "Complex_2_LSTM_92_LogReg\n",
            "\n",
            "The model has 2,167,162 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.3734104675366669 \t\t Validation Loss: 0.6413206801964686\n",
            "\t\t Validation Loss Decreased(inf--->0.641321) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.20585094846476726 \t\t Validation Loss: 0.029924862946455296\n",
            "\t\t Validation Loss Decreased(0.641321--->0.029925) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005222037396035032 \t\t Validation Loss: 0.011069001809049111\n",
            "\t\t Validation Loss Decreased(0.029925--->0.011069) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0038862091778601345 \t\t Validation Loss: 0.0017282491205081057\n",
            "\t\t Validation Loss Decreased(0.011069--->0.001728) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0034999195612161545 \t\t Validation Loss: 0.010948600784803812\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006216428868749456 \t\t Validation Loss: 0.003436692972452595\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010014651196920691 \t\t Validation Loss: 0.01614636103980816\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013958016023467723 \t\t Validation Loss: 0.008529713932568064\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.019073461559978692 \t\t Validation Loss: 0.03274575133736317\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.02806040327492598 \t\t Validation Loss: 0.035100595165903754\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.030196264232005422 \t\t Validation Loss: 0.05619856844154688\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.050912071570024094 \t\t Validation Loss: 0.17430691421031952\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.054790061369941044 \t\t Validation Loss: 0.07143004224277459\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.060472170564320846 \t\t Validation Loss: 0.3460257397248195\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.07870029810992246 \t\t Validation Loss: 0.09461491153790401\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.03615062529491764 \t\t Validation Loss: 0.202252831023473\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.04957130632199649 \t\t Validation Loss: 0.08953532519248816\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.031203952523552486 \t\t Validation Loss: 0.15324469999625132\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.038581344428019144 \t\t Validation Loss: 0.07968108384655072\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.028861396574742487 \t\t Validation Loss: 0.1357259561236088\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.035370998002739774 \t\t Validation Loss: 0.08340571878048089\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.026374827601594496 \t\t Validation Loss: 0.12138706187789257\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.031527503885995796 \t\t Validation Loss: 0.08369114708441955\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.023910724762449594 \t\t Validation Loss: 0.10453535043276273\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.027610313834786113 \t\t Validation Loss: 0.07748991165023583\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.020824044363925587 \t\t Validation Loss: 0.08914890455511901\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.02318901807302609 \t\t Validation Loss: 0.0719238631427288\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.016995805196111668 \t\t Validation Loss: 0.06666217629726116\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01891535833616414 \t\t Validation Loss: 0.06452373014046596\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0150571992177189 \t\t Validation Loss: 0.05322336505811948\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.016525753205830884 \t\t Validation Loss: 0.05931798024819447\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013961539487354457 \t\t Validation Loss: 0.04753737137294733\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01498527026332512 \t\t Validation Loss: 0.05592845380306244\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013394852000527788 \t\t Validation Loss: 0.041721491572948605\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013986089850486433 \t\t Validation Loss: 0.05341821120908627\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012976738197942945 \t\t Validation Loss: 0.04273930521538624\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014406548479471254 \t\t Validation Loss: 0.051088859971899256\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012398831351232287 \t\t Validation Loss: 0.04064700508920046\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014222735839556097 \t\t Validation Loss: 0.0519817045961435\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012962973953501598 \t\t Validation Loss: 0.0430462288741882\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014713180046906141 \t\t Validation Loss: 0.051474934443831444\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013275328391181255 \t\t Validation Loss: 0.04494488038695776\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015224406601093407 \t\t Validation Loss: 0.05206401230624089\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013446106882199544 \t\t Validation Loss: 0.04396193030361946\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014334639034082962 \t\t Validation Loss: 0.051549114573460356\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013444274943951215 \t\t Validation Loss: 0.04140646970615937\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014057971030549222 \t\t Validation Loss: 0.04991168414170925\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012926140073309274 \t\t Validation Loss: 0.04234319294874485\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01430940699002177 \t\t Validation Loss: 0.049549260964760415\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012836261521836088 \t\t Validation Loss: 0.04217937044226206\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014255008141107455 \t\t Validation Loss: 0.048694923090247005\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012503024412482674 \t\t Validation Loss: 0.04042495214022123\n",
            "Epoch 53 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013781781888856376 \t\t Validation Loss: 0.04603507283788461\n",
            "Epoch 54 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011266055812656477 \t\t Validation Loss: 0.03632095757012184\n",
            "Epoch 55 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013015130295644741 \t\t Validation Loss: 0.04377226488521466\n",
            "Epoch 56 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010980926732520095 \t\t Validation Loss: 0.033045570294444375\n",
            "Epoch 57 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012459829891717213 \t\t Validation Loss: 0.04285630340186449\n",
            "Epoch 58 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010662295978289802 \t\t Validation Loss: 0.03136307989748625\n",
            "Epoch 59 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01214306426983377 \t\t Validation Loss: 0.04186601850848932\n",
            "Epoch 60 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010703968490643829 \t\t Validation Loss: 0.03186571841629652\n",
            "Epoch 61 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011865982310720594 \t\t Validation Loss: 0.042038303441726245\n",
            "Epoch 62 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010725657955894398 \t\t Validation Loss: 0.03125012723299173\n",
            "Epoch 63 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011963284909536407 \t\t Validation Loss: 0.03876903954033668\n",
            "Epoch 64 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009684609699163687 \t\t Validation Loss: 0.025813260330603674\n",
            "Epoch 65 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010524620182459822 \t\t Validation Loss: 0.03834914802931822\n",
            "Epoch 66 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010079720971564687 \t\t Validation Loss: 0.02543027211840336\n",
            "Epoch 67 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010621564009705105 \t\t Validation Loss: 0.036587810287108787\n",
            "Epoch 68 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009623172137621991 \t\t Validation Loss: 0.02388708873723562\n",
            "Epoch 69 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009926071958427594 \t\t Validation Loss: 0.03577609775731197\n",
            "Epoch 70 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0094123597105814 \t\t Validation Loss: 0.022403396021288175\n",
            "Epoch 71 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010048339325884307 \t\t Validation Loss: 0.03351170970843388\n",
            "Epoch 72 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008927867464670862 \t\t Validation Loss: 0.020957784369014777\n",
            "Epoch 73 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00992871226264617 \t\t Validation Loss: 0.03411628692769087\n",
            "Epoch 74 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00869007787597995 \t\t Validation Loss: 0.01973755070223258\n",
            "Epoch 75 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009298328470715598 \t\t Validation Loss: 0.03271816112101078\n",
            "Epoch 76 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009083630262269965 \t\t Validation Loss: 0.01904193526850297\n",
            "Epoch 77 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009217614726465498 \t\t Validation Loss: 0.031739527287964635\n",
            "Epoch 78 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009042691396914321 \t\t Validation Loss: 0.019514052268977348\n",
            "Epoch 79 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009537398550860785 \t\t Validation Loss: 0.03226103103504731\n",
            "Epoch 80 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009393324671199898 \t\t Validation Loss: 0.018896043515549257\n",
            "Epoch 81 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009575416422071489 \t\t Validation Loss: 0.02986182473026789\n",
            "Epoch 82 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008649235433974379 \t\t Validation Loss: 0.01634390136370292\n",
            "Epoch 83 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00925404778083887 \t\t Validation Loss: 0.026839022882855855\n",
            "Epoch 84 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008389090464727298 \t\t Validation Loss: 0.01222482595879298\n",
            "Epoch 85 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008415087466002317 \t\t Validation Loss: 0.02700863520686443\n",
            "Epoch 86 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00856695425967258 \t\t Validation Loss: 0.015102705022749992\n",
            "Epoch 87 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008946878505191085 \t\t Validation Loss: 0.02565715082276326\n",
            "Epoch 88 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008341001936649854 \t\t Validation Loss: 0.014460480198837243\n",
            "Epoch 89 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008977269813594585 \t\t Validation Loss: 0.024616102688014507\n",
            "Epoch 90 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007923174558232564 \t\t Validation Loss: 0.012307148749152055\n",
            "Epoch 91 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00844258674407831 \t\t Validation Loss: 0.02539905798263275\n",
            "Epoch 92 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007728344513845907 \t\t Validation Loss: 0.013082466267335873\n",
            "Epoch 93 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007962479230922621 \t\t Validation Loss: 0.026123091721763976\n",
            "Epoch 94 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007553825825075242 \t\t Validation Loss: 0.012428853768282212\n",
            "Epoch 95 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007449321974245076 \t\t Validation Loss: 0.023454354932674996\n",
            "Epoch 96 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006608508193764735 \t\t Validation Loss: 0.010198690713598179\n",
            "Epoch 97 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007098442516163797 \t\t Validation Loss: 0.022996115498244762\n",
            "Epoch 98 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006559852825326694 \t\t Validation Loss: 0.009804324640964087\n",
            "Epoch 99 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006912645078390031 \t\t Validation Loss: 0.02303094108803914\n",
            "Epoch 100 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006985493430653839 \t\t Validation Loss: 0.009647003088433009\n",
            "\n",
            "Score: 100.8970891497462\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "\n",
            "Complex_2_LSTM_36_LogReg\n",
            "\n",
            "The model has 548,682 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.4826631148068889 \t\t Validation Loss: 1.1066790039722736\n",
            "\t\t Validation Loss Decreased(inf--->1.106679) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.25867146648768635 \t\t Validation Loss: 0.1613284068611952\n",
            "\t\t Validation Loss Decreased(1.106679--->0.161328) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.03458407211646035 \t\t Validation Loss: 0.0568941580848052\n",
            "\t\t Validation Loss Decreased(0.161328--->0.056894) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.023864332838235674 \t\t Validation Loss: 0.10812797454687265\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.025688898699307763 \t\t Validation Loss: 0.02548920685568681\n",
            "\t\t Validation Loss Decreased(0.056894--->0.025489) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01229279296397156 \t\t Validation Loss: 0.04748940668427027\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015766356624914584 \t\t Validation Loss: 0.020344150252640247\n",
            "\t\t Validation Loss Decreased(0.025489--->0.020344) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013444023700808553 \t\t Validation Loss: 0.043758609976906046\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.016124800376191333 \t\t Validation Loss: 0.018340512393758848\n",
            "\t\t Validation Loss Decreased(0.020344--->0.018341) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01500080489691951 \t\t Validation Loss: 0.04425602850432579\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.016480432996350165 \t\t Validation Loss: 0.017461876313273724\n",
            "\t\t Validation Loss Decreased(0.018341--->0.017462) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015574146353174001 \t\t Validation Loss: 0.048917119892743915\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01824354926891927 \t\t Validation Loss: 0.016108904893581685\n",
            "\t\t Validation Loss Decreased(0.017462--->0.016109) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015035750152467674 \t\t Validation Loss: 0.046204036388259664\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01709248374115575 \t\t Validation Loss: 0.01664058896354758\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013917646380267231 \t\t Validation Loss: 0.04627269463470349\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01651743012543365 \t\t Validation Loss: 0.015440815844788002\n",
            "\t\t Validation Loss Decreased(0.016109--->0.015441) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012524343690381863 \t\t Validation Loss: 0.03687465943109531\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014610819106392964 \t\t Validation Loss: 0.015471552677739125\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01287591689568315 \t\t Validation Loss: 0.039345563461001105\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014866186158001624 \t\t Validation Loss: 0.013847981221400775\n",
            "\t\t Validation Loss Decreased(0.015441--->0.013848) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01153193658054177 \t\t Validation Loss: 0.03255788774157946\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013111839640407345 \t\t Validation Loss: 0.013660574725900706\n",
            "\t\t Validation Loss Decreased(0.013848--->0.013661) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010938554434294536 \t\t Validation Loss: 0.030608947245547406\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012240774098884416 \t\t Validation Loss: 0.012887695338577032\n",
            "\t\t Validation Loss Decreased(0.013661--->0.012888) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009177313219265055 \t\t Validation Loss: 0.022616208387682073\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010020865712667236 \t\t Validation Loss: 0.013212812598794699\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00911985826521256 \t\t Validation Loss: 0.02074885103278435\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009312177961062942 \t\t Validation Loss: 0.011320752718557533\n",
            "\t\t Validation Loss Decreased(0.012888--->0.011321) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008026908386209225 \t\t Validation Loss: 0.017739456254415788\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008460761655147213 \t\t Validation Loss: 0.012036941121690549\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008517526137340511 \t\t Validation Loss: 0.014345559040800883\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007499614716318713 \t\t Validation Loss: 0.01109366722476597\n",
            "\t\t Validation Loss Decreased(0.011321--->0.011094) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00714299497918251 \t\t Validation Loss: 0.013505140964228373\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0074742621257649485 \t\t Validation Loss: 0.008674107724800706\n",
            "\t\t Validation Loss Decreased(0.011094--->0.008674) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006615384629726209 \t\t Validation Loss: 0.011322811926500155\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006619575447591676 \t\t Validation Loss: 0.008879801437545281\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005829092008187561 \t\t Validation Loss: 0.008696851069824053\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006162615206621184 \t\t Validation Loss: 0.007299880121046534\n",
            "\t\t Validation Loss Decreased(0.008674--->0.007300) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006072128540836275 \t\t Validation Loss: 0.008716266691828003\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006127660011721624 \t\t Validation Loss: 0.006525965532861077\n",
            "\t\t Validation Loss Decreased(0.007300--->0.006526) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005144790316793464 \t\t Validation Loss: 0.007865740237041162\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0055882041641460685 \t\t Validation Loss: 0.006439659860916436\n",
            "\t\t Validation Loss Decreased(0.006526--->0.006440) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0054023753272721894 \t\t Validation Loss: 0.007772596911169016\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005881405849248875 \t\t Validation Loss: 0.006109259154002827\n",
            "\t\t Validation Loss Decreased(0.006440--->0.006109) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005441278002124179 \t\t Validation Loss: 0.006871968507766724\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005577110883628798 \t\t Validation Loss: 0.005755630639704088\n",
            "\t\t Validation Loss Decreased(0.006109--->0.005756) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004931051145634941 \t\t Validation Loss: 0.00510939174833206\n",
            "\t\t Validation Loss Decreased(0.005756--->0.005109) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004787775403127779 \t\t Validation Loss: 0.005458057584921614\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004400786863501511 \t\t Validation Loss: 0.005147217826631207\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004738330679026911 \t\t Validation Loss: 0.004261039398932973\n",
            "\t\t Validation Loss Decreased(0.005109--->0.004261) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0039388463546119225 \t\t Validation Loss: 0.00510477045407662\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 170.46276173857868\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "\n",
            "Complex_3_Linear_108_LogReg\n",
            "\n",
            "The model has 1,458,682 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.5658585715006936 \t\t Validation Loss: 0.9091860606120183\n",
            "\t\t Validation Loss Decreased(inf--->0.909186) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.4544045168252007 \t\t Validation Loss: 0.7498518182681158\n",
            "\t\t Validation Loss Decreased(0.909186--->0.749852) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.3428513454525052 \t\t Validation Loss: 0.5777738438202784\n",
            "\t\t Validation Loss Decreased(0.749852--->0.577774) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.14057694591040648 \t\t Validation Loss: 0.18969145302589124\n",
            "\t\t Validation Loss Decreased(0.577774--->0.189691) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.04730720377080394 \t\t Validation Loss: 0.0022747927936367118\n",
            "\t\t Validation Loss Decreased(0.189691--->0.002275) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005825614537792029 \t\t Validation Loss: 0.006224140023382811\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0035662541002337194 \t\t Validation Loss: 0.002716538657505925\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003696773904438659 \t\t Validation Loss: 0.022219186290525474\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006126069992377951 \t\t Validation Loss: 0.024794240285140965\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.021040503552334534 \t\t Validation Loss: 0.5053695486142085\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.027201521834924917 \t\t Validation Loss: 0.1025126063479827\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011388584236752847 \t\t Validation Loss: 0.001874165779624421\n",
            "\t\t Validation Loss Decreased(0.002275--->0.001874) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005866292878242864 \t\t Validation Loss: 0.002770717761730059\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007836891796024566 \t\t Validation Loss: 0.016346837501399793\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009670204430271091 \t\t Validation Loss: 0.11397690526567973\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006260817098066311 \t\t Validation Loss: 0.012444415577472402\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.012375858313454365 \t\t Validation Loss: 0.0689519139436575\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007230582765014087 \t\t Validation Loss: 0.015951088808763485\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01149336079362075 \t\t Validation Loss: 0.13378728926181793\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008588824984365823 \t\t Validation Loss: 0.007389999496249052\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008882114506402129 \t\t Validation Loss: 0.04259138540006601\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004586041783851043 \t\t Validation Loss: 0.01360800231878574\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007818400191231253 \t\t Validation Loss: 0.10331438797024581\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00819271444059858 \t\t Validation Loss: 0.009795749577908562\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007125032843858306 \t\t Validation Loss: 0.07065400366599743\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006779860023950302 \t\t Validation Loss: 0.012269561286442555\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007194934882431266 \t\t Validation Loss: 0.09459979746204156\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006777853445414253 \t\t Validation Loss: 0.008456728403241588\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005798915051855147 \t\t Validation Loss: 0.05448974912556318\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005185628218601483 \t\t Validation Loss: 0.012742917089221569\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00606544702187986 \t\t Validation Loss: 0.08248294717990436\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006672469935241483 \t\t Validation Loss: 0.007681744835840968\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004889180454287074 \t\t Validation Loss: 0.05547558559248081\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005408657253782793 \t\t Validation Loss: 0.013654579551747212\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005865850013907294 \t\t Validation Loss: 0.09652221231506421\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007715228573746375 \t\t Validation Loss: 0.005831322364079265\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003314451893439164 \t\t Validation Loss: 0.030372761763059176\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004214107468755715 \t\t Validation Loss: 0.013999042781786276\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004374571589790788 \t\t Validation Loss: 0.0414461216960962\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00458400252564634 \t\t Validation Loss: 0.013049623714043545\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004020318557269167 \t\t Validation Loss: 0.04830165680211324\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005524739514520341 \t\t Validation Loss: 0.009774423096902095\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0031829355001399243 \t\t Validation Loss: 0.033726715840972386\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005621077552613978 \t\t Validation Loss: 0.010828234685155062\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002963142494696218 \t\t Validation Loss: 0.03626302715677481\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006462413016895487 \t\t Validation Loss: 0.009811110740814071\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002756219733242147 \t\t Validation Loss: 0.033545964684050814\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007417524888689906 \t\t Validation Loss: 0.009908694129150648\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002758136989762755 \t\t Validation Loss: 0.03428607075833357\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008719639828258723 \t\t Validation Loss: 0.008891932612571578\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003035559937219463 \t\t Validation Loss: 0.03877539488558586\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010261736119581337 \t\t Validation Loss: 0.007988697079081949\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 116.27784938134518\n",
            "\n",
            "Accuracy: 47.46192893401015\n",
            "\n",
            "\n",
            "Complex_3_LSTM_92_LogReg\n",
            "\n",
            "The model has 2,224,554 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.698568233188141 \t\t Validation Loss: 0.7283597382215353\n",
            "\t\t Validation Loss Decreased(inf--->0.728360) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.4434205947909504 \t\t Validation Loss: 0.6438015527450122\n",
            "\t\t Validation Loss Decreased(0.728360--->0.643802) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.2916093725912474 \t\t Validation Loss: 0.691867607144209\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.12102409823785058 \t\t Validation Loss: 0.6849901756415\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.022914725856075215 \t\t Validation Loss: 0.5366124797325867\n",
            "\t\t Validation Loss Decreased(0.643802--->0.536612) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004709902766940964 \t\t Validation Loss: 0.26464378317961323\n",
            "\t\t Validation Loss Decreased(0.536612--->0.264644) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004850217556530559 \t\t Validation Loss: 0.10188622013307534\n",
            "\t\t Validation Loss Decreased(0.264644--->0.101886) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0026342759579042526 \t\t Validation Loss: 0.05493821264602817\n",
            "\t\t Validation Loss Decreased(0.101886--->0.054938) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003796165581710788 \t\t Validation Loss: 0.030447713582991406\n",
            "\t\t Validation Loss Decreased(0.054938--->0.030448) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011264109909314209 \t\t Validation Loss: 0.11754223790306312\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005129233432927396 \t\t Validation Loss: 0.01861097472003446\n",
            "\t\t Validation Loss Decreased(0.030448--->0.018611) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01680926888759525 \t\t Validation Loss: 0.3229759197968703\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007914244686568005 \t\t Validation Loss: 0.06284702948939341\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01745706729156695 \t\t Validation Loss: 0.027010894470060103\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.002204120675152218 \t\t Validation Loss: 0.023239442971176826\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0021066305021491104 \t\t Validation Loss: 0.02072234792061723\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0038670735564271643 \t\t Validation Loss: 0.008966908691665875\n",
            "\t\t Validation Loss Decreased(0.018611--->0.008967) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007014476571458619 \t\t Validation Loss: 0.23755045464405647\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004962041599650842 \t\t Validation Loss: 0.05645816429303242\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010857654777135246 \t\t Validation Loss: 0.02132170801409162\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003346509883822118 \t\t Validation Loss: 0.052725973539054394\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004570139503591014 \t\t Validation Loss: 0.009481464288770579\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00587974810450514 \t\t Validation Loss: 0.08580232941760467\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.0036130031646304838 \t\t Validation Loss: 0.007210409868723498\n",
            "\t\t Validation Loss Decreased(0.008967--->0.007210) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007245908469288937 \t\t Validation Loss: 0.08697810224615611\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004351382250771732 \t\t Validation Loss: 0.006244195204299803\n",
            "\t\t Validation Loss Decreased(0.007210--->0.006244) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008044064410716749 \t\t Validation Loss: 0.07874517615597981\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005075952972637842 \t\t Validation Loss: 0.005870837736158417\n",
            "\t\t Validation Loss Decreased(0.006244--->0.005871) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008435860479162453 \t\t Validation Loss: 0.11840967117593838\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004596192402312079 \t\t Validation Loss: 0.0069402237691415045\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008502259723074432 \t\t Validation Loss: 0.07503676830002895\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004044616187456995 \t\t Validation Loss: 0.004424446154958927\n",
            "\t\t Validation Loss Decreased(0.005871--->0.004424) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008185045773201194 \t\t Validation Loss: 0.0770030663563655\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.003595999160441696 \t\t Validation Loss: 0.004601268088803268\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008838394008627211 \t\t Validation Loss: 0.058145518987797774\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004560433694702649 \t\t Validation Loss: 0.004313551838724659\n",
            "\t\t Validation Loss Decreased(0.004424--->0.004314) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007586291870039359 \t\t Validation Loss: 0.07056639887965642\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.004788352499048956 \t\t Validation Loss: 0.004596634544073963\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007066186899759782 \t\t Validation Loss: 0.05342119115476425\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.005925211367965047 \t\t Validation Loss: 0.003865089142121948\n",
            "\t\t Validation Loss Decreased(0.004314--->0.003865) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00601692490336309 \t\t Validation Loss: 0.07878664436821754\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.006115020366033187 \t\t Validation Loss: 0.005758362078967576\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007278583662244617 \t\t Validation Loss: 0.06568584978007354\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009032503909406226 \t\t Validation Loss: 0.002071914030238986\n",
            "\t\t Validation Loss Decreased(0.003865--->0.002072) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007918095543376497 \t\t Validation Loss: 0.159648053921186\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00942711580569881 \t\t Validation Loss: 0.017877639629519902\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00709265103009907 \t\t Validation Loss: 0.029310891225647468\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009811200118132841 \t\t Validation Loss: 0.012131363950454844\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011612266298175815 \t\t Validation Loss: 0.020443151848247416\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013543866419019428 \t\t Validation Loss: 0.006322178658312903\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01142982389098285 \t\t Validation Loss: 0.054089613258838654\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015383172036161193 \t\t Validation Loss: 0.0025783639329557237\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 172.3034977791878\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "\n",
            "Complex_3_LSTM_36_LogReg\n",
            "\n",
            "The model has 606,074 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.3854325029092866 \t\t Validation Loss: 1.2258744560755217\n",
            "\t\t Validation Loss Decreased(inf--->1.225874) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.23174456386147319 \t\t Validation Loss: 0.5279250970253577\n",
            "\t\t Validation Loss Decreased(1.225874--->0.527925) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.03567818898355236 \t\t Validation Loss: 0.11799424571486619\n",
            "\t\t Validation Loss Decreased(0.527925--->0.117994) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.020322036729634715 \t\t Validation Loss: 0.0022229044429528024\n",
            "\t\t Validation Loss Decreased(0.117994--->0.002223) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.022836691289322097 \t\t Validation Loss: 0.003110898586993034\n",
            "Epoch 6 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007453582867841564 \t\t Validation Loss: 0.004525352932082919\n",
            "Epoch 7 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01174845584397632 \t\t Validation Loss: 0.002162401139055594\n",
            "\t\t Validation Loss Decreased(0.002223--->0.002162) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.007214223020521269 \t\t Validation Loss: 0.006012525570650513\n",
            "Epoch 9 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.013395757137520893 \t\t Validation Loss: 0.008236052217678381\n",
            "Epoch 10 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009784601679122125 \t\t Validation Loss: 0.016608360760773603\n",
            "Epoch 11 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.02488678353649841 \t\t Validation Loss: 0.1690541786643175\n",
            "Epoch 12 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.02130930986557458 \t\t Validation Loss: 0.026421087471624978\n",
            "Epoch 13 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.029165495825755235 \t\t Validation Loss: 0.294822735282091\n",
            "Epoch 14 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.022874119922808196 \t\t Validation Loss: 0.019507461800598182\n",
            "Epoch 15 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01669698760756669 \t\t Validation Loss: 0.06473916625747314\n",
            "Epoch 16 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010887467207991192 \t\t Validation Loss: 0.02399308356241538\n",
            "Epoch 17 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.021824618493131286 \t\t Validation Loss: 0.19629824046905225\n",
            "Epoch 18 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.020636013913522096 \t\t Validation Loss: 0.0242334004634848\n",
            "Epoch 19 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01695026458882903 \t\t Validation Loss: 0.1197851595397179\n",
            "Epoch 20 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014306424640282686 \t\t Validation Loss: 0.025561756597688563\n",
            "Epoch 21 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01765266347777199 \t\t Validation Loss: 0.15877007119930708\n",
            "Epoch 22 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014832160204359507 \t\t Validation Loss: 0.01814880075219732\n",
            "Epoch 23 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.016612053213238314 \t\t Validation Loss: 0.1010325439274311\n",
            "Epoch 24 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011815850624525165 \t\t Validation Loss: 0.014751496617324077\n",
            "Epoch 25 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.015905695913888112 \t\t Validation Loss: 0.1658503464781321\n",
            "Epoch 26 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.014497352413191283 \t\t Validation Loss: 0.03663538324718292\n",
            "Epoch 27 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01655110190433727 \t\t Validation Loss: 0.1783405548104873\n",
            "Epoch 28 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.017669862024548993 \t\t Validation Loss: 0.018518619454250887\n",
            "Epoch 29 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010194684948293946 \t\t Validation Loss: 0.04235228552268101\n",
            "Epoch 30 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009030584890923992 \t\t Validation Loss: 0.024017624986859467\n",
            "Epoch 31 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010969494340463064 \t\t Validation Loss: 0.08225202961609913\n",
            "Epoch 32 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01125201367697603 \t\t Validation Loss: 0.022167925722897053\n",
            "Epoch 33 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011113546105932343 \t\t Validation Loss: 0.08194117162090081\n",
            "Epoch 34 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.011119858375939264 \t\t Validation Loss: 0.022049923427402973\n",
            "Epoch 35 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01138183248002787 \t\t Validation Loss: 0.08708254620432854\n",
            "Epoch 36 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010261894906933949 \t\t Validation Loss: 0.019569168726985272\n",
            "Epoch 37 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009966234975444103 \t\t Validation Loss: 0.05771622219337867\n",
            "Epoch 38 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009867543008530865 \t\t Validation Loss: 0.021158917162280817\n",
            "Epoch 39 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009467903310684738 \t\t Validation Loss: 0.07181333320645186\n",
            "Epoch 40 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009952385057191792 \t\t Validation Loss: 0.018674080439198475\n",
            "Epoch 41 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009352308625632242 \t\t Validation Loss: 0.07843350275204732\n",
            "Epoch 42 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009572467198784186 \t\t Validation Loss: 0.01807961569955716\n",
            "Epoch 43 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01049925309819849 \t\t Validation Loss: 0.0906745009124279\n",
            "Epoch 44 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.01085735413220686 \t\t Validation Loss: 0.01583914695164332\n",
            "Epoch 45 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009158497839864041 \t\t Validation Loss: 0.05443996305649097\n",
            "Epoch 46 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.008556253198106345 \t\t Validation Loss: 0.016038654515376456\n",
            "Epoch 47 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00799860168135146 \t\t Validation Loss: 0.06391000188887119\n",
            "Epoch 48 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009771726416688211 \t\t Validation Loss: 0.016708326060324907\n",
            "Epoch 49 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.009140426422016242 \t\t Validation Loss: 0.07749903803834549\n",
            "Epoch 50 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.010973956483114208 \t\t Validation Loss: 0.01721478604639952\n",
            "Epoch 51 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00865187284511489 \t\t Validation Loss: 0.057613925578502506\n",
            "Epoch 52 \t\t Epoch time: 0m 2s\n",
            "\t\t Training Loss: 0.00985466585513456 \t\t Validation Loss: 0.015298545647125978\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 180.07513086928935\n",
            "\n",
            "Accuracy: 52.53807106598985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLXuzL84Xp8a",
        "outputId": "5bb81549-e41a-4acc-b432-e51a6b5d6ad6"
      },
      "source": [
        "# train all complex with VADER -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"VADER\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = VADER()\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt', map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"\\n{model_name[0]}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Complex_1_Linear_108_VADER\n",
            "\n",
            "The model has 1,398,266 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8205133659161024 \t\t Validation Loss: 0.7502201039057511\n",
            "\t\t Validation Loss Decreased(inf--->0.750220) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.7022544966721153 \t\t Validation Loss: 0.8109293740529281\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.6017229962917799 \t\t Validation Loss: 0.8636668714193197\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.5130690282613442 \t\t Validation Loss: 0.9081500952060406\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.43547707647588607 \t\t Validation Loss: 0.9441705048084259\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.36838063059022297 \t\t Validation Loss: 0.9710197586279649\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.31145669346851473 \t\t Validation Loss: 0.9875978598227868\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.26427223741051714 \t\t Validation Loss: 0.9927999193851764\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.22603232538126208 \t\t Validation Loss: 0.9859708410042983\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1955420669998873 \t\t Validation Loss: 0.9672207603087792\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.17135426711693807 \t\t Validation Loss: 0.937488853931427\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.151993168926974 \t\t Validation Loss: 0.8983774185180664\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.13614770606462215 \t\t Validation Loss: 0.8518628340501052\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.12277648496957547 \t\t Validation Loss: 0.7999963806225703\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.11112483637092786 \t\t Validation Loss: 0.7446809388124026\n",
            "\t\t Validation Loss Decreased(0.750220--->0.744681) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10068461330720803 \t\t Validation Loss: 0.6875482568374047\n",
            "\t\t Validation Loss Decreased(0.744681--->0.687548) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.09113335213938577 \t\t Validation Loss: 0.6299246939329001\n",
            "\t\t Validation Loss Decreased(0.687548--->0.629925) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08227621543384786 \t\t Validation Loss: 0.5728513094095083\n",
            "\t\t Validation Loss Decreased(0.629925--->0.572851) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.074000995448866 \t\t Validation Loss: 0.5171308746704688\n",
            "\t\t Validation Loss Decreased(0.572851--->0.517131) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.06624701055949805 \t\t Validation Loss: 0.4633796696479504\n",
            "\t\t Validation Loss Decreased(0.517131--->0.463380) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.058984656229841746 \t\t Validation Loss: 0.41207157190029436\n",
            "\t\t Validation Loss Decreased(0.463380--->0.412072) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.05220268068306551 \t\t Validation Loss: 0.3635727602701921\n",
            "\t\t Validation Loss Decreased(0.412072--->0.363573) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.04590011660852847 \t\t Validation Loss: 0.31816568741431606\n",
            "\t\t Validation Loss Decreased(0.363573--->0.318166) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.040080732851591264 \t\t Validation Loss: 0.276062018596209\n",
            "\t\t Validation Loss Decreased(0.318166--->0.276062) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.034749276630857306 \t\t Validation Loss: 0.23741049720690802\n",
            "\t\t Validation Loss Decreased(0.276062--->0.237410) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.02990866105726643 \t\t Validation Loss: 0.20229903150063294\n",
            "\t\t Validation Loss Decreased(0.237410--->0.202299) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.02555796375963837 \t\t Validation Loss: 0.1707561337030851\n",
            "\t\t Validation Loss Decreased(0.202299--->0.170756) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.021691059152520186 \t\t Validation Loss: 0.14274971244426873\n",
            "\t\t Validation Loss Decreased(0.170756--->0.142750) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.01829551802192991 \t\t Validation Loss: 0.11818913255746548\n",
            "\t\t Validation Loss Decreased(0.142750--->0.118189) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.015352498425321805 \t\t Validation Loss: 0.09692745827711545\n",
            "\t\t Validation Loss Decreased(0.118189--->0.096927) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.012836774367511877 \t\t Validation Loss: 0.0787678687618329\n",
            "\t\t Validation Loss Decreased(0.096927--->0.078768) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.010717452217422024 \t\t Validation Loss: 0.0634710152561848\n",
            "\t\t Validation Loss Decreased(0.078768--->0.063471) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0089590935621763 \t\t Validation Loss: 0.05076623478761086\n",
            "\t\t Validation Loss Decreased(0.063471--->0.050766) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.007523085597927707 \t\t Validation Loss: 0.040363145275757864\n",
            "\t\t Validation Loss Decreased(0.050766--->0.040363) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.006369242520147079 \t\t Validation Loss: 0.03196433258171265\n",
            "\t\t Validation Loss Decreased(0.040363--->0.031964) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005457400495337474 \t\t Validation Loss: 0.02527696804071848\n",
            "\t\t Validation Loss Decreased(0.031964--->0.025277) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004748839494847768 \t\t Validation Loss: 0.0200228773487302\n",
            "\t\t Validation Loss Decreased(0.025277--->0.020023) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004207493101775243 \t\t Validation Loss: 0.015946602520461265\n",
            "\t\t Validation Loss Decreased(0.020023--->0.015947) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00380087284547453 \t\t Validation Loss: 0.012820755668844167\n",
            "\t\t Validation Loss Decreased(0.015947--->0.012821) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035005891506837026 \t\t Validation Loss: 0.010448650349504672\n",
            "\t\t Validation Loss Decreased(0.012821--->0.010449) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032825170940644034 \t\t Validation Loss: 0.008664718041053185\n",
            "\t\t Validation Loss Decreased(0.010449--->0.008665) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003126763576654264 \t\t Validation Loss: 0.007333086624454994\n",
            "\t\t Validation Loss Decreased(0.008665--->0.007333) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030173377489444574 \t\t Validation Loss: 0.006344650173559785\n",
            "\t\t Validation Loss Decreased(0.007333--->0.006345) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002941674500942935 \t\t Validation Loss: 0.005613792024982663\n",
            "\t\t Validation Loss Decreased(0.006345--->0.005614) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028901895165216883 \t\t Validation Loss: 0.005074420433419828\n",
            "\t\t Validation Loss Decreased(0.005614--->0.005074) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028556889124416018 \t\t Validation Loss: 0.0046764282026113225\n",
            "\t\t Validation Loss Decreased(0.005074--->0.004676) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002832916547934449 \t\t Validation Loss: 0.00438233576894093\n",
            "\t\t Validation Loss Decreased(0.004676--->0.004382) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002818099669860424 \t\t Validation Loss: 0.004164416334018684\n",
            "\t\t Validation Loss Decreased(0.004382--->0.004164) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028085833103899415 \t\t Validation Loss: 0.0040023061625946025\n",
            "\t\t Validation Loss Decreased(0.004164--->0.004002) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002802537496176523 \t\t Validation Loss: 0.003881168900988996\n",
            "\t\t Validation Loss Decreased(0.004002--->0.003881) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027987258791973865 \t\t Validation Loss: 0.003790233561840768\n",
            "\t\t Validation Loss Decreased(0.003881--->0.003790) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027963208582101238 \t\t Validation Loss: 0.0037216164100055513\n",
            "\t\t Validation Loss Decreased(0.003790--->0.003722) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 118.25702926713198\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "\n",
            "Complex_1_LSTM_92_VADER\n",
            "\n",
            "The model has 2,164,138 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.6349756968635563 \t\t Validation Loss: 0.35434219470390904\n",
            "\t\t Validation Loss Decreased(inf--->0.354342) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.5251613865744926 \t\t Validation Loss: 0.4064820959017827\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.42874730027255575 \t\t Validation Loss: 0.44914360229785627\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.34612737223505974 \t\t Validation Loss: 0.4802494771205462\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.27575425043500756 \t\t Validation Loss: 0.4973233754818256\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.21691633618713632 \t\t Validation Loss: 0.49833897673166716\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.17118622742693973 \t\t Validation Loss: 0.48377135396003723\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1349831709254382 \t\t Validation Loss: 0.4548556414934305\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10702889041056403 \t\t Validation Loss: 0.41467021291072553\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08494263385246331 \t\t Validation Loss: 0.3667385452068769\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.06780507842808761 \t\t Validation Loss: 0.31533224880695343\n",
            "\t\t Validation Loss Decreased(0.354342--->0.315332) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.054549211544941203 \t\t Validation Loss: 0.26396996126725125\n",
            "\t\t Validation Loss Decreased(0.315332--->0.263970) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.04368305045205194 \t\t Validation Loss: 0.21516609077270216\n",
            "\t\t Validation Loss Decreased(0.263970--->0.215166) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03503207622309895 \t\t Validation Loss: 0.17079792859462592\n",
            "\t\t Validation Loss Decreased(0.215166--->0.170798) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.02758741336308319 \t\t Validation Loss: 0.13222859455988958\n",
            "\t\t Validation Loss Decreased(0.170798--->0.132229) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.021791177913483634 \t\t Validation Loss: 0.09971979604317592\n",
            "\t\t Validation Loss Decreased(0.132229--->0.099720) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.016973960402144772 \t\t Validation Loss: 0.07315491655698189\n",
            "\t\t Validation Loss Decreased(0.099720--->0.073155) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.013356588658171933 \t\t Validation Loss: 0.052102626516268805\n",
            "\t\t Validation Loss Decreased(0.073155--->0.052103) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.010486005287856568 \t\t Validation Loss: 0.03599469378017462\n",
            "\t\t Validation Loss Decreased(0.052103--->0.035995) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.008064547594561166 \t\t Validation Loss: 0.024122235365211964\n",
            "\t\t Validation Loss Decreased(0.035995--->0.024122) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.006335598448090054 \t\t Validation Loss: 0.01568911589968663\n",
            "\t\t Validation Loss Decreased(0.024122--->0.015689) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005246775492245483 \t\t Validation Loss: 0.009921146091073751\n",
            "\t\t Validation Loss Decreased(0.015689--->0.009921) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00425887612210637 \t\t Validation Loss: 0.006172924553259061\n",
            "\t\t Validation Loss Decreased(0.009921--->0.006173) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003707792123817411 \t\t Validation Loss: 0.0038233203634333154\n",
            "\t\t Validation Loss Decreased(0.006173--->0.003823) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003226933996130184 \t\t Validation Loss: 0.0025121428591843983\n",
            "\t\t Validation Loss Decreased(0.003823--->0.002512) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002963269072761004 \t\t Validation Loss: 0.0018494280162625588\n",
            "\t\t Validation Loss Decreased(0.002512--->0.001849) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029015141473202086 \t\t Validation Loss: 0.0015945497031610173\n",
            "\t\t Validation Loss Decreased(0.001849--->0.001595) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025350300664384222 \t\t Validation Loss: 0.001585904795389909\n",
            "\t\t Validation Loss Decreased(0.001595--->0.001586) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027049380382900504 \t\t Validation Loss: 0.0016993170407099219\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026676464854185847 \t\t Validation Loss: 0.0018711758580488653\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002718400435106879 \t\t Validation Loss: 0.0020506572372351703\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025668129296595784 \t\t Validation Loss: 0.002230749617760571\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024391063662069674 \t\t Validation Loss: 0.0023593875364615368\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027120862206494486 \t\t Validation Loss: 0.0024356942247742643\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00254128933915667 \t\t Validation Loss: 0.002562013168174487\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002600294992491301 \t\t Validation Loss: 0.0027140651274329196\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025020271848031394 \t\t Validation Loss: 0.0027630191941100815\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024927231196842686 \t\t Validation Loss: 0.0027669924669540846\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002592131688976912 \t\t Validation Loss: 0.0027076079098221203\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002685131166898016 \t\t Validation Loss: 0.0026791384533190955\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002523964360620625 \t\t Validation Loss: 0.002735333239588027\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002594741353941326 \t\t Validation Loss: 0.0027919746935367584\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00246261472411051 \t\t Validation Loss: 0.0028059476485046055\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026177991043172173 \t\t Validation Loss: 0.0028105568176565263\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002484260016825755 \t\t Validation Loss: 0.0027782150741236713\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002426631037598929 \t\t Validation Loss: 0.0028056701770625436\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025506476553886926 \t\t Validation Loss: 0.0027571073375069178\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002704383906077694 \t\t Validation Loss: 0.002760973427659617\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002482085163088364 \t\t Validation Loss: 0.0027461331695891344\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00254259816043683 \t\t Validation Loss: 0.002810378764899304\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026305109636883275 \t\t Validation Loss: 0.0028234824556140946\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025904321212422204 \t\t Validation Loss: 0.002759365660424989\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 95.13237626903553\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "\n",
            "Complex_1_LSTM_36_VADER\n",
            "\n",
            "The model has 545,658 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.9875520415654456 \t\t Validation Loss: 0.6155361372690934\n",
            "\t\t Validation Loss Decreased(inf--->0.615536) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8359071457683033 \t\t Validation Loss: 0.7376980506456815\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.7062928114917029 \t\t Validation Loss: 0.8571983346572289\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.59394673253032 \t\t Validation Loss: 0.968923596235422\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.4985683542311292 \t\t Validation Loss: 1.0647631241725042\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.4199919601197581 \t\t Validation Loss: 1.1372286860759442\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3565683075151331 \t\t Validation Loss: 1.1804269002034113\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.30599545548997215 \t\t Validation Loss: 1.192453581553239\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2654503352713545 \t\t Validation Loss: 1.1745010706094594\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.23240842545606397 \t\t Validation Loss: 1.131076413851518\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.20421599687353983 \t\t Validation Loss: 1.0669157871833215\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.180282058546672 \t\t Validation Loss: 0.9887559964106634\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.15931703673512046 \t\t Validation Loss: 0.9020545986982492\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1401689448802908 \t\t Validation Loss: 0.8106857102650863\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.12265001963572325 \t\t Validation Loss: 0.7185543362910931\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10706642021840387 \t\t Validation Loss: 0.6282940277686486\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.09196477771950634 \t\t Validation Loss: 0.5417211674726926\n",
            "\t\t Validation Loss Decreased(0.615536--->0.541721) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.07857313389713699 \t\t Validation Loss: 0.4606026296432202\n",
            "\t\t Validation Loss Decreased(0.541721--->0.460603) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0666355748874815 \t\t Validation Loss: 0.3859535753726959\n",
            "\t\t Validation Loss Decreased(0.460603--->0.385954) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.05622074421734322 \t\t Validation Loss: 0.3186009262616818\n",
            "\t\t Validation Loss Decreased(0.385954--->0.318601) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.04597084615316645 \t\t Validation Loss: 0.25871365460065693\n",
            "\t\t Validation Loss Decreased(0.318601--->0.258714) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.037851945094361496 \t\t Validation Loss: 0.20695508729953033\n",
            "\t\t Validation Loss Decreased(0.258714--->0.206955) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03084725298528635 \t\t Validation Loss: 0.16267762390466836\n",
            "\t\t Validation Loss Decreased(0.206955--->0.162678) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.024379167483955924 \t\t Validation Loss: 0.125477634370327\n",
            "\t\t Validation Loss Decreased(0.162678--->0.125478) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.01969483229205818 \t\t Validation Loss: 0.09497809753968166\n",
            "\t\t Validation Loss Decreased(0.125478--->0.094978) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.015765378393257992 \t\t Validation Loss: 0.07054142768566425\n",
            "\t\t Validation Loss Decreased(0.094978--->0.070541) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.011917973086678397 \t\t Validation Loss: 0.05147249323244278\n",
            "\t\t Validation Loss Decreased(0.070541--->0.051472) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0099576999200508 \t\t Validation Loss: 0.03686403569120627\n",
            "\t\t Validation Loss Decreased(0.051472--->0.036864) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0076357446189315335 \t\t Validation Loss: 0.025795729401019905\n",
            "\t\t Validation Loss Decreased(0.036864--->0.025796) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.006400893234672981 \t\t Validation Loss: 0.01781314969635927\n",
            "\t\t Validation Loss Decreased(0.025796--->0.017813) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005331808424277886 \t\t Validation Loss: 0.01241513889712783\n",
            "\t\t Validation Loss Decreased(0.017813--->0.012415) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004527072145326718 \t\t Validation Loss: 0.008477461667588124\n",
            "\t\t Validation Loss Decreased(0.012415--->0.008477) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0038819106585169967 \t\t Validation Loss: 0.005857440463912029\n",
            "\t\t Validation Loss Decreased(0.008477--->0.005857) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003830727913442093 \t\t Validation Loss: 0.0041281466300670915\n",
            "\t\t Validation Loss Decreased(0.005857--->0.004128) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032210390152711725 \t\t Validation Loss: 0.003101025224448397\n",
            "\t\t Validation Loss Decreased(0.004128--->0.003101) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032057027533851767 \t\t Validation Loss: 0.0024668030136336502\n",
            "\t\t Validation Loss Decreased(0.003101--->0.002467) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029963648522499243 \t\t Validation Loss: 0.0020878966987276306\n",
            "\t\t Validation Loss Decreased(0.002467--->0.002088) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032286811494142624 \t\t Validation Loss: 0.0019030243722507013\n",
            "\t\t Validation Loss Decreased(0.002088--->0.001903) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003072948695311474 \t\t Validation Loss: 0.0017670579254627228\n",
            "\t\t Validation Loss Decreased(0.001903--->0.001767) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030939086431612237 \t\t Validation Loss: 0.001706460347542396\n",
            "\t\t Validation Loss Decreased(0.001767--->0.001706) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002944760471330704 \t\t Validation Loss: 0.001692163037100377\n",
            "\t\t Validation Loss Decreased(0.001706--->0.001692) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00282064318103162 \t\t Validation Loss: 0.001693718869668933\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002818434511789599 \t\t Validation Loss: 0.0017005439507416808\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002823080079365723 \t\t Validation Loss: 0.0017037414184484917\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002779218851198518 \t\t Validation Loss: 0.001717222290328489\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030446485867975533 \t\t Validation Loss: 0.0017241169385110529\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029423081778568796 \t\t Validation Loss: 0.0017341726790898694\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002954150185089659 \t\t Validation Loss: 0.0017226053706298654\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003143404774065759 \t\t Validation Loss: 0.00173754928758941\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002814917384672004 \t\t Validation Loss: 0.0017386490407471473\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002852794396489657 \t\t Validation Loss: 0.0017402285772662323\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031369532272848928 \t\t Validation Loss: 0.0017273222955946738\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 212.26048937182742\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "\n",
            "Complex_2_Linear_108_VADER\n",
            "\n",
            "The model has 1,401,290 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.7444993202650064 \t\t Validation Loss: 0.9428436228862176\n",
            "\t\t Validation Loss Decreased(inf--->0.942844) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.6347386853366688 \t\t Validation Loss: 1.0522046089172363\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.5332937689837873 \t\t Validation Loss: 1.1597776275414686\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.4334902112321878 \t\t Validation Loss: 1.256542146205902\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3437999136670417 \t\t Validation Loss: 1.320566910963792\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2736331763475932 \t\t Validation Loss: 1.328623652458191\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.22456786979770138 \t\t Validation Loss: 1.2761771770624013\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.19064194677598975 \t\t Validation Loss: 1.1797262246792133\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.16495338280219585 \t\t Validation Loss: 1.0598689776200514\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1431284081473997 \t\t Validation Loss: 0.9301930620120122\n",
            "\t\t Validation Loss Decreased(0.942844--->0.930193) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.12305555164474188 \t\t Validation Loss: 0.7976699288074787\n",
            "\t\t Validation Loss Decreased(0.930193--->0.797670) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10392707055182876 \t\t Validation Loss: 0.6662218364385458\n",
            "\t\t Validation Loss Decreased(0.797670--->0.666222) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08561465827546813 \t\t Validation Loss: 0.539081892141929\n",
            "\t\t Validation Loss Decreased(0.666222--->0.539082) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.06834012371286549 \t\t Validation Loss: 0.41972310267961943\n",
            "\t\t Validation Loss Decreased(0.539082--->0.419723) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.052503453183768166 \t\t Validation Loss: 0.3119236139150766\n",
            "\t\t Validation Loss Decreased(0.419723--->0.311924) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03856844251119607 \t\t Validation Loss: 0.21931549333609068\n",
            "\t\t Validation Loss Decreased(0.311924--->0.219315) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.026951106211699143 \t\t Validation Loss: 0.14461373595091012\n",
            "\t\t Validation Loss Decreased(0.219315--->0.144614) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.017901043904743886 \t\t Validation Loss: 0.08879786185347117\n",
            "\t\t Validation Loss Decreased(0.144614--->0.088798) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0114059786696138 \t\t Validation Loss: 0.050652797118975565\n",
            "\t\t Validation Loss Decreased(0.088798--->0.050653) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.007168405752226307 \t\t Validation Loss: 0.02703062788798259\n",
            "\t\t Validation Loss Decreased(0.050653--->0.027031) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004682448384276516 \t\t Validation Loss: 0.013817931382128825\n",
            "\t\t Validation Loss Decreased(0.027031--->0.013818) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033806130554325676 \t\t Validation Loss: 0.0071064615980363805\n",
            "\t\t Validation Loss Decreased(0.013818--->0.007106) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002773898744930488 \t\t Validation Loss: 0.0039617103040934755\n",
            "\t\t Validation Loss Decreased(0.007106--->0.003962) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025224242519864158 \t\t Validation Loss: 0.002567095140245958\n",
            "\t\t Validation Loss Decreased(0.003962--->0.002567) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002430025235642805 \t\t Validation Loss: 0.001962314807594969\n",
            "\t\t Validation Loss Decreased(0.002567--->0.001962) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024005036079010147 \t\t Validation Loss: 0.0016972103711360921\n",
            "\t\t Validation Loss Decreased(0.001962--->0.001697) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023929305316414684 \t\t Validation Loss: 0.0015769559496010726\n",
            "\t\t Validation Loss Decreased(0.001697--->0.001577) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002391989995738038 \t\t Validation Loss: 0.001520049785120556\n",
            "\t\t Validation Loss Decreased(0.001577--->0.001520) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002392634262513319 \t\t Validation Loss: 0.0014920805742104466\n",
            "\t\t Validation Loss Decreased(0.001520--->0.001492) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002393447539165012 \t\t Validation Loss: 0.0014778436833204557\n",
            "\t\t Validation Loss Decreased(0.001492--->0.001478) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002394141145585407 \t\t Validation Loss: 0.0014702587616808999\n",
            "\t\t Validation Loss Decreased(0.001478--->0.001470) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023947118819263335 \t\t Validation Loss: 0.0014659007093331849\n",
            "\t\t Validation Loss Decreased(0.001470--->0.001466) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002395207093827225 \t\t Validation Loss: 0.0014630580038871043\n",
            "\t\t Validation Loss Decreased(0.001466--->0.001463) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002395665240033555 \t\t Validation Loss: 0.001460905766669804\n",
            "\t\t Validation Loss Decreased(0.001463--->0.001461) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002396110852714628 \t\t Validation Loss: 0.0014590638546416392\n",
            "\t\t Validation Loss Decreased(0.001461--->0.001459) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002396553644683917 \t\t Validation Loss: 0.0014573191471684438\n",
            "\t\t Validation Loss Decreased(0.001459--->0.001457) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002397001602189817 \t\t Validation Loss: 0.0014555974093337471\n",
            "\t\t Validation Loss Decreased(0.001457--->0.001456) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002397460039434451 \t\t Validation Loss: 0.0014538732111059988\n",
            "\t\t Validation Loss Decreased(0.001456--->0.001454) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002397927358066915 \t\t Validation Loss: 0.0014521218097517984\n",
            "\t\t Validation Loss Decreased(0.001454--->0.001452) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023984059595465157 \t\t Validation Loss: 0.0014503359987149732\n",
            "\t\t Validation Loss Decreased(0.001452--->0.001450) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023988975657187905 \t\t Validation Loss: 0.0014485192570226411\n",
            "\t\t Validation Loss Decreased(0.001450--->0.001449) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023994010446807116 \t\t Validation Loss: 0.0014466635027649598\n",
            "\t\t Validation Loss Decreased(0.001449--->0.001447) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0023999181851379675 \t\t Validation Loss: 0.0014447710284282668\n",
            "\t\t Validation Loss Decreased(0.001447--->0.001445) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024004500135905234 \t\t Validation Loss: 0.0014428377191786869\n",
            "\t\t Validation Loss Decreased(0.001445--->0.001443) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002400994735826562 \t\t Validation Loss: 0.001440869603986637\n",
            "\t\t Validation Loss Decreased(0.001443--->0.001441) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024015557601406064 \t\t Validation Loss: 0.0014388592054064458\n",
            "\t\t Validation Loss Decreased(0.001441--->0.001439) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002402131739890555 \t\t Validation Loss: 0.0014368028451616948\n",
            "\t\t Validation Loss Decreased(0.001439--->0.001437) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00240272402171851 \t\t Validation Loss: 0.0014347063955098677\n",
            "\t\t Validation Loss Decreased(0.001437--->0.001435) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002403332672484622 \t\t Validation Loss: 0.0014325631804800092\n",
            "\t\t Validation Loss Decreased(0.001435--->0.001433) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002403960489302974 \t\t Validation Loss: 0.001430374433626779\n",
            "\t\t Validation Loss Decreased(0.001433--->0.001430) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024046069601034695 \t\t Validation Loss: 0.0014281452234941893\n",
            "\t\t Validation Loss Decreased(0.001430--->0.001428) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024052715531512592 \t\t Validation Loss: 0.0014258721135914899\n",
            "\t\t Validation Loss Decreased(0.001428--->0.001426) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 93.58279465418782\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "\n",
            "Complex_2_LSTM_92_VADER\n",
            "\n",
            "The model has 2,167,162 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 1.0813959221657667 \t\t Validation Loss: 0.5233969585253642\n",
            "\t\t Validation Loss Decreased(inf--->0.523397) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.931898082707178 \t\t Validation Loss: 0.6067376480652735\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.7776553317010906 \t\t Validation Loss: 0.7121313879123101\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.6094575845258864 \t\t Validation Loss: 0.8463624647030463\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.44421254551491224 \t\t Validation Loss: 0.9900471522257879\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3130957574923397 \t\t Validation Loss: 1.084785663164579\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2295870143366424 \t\t Validation Loss: 1.082398873109084\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.18181214699320294 \t\t Validation Loss: 1.000942661212041\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.15148141109888014 \t\t Validation Loss: 0.8840237030616174\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.12879351957078167 \t\t Validation Loss: 0.7586844471784738\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10863303758417936 \t\t Validation Loss: 0.6343300480108994\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.09012549583063889 \t\t Validation Loss: 0.5155251988997827\n",
            "\t\t Validation Loss Decreased(0.523397--->0.515525) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.07369096367305296 \t\t Validation Loss: 0.4051326880088219\n",
            "\t\t Validation Loss Decreased(0.515525--->0.405133) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.057810440477034125 \t\t Validation Loss: 0.3053869731151141\n",
            "\t\t Validation Loss Decreased(0.405133--->0.305387) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.04391519220331935 \t\t Validation Loss: 0.21890760041200197\n",
            "\t\t Validation Loss Decreased(0.305387--->0.218908) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03169506941796155 \t\t Validation Loss: 0.14795557180276284\n",
            "\t\t Validation Loss Decreased(0.218908--->0.147956) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.02253773282093273 \t\t Validation Loss: 0.09330131715306869\n",
            "\t\t Validation Loss Decreased(0.147956--->0.093301) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.01545750190868873 \t\t Validation Loss: 0.05395249936443109\n",
            "\t\t Validation Loss Decreased(0.093301--->0.053952) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.010243863517078696 \t\t Validation Loss: 0.028321283224683542\n",
            "\t\t Validation Loss Decreased(0.053952--->0.028321) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0067585012497934135 \t\t Validation Loss: 0.013569859214700185\n",
            "\t\t Validation Loss Decreased(0.028321--->0.013570) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004762461015677734 \t\t Validation Loss: 0.005945128890184255\n",
            "\t\t Validation Loss Decreased(0.013570--->0.005945) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003506815020070487 \t\t Validation Loss: 0.0027751342142717196\n",
            "\t\t Validation Loss Decreased(0.005945--->0.002775) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030180317620318886 \t\t Validation Loss: 0.0018183645219183885\n",
            "\t\t Validation Loss Decreased(0.002775--->0.001818) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002858041875328667 \t\t Validation Loss: 0.0018163044319058268\n",
            "\t\t Validation Loss Decreased(0.001818--->0.001816) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002635134616866708 \t\t Validation Loss: 0.0021249275260533276\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002543251635821385 \t\t Validation Loss: 0.002479803470823054\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027399852138792944 \t\t Validation Loss: 0.002673672870374643\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002681360882077668 \t\t Validation Loss: 0.002844606964992216\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002736659426355382 \t\t Validation Loss: 0.0028107846693064156\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025635374508596754 \t\t Validation Loss: 0.002893317974387453\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025704335201078574 \t\t Validation Loss: 0.002899624810267526\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002572854724398034 \t\t Validation Loss: 0.0029058612059228695\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00243890391524277 \t\t Validation Loss: 0.002983429263202617\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026054069119807637 \t\t Validation Loss: 0.002923930368314569\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026141503570928566 \t\t Validation Loss: 0.002959407615260436\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002496708127846186 \t\t Validation Loss: 0.0030211756776015344\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002554492065305444 \t\t Validation Loss: 0.003030050814581605\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025939400704275513 \t\t Validation Loss: 0.0031746065673919823\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026938107377547467 \t\t Validation Loss: 0.0032328262817687714\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002614896797977791 \t\t Validation Loss: 0.0031443532162274304\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025990688858704794 \t\t Validation Loss: 0.0031314654580245796\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025709053293827013 \t\t Validation Loss: 0.0031710157367902305\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025875866067016848 \t\t Validation Loss: 0.0031799232282747445\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025630201578039575 \t\t Validation Loss: 0.003033082941188835\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002547778867222872 \t\t Validation Loss: 0.0030194887479480645\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025913309906861067 \t\t Validation Loss: 0.003063038161669213\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002626436799481461 \t\t Validation Loss: 0.003166319408382361\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002629850744081007 \t\t Validation Loss: 0.003143086939906845\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025968531892332876 \t\t Validation Loss: 0.003037814346428674\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002655179735361221 \t\t Validation Loss: 0.003111676312983036\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026855450532575313 \t\t Validation Loss: 0.0032076156393696484\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002625181629146273 \t\t Validation Loss: 0.0032008769026455972\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 99.35473508883248\n",
            "\n",
            "Accuracy: 51.776649746192895\n",
            "\n",
            "\n",
            "Complex_2_LSTM_36_VADER\n",
            "\n",
            "The model has 548,682 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8271948002228463 \t\t Validation Loss: 1.108174672493568\n",
            "\t\t Validation Loss Decreased(inf--->1.108175) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.6539113538062854 \t\t Validation Loss: 1.2157772871164174\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.5235366115497576 \t\t Validation Loss: 1.3189357656698961\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.41753392053667354 \t\t Validation Loss: 1.4072013268103967\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3363623514997999 \t\t Validation Loss: 1.4646487694520216\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2790955270552454 \t\t Validation Loss: 1.4783397546181312\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.24156291854592996 \t\t Validation Loss: 1.4482524807636554\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2159242909968 \t\t Validation Loss: 1.3850029340157142\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.19666552560630482 \t\t Validation Loss: 1.3011468282112708\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.17998756002634764 \t\t Validation Loss: 1.2055548429489136\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1643431422338393 \t\t Validation Loss: 1.1030106315245995\n",
            "\t\t Validation Loss Decreased(1.108175--->1.103011) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.14939206016778545 \t\t Validation Loss: 0.9964175407703106\n",
            "\t\t Validation Loss Decreased(1.103011--->0.996418) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.13423245624211189 \t\t Validation Loss: 0.8867975244155297\n",
            "\t\t Validation Loss Decreased(0.996418--->0.886798) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.11888746624636287 \t\t Validation Loss: 0.7756948241820703\n",
            "\t\t Validation Loss Decreased(0.886798--->0.775695) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.10389034745812013 \t\t Validation Loss: 0.6646955586396731\n",
            "\t\t Validation Loss Decreased(0.775695--->0.664696) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08857120539539971 \t\t Validation Loss: 0.5556519696345696\n",
            "\t\t Validation Loss Decreased(0.664696--->0.555652) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.07453318966577786 \t\t Validation Loss: 0.45131261073626006\n",
            "\t\t Validation Loss Decreased(0.555652--->0.451313) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.05984702581741117 \t\t Validation Loss: 0.3540143852050488\n",
            "\t\t Validation Loss Decreased(0.451313--->0.354014) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.04711953376898089 \t\t Validation Loss: 0.2666775928093837\n",
            "\t\t Validation Loss Decreased(0.354014--->0.266678) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03601013724314603 \t\t Validation Loss: 0.19151220126793936\n",
            "\t\t Validation Loss Decreased(0.266678--->0.191512) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.026939066331456037 \t\t Validation Loss: 0.13027683760111147\n",
            "\t\t Validation Loss Decreased(0.191512--->0.130277) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.019230931062830258 \t\t Validation Loss: 0.08348352404741141\n",
            "\t\t Validation Loss Decreased(0.130277--->0.083484) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.012772149035103014 \t\t Validation Loss: 0.05035835189314989\n",
            "\t\t Validation Loss Decreased(0.083484--->0.050358) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.009071211922108321 \t\t Validation Loss: 0.028967734139699202\n",
            "\t\t Validation Loss Decreased(0.050358--->0.028968) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00634533807169646 \t\t Validation Loss: 0.015997514916727178\n",
            "\t\t Validation Loss Decreased(0.028968--->0.015998) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005096488720359835 \t\t Validation Loss: 0.008759662544784637\n",
            "\t\t Validation Loss Decreased(0.015998--->0.008760) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004351326443783536 \t\t Validation Loss: 0.005301529899812662\n",
            "\t\t Validation Loss Decreased(0.008760--->0.005302) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003841797387026049 \t\t Validation Loss: 0.0037058055884419726\n",
            "\t\t Validation Loss Decreased(0.005302--->0.003706) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003548039523003673 \t\t Validation Loss: 0.003148827549571601\n",
            "\t\t Validation Loss Decreased(0.003706--->0.003149) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003696734123435375 \t\t Validation Loss: 0.0029896725004968736\n",
            "\t\t Validation Loss Decreased(0.003149--->0.002990) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033739511933605615 \t\t Validation Loss: 0.00298697198741138\n",
            "\t\t Validation Loss Decreased(0.002990--->0.002987) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035922649140293535 \t\t Validation Loss: 0.0030234192784589073\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034904231071925243 \t\t Validation Loss: 0.0030572858388320757\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032193433820597223 \t\t Validation Loss: 0.003084110597578379\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035950985856706627 \t\t Validation Loss: 0.003080985535724232\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003328594605664949 \t\t Validation Loss: 0.003048650875615959\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003489231168468659 \t\t Validation Loss: 0.0030473951841346347\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033995853406628847 \t\t Validation Loss: 0.0030324197499654614\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034350134035874462 \t\t Validation Loss: 0.0030668296206455966\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035478268306375154 \t\t Validation Loss: 0.0030700126035998645\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034231020820427787 \t\t Validation Loss: 0.0030750523482520995\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034405191765619894 \t\t Validation Loss: 0.003160835951208495\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00361476355937083 \t\t Validation Loss: 0.0031099611123164114\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035708239962774758 \t\t Validation Loss: 0.00308280846533867\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032529945397195784 \t\t Validation Loss: 0.0030228295632136557\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035403963253908865 \t\t Validation Loss: 0.0030596764722409155\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032638738756193904 \t\t Validation Loss: 0.0030763316004035566\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0036524746699158 \t\t Validation Loss: 0.003054148401133716\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032652437394308682 \t\t Validation Loss: 0.0030047448459439552\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031371046663136097 \t\t Validation Loss: 0.0030501565555683696\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003592748520581203 \t\t Validation Loss: 0.003095796269078094\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034758796006113895 \t\t Validation Loss: 0.003048452056156328\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 225.78799175126903\n",
            "\n",
            "Accuracy: 48.73096446700508\n",
            "\n",
            "\n",
            "Complex_3_Linear_108_VADER\n",
            "\n",
            "The model has 1,458,682 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.9728265903210519 \t\t Validation Loss: 0.769251151726796\n",
            "\t\t Validation Loss Decreased(inf--->0.769251) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.7513606792718575 \t\t Validation Loss: 1.137657642364502\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.48802147616909164 \t\t Validation Loss: 1.5192586458646333\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3091078601160867 \t\t Validation Loss: 1.4002644465519831\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.24162352677887758 \t\t Validation Loss: 1.1800089111694922\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1899951052099995 \t\t Validation Loss: 0.9463021205021784\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.13173329939348372 \t\t Validation Loss: 0.7116226164194254\n",
            "\t\t Validation Loss Decreased(0.769251--->0.711623) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.07652648040282263 \t\t Validation Loss: 0.5367433933111337\n",
            "\t\t Validation Loss Decreased(0.711623--->0.536743) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.033282256043339904 \t\t Validation Loss: 0.420324275126824\n",
            "\t\t Validation Loss Decreased(0.536743--->0.420324) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.008880768104997539 \t\t Validation Loss: 0.35238326226289457\n",
            "\t\t Validation Loss Decreased(0.420324--->0.352383) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004469682884125694 \t\t Validation Loss: 0.33673210327441877\n",
            "\t\t Validation Loss Decreased(0.352383--->0.336732) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005020024816543367 \t\t Validation Loss: 0.3303288364639649\n",
            "\t\t Validation Loss Decreased(0.336732--->0.330329) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0041144587322943715 \t\t Validation Loss: 0.3169216602467574\n",
            "\t\t Validation Loss Decreased(0.330329--->0.316922) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0037786301895598503 \t\t Validation Loss: 0.30233048045864475\n",
            "\t\t Validation Loss Decreased(0.316922--->0.302330) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003601361280055465 \t\t Validation Loss: 0.2899955390737607\n",
            "\t\t Validation Loss Decreased(0.302330--->0.289996) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003444640205015202 \t\t Validation Loss: 0.27882515581754536\n",
            "\t\t Validation Loss Decreased(0.289996--->0.278825) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003298707414934462 \t\t Validation Loss: 0.26756469074350137\n",
            "\t\t Validation Loss Decreased(0.278825--->0.267565) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031687053804587877 \t\t Validation Loss: 0.25712294675982916\n",
            "\t\t Validation Loss Decreased(0.267565--->0.257123) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003050594683421927 \t\t Validation Loss: 0.24710139231039926\n",
            "\t\t Validation Loss Decreased(0.257123--->0.247101) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00294945651281474 \t\t Validation Loss: 0.23803249545968497\n",
            "\t\t Validation Loss Decreased(0.247101--->0.238032) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002863969684521844 \t\t Validation Loss: 0.2295130456869419\n",
            "\t\t Validation Loss Decreased(0.238032--->0.229513) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002788610621103765 \t\t Validation Loss: 0.2218409078912093\n",
            "\t\t Validation Loss Decreased(0.229513--->0.221841) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027248001424595714 \t\t Validation Loss: 0.2146275847290571\n",
            "\t\t Validation Loss Decreased(0.221841--->0.214628) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026715943327085493 \t\t Validation Loss: 0.20792719865074524\n",
            "\t\t Validation Loss Decreased(0.214628--->0.207927) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002629127325899144 \t\t Validation Loss: 0.20147263444960117\n",
            "\t\t Validation Loss Decreased(0.207927--->0.201473) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025929428976834627 \t\t Validation Loss: 0.19514814090843385\n",
            "\t\t Validation Loss Decreased(0.201473--->0.195148) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025621233342806933 \t\t Validation Loss: 0.18909507065724868\n",
            "\t\t Validation Loss Decreased(0.195148--->0.189095) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002539454831905361 \t\t Validation Loss: 0.18385534446973067\n",
            "\t\t Validation Loss Decreased(0.189095--->0.183855) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025175141596952707 \t\t Validation Loss: 0.17893647216260433\n",
            "\t\t Validation Loss Decreased(0.183855--->0.178936) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002503497033644266 \t\t Validation Loss: 0.1742591683824475\n",
            "\t\t Validation Loss Decreased(0.178936--->0.174259) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024873800355487983 \t\t Validation Loss: 0.17000504138951117\n",
            "\t\t Validation Loss Decreased(0.174259--->0.170005) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024795425376175225 \t\t Validation Loss: 0.16601676730295786\n",
            "\t\t Validation Loss Decreased(0.170005--->0.166017) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002467325160191772 \t\t Validation Loss: 0.1621941952034831\n",
            "\t\t Validation Loss Decreased(0.166017--->0.162194) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024608651465560134 \t\t Validation Loss: 0.1585688699896519\n",
            "\t\t Validation Loss Decreased(0.162194--->0.158569) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002451646262842759 \t\t Validation Loss: 0.1553190738464204\n",
            "\t\t Validation Loss Decreased(0.158569--->0.155319) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002446899885890653 \t\t Validation Loss: 0.15230684190128857\n",
            "\t\t Validation Loss Decreased(0.155319--->0.152307) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002439577053804454 \t\t Validation Loss: 0.14954710400734955\n",
            "\t\t Validation Loss Decreased(0.152307--->0.149547) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024353617273673816 \t\t Validation Loss: 0.14698630582111386\n",
            "\t\t Validation Loss Decreased(0.149547--->0.146986) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002430696872604155 \t\t Validation Loss: 0.14458984415978193\n",
            "\t\t Validation Loss Decreased(0.146986--->0.144590) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002426990754650654 \t\t Validation Loss: 0.1424104105681181\n",
            "\t\t Validation Loss Decreased(0.144590--->0.142410) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024239455311987046 \t\t Validation Loss: 0.14032706166975772\n",
            "\t\t Validation Loss Decreased(0.142410--->0.140327) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024208182097739867 \t\t Validation Loss: 0.13841169112576887\n",
            "\t\t Validation Loss Decreased(0.140327--->0.138412) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002418201633829724 \t\t Validation Loss: 0.13650912401051477\n",
            "\t\t Validation Loss Decreased(0.138412--->0.136509) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002415758972937191 \t\t Validation Loss: 0.13479558331891894\n",
            "\t\t Validation Loss Decreased(0.136509--->0.134796) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024142357618296265 \t\t Validation Loss: 0.13315179019879836\n",
            "\t\t Validation Loss Decreased(0.134796--->0.133152) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024126701723985577 \t\t Validation Loss: 0.1316177694556805\n",
            "\t\t Validation Loss Decreased(0.133152--->0.131618) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002410627750528825 \t\t Validation Loss: 0.13019683939189866\n",
            "\t\t Validation Loss Decreased(0.131618--->0.130197) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024103696782116755 \t\t Validation Loss: 0.12874003840037263\n",
            "\t\t Validation Loss Decreased(0.130197--->0.128740) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024086280546553834 \t\t Validation Loss: 0.127419308723452\n",
            "\t\t Validation Loss Decreased(0.128740--->0.127419) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002408445992104306 \t\t Validation Loss: 0.12613719486846373\n",
            "\t\t Validation Loss Decreased(0.127419--->0.126137) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024078460504628114 \t\t Validation Loss: 0.12495476374617563\n",
            "\t\t Validation Loss Decreased(0.126137--->0.124955) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024075276498380744 \t\t Validation Loss: 0.12383062242028806\n",
            "\t\t Validation Loss Decreased(0.124955--->0.123831) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024080390890909207 \t\t Validation Loss: 0.12270894069940998\n",
            "\t\t Validation Loss Decreased(0.123831--->0.122709) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024073407466120617 \t\t Validation Loss: 0.12172893882514192\n",
            "\t\t Validation Loss Decreased(0.122709--->0.121729) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024082214442534824 \t\t Validation Loss: 0.12079301857962631\n",
            "\t\t Validation Loss Decreased(0.121729--->0.120793) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024082588277304093 \t\t Validation Loss: 0.1198489145339968\n",
            "\t\t Validation Loss Decreased(0.120793--->0.119849) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024077103400879816 \t\t Validation Loss: 0.11900644581048535\n",
            "\t\t Validation Loss Decreased(0.119849--->0.119006) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024101051228196434 \t\t Validation Loss: 0.11810725425871518\n",
            "\t\t Validation Loss Decreased(0.119006--->0.118107) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024094429320177515 \t\t Validation Loss: 0.11739399432777785\n",
            "\t\t Validation Loss Decreased(0.118107--->0.117394) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002412501211290726 \t\t Validation Loss: 0.1165978277925975\n",
            "\t\t Validation Loss Decreased(0.117394--->0.116598) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024134410133056744 \t\t Validation Loss: 0.11586976192819957\n",
            "\t\t Validation Loss Decreased(0.116598--->0.115870) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024168175673489837 \t\t Validation Loss: 0.11515732378197405\n",
            "\t\t Validation Loss Decreased(0.115870--->0.115157) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024173497505536354 \t\t Validation Loss: 0.11453927542942648\n",
            "\t\t Validation Loss Decreased(0.115157--->0.114539) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002424065102319661 \t\t Validation Loss: 0.11387270379166764\n",
            "\t\t Validation Loss Decreased(0.114539--->0.113873) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002424309782155261 \t\t Validation Loss: 0.11333338439894411\n",
            "\t\t Validation Loss Decreased(0.113873--->0.113333) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024313741423365835 \t\t Validation Loss: 0.112731231107878\n",
            "\t\t Validation Loss Decreased(0.113333--->0.112731) \t Saving The Model\n",
            "Epoch 67 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024300059791327127 \t\t Validation Loss: 0.1122745038368381\n",
            "\t\t Validation Loss Decreased(0.112731--->0.112275) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024416414325477906 \t\t Validation Loss: 0.1118049467734706\n",
            "\t\t Validation Loss Decreased(0.112275--->0.111805) \t Saving The Model\n",
            "Epoch 69 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024427749937073 \t\t Validation Loss: 0.11138509894506289\n",
            "\t\t Validation Loss Decreased(0.111805--->0.111385) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002458756599210303 \t\t Validation Loss: 0.11085289695228522\n",
            "\t\t Validation Loss Decreased(0.111385--->0.110853) \t Saving The Model\n",
            "Epoch 71 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024571310008271925 \t\t Validation Loss: 0.1105730096057344\n",
            "\t\t Validation Loss Decreased(0.110853--->0.110573) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024817148254982924 \t\t Validation Loss: 0.11005212291358756\n",
            "\t\t Validation Loss Decreased(0.110573--->0.110052) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024766927109905394 \t\t Validation Loss: 0.10990315215447201\n",
            "\t\t Validation Loss Decreased(0.110052--->0.109903) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002512807460703157 \t\t Validation Loss: 0.10941698008145277\n",
            "\t\t Validation Loss Decreased(0.109903--->0.109417) \t Saving The Model\n",
            "Epoch 75 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0024972151228654627 \t\t Validation Loss: 0.10944783804006875\n",
            "Epoch 76 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00254737218917423 \t\t Validation Loss: 0.10878430772572756\n",
            "\t\t Validation Loss Decreased(0.109417--->0.108784) \t Saving The Model\n",
            "Epoch 77 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002523913506003147 \t\t Validation Loss: 0.10893927401719758\n",
            "Epoch 78 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025926739185092016 \t\t Validation Loss: 0.108288105774241\n",
            "\t\t Validation Loss Decreased(0.108784--->0.108288) \t Saving The Model\n",
            "Epoch 79 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002558459578132307 \t\t Validation Loss: 0.10859994841023134\n",
            "Epoch 80 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026463205498215314 \t\t Validation Loss: 0.10776288690976799\n",
            "\t\t Validation Loss Decreased(0.108288--->0.107763) \t Saving The Model\n",
            "Epoch 81 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025888765655533484 \t\t Validation Loss: 0.10837924005821921\n",
            "Epoch 82 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027102163719088844 \t\t Validation Loss: 0.10750004040220609\n",
            "\t\t Validation Loss Decreased(0.107763--->0.107500) \t Saving The Model\n",
            "Epoch 83 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026244218366824694 \t\t Validation Loss: 0.10833537918873705\n",
            "Epoch 84 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002788345452171524 \t\t Validation Loss: 0.10714377238988303\n",
            "\t\t Validation Loss Decreased(0.107500--->0.107144) \t Saving The Model\n",
            "Epoch 85 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026557952845217407 \t\t Validation Loss: 0.1083569704877356\n",
            "Epoch 86 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028784684333446864 \t\t Validation Loss: 0.10695635770949033\n",
            "\t\t Validation Loss Decreased(0.107144--->0.106956) \t Saving The Model\n",
            "Epoch 87 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026860092011456553 \t\t Validation Loss: 0.1085106059192465\n",
            "Epoch 88 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029794591104586586 \t\t Validation Loss: 0.10652842743393894\n",
            "\t\t Validation Loss Decreased(0.106956--->0.106528) \t Saving The Model\n",
            "Epoch 89 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026852298131514645 \t\t Validation Loss: 0.1088830335244823\n",
            "Epoch 90 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030810437574227516 \t\t Validation Loss: 0.10602204843710822\n",
            "\t\t Validation Loss Decreased(0.106528--->0.106022) \t Saving The Model\n",
            "Epoch 91 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026571440095132268 \t\t Validation Loss: 0.10923100898687083\n",
            "Epoch 92 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031771322978169634 \t\t Validation Loss: 0.10536691089733861\n",
            "\t\t Validation Loss Decreased(0.106022--->0.105367) \t Saving The Model\n",
            "Epoch 93 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026059909786626295 \t\t Validation Loss: 0.10985963100280899\n",
            "Epoch 94 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032866196360791453 \t\t Validation Loss: 0.10433699602547747\n",
            "\t\t Validation Loss Decreased(0.105367--->0.104337) \t Saving The Model\n",
            "Epoch 95 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0025467111515132964 \t\t Validation Loss: 0.11098960862280084\n",
            "Epoch 96 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0034180506072762247 \t\t Validation Loss: 0.10312011463639255\n",
            "\t\t Validation Loss Decreased(0.104337--->0.103120) \t Saving The Model\n",
            "Epoch 97 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002489937044604606 \t\t Validation Loss: 0.11220574751496315\n",
            "Epoch 98 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003533249598223011 \t\t Validation Loss: 0.10186028358741449\n",
            "\t\t Validation Loss Decreased(0.103120--->0.101860) \t Saving The Model\n",
            "Epoch 99 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002455755843264932 \t\t Validation Loss: 0.11332836095243692\n",
            "Epoch 100 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003613270103390253 \t\t Validation Loss: 0.10095861930256853\n",
            "\t\t Validation Loss Decreased(0.101860--->0.100959) \t Saving The Model\n",
            "\n",
            "Score: 9109.114847715737\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "\n",
            "Complex_3_LSTM_92_VADER\n",
            "\n",
            "The model has 2,224,554 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 1.0949839169058848 \t\t Validation Loss: 0.6553330306823437\n",
            "\t\t Validation Loss Decreased(inf--->0.655333) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8219914976507425 \t\t Validation Loss: 1.1434736343530507\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.4859074152553001 \t\t Validation Loss: 1.481247956936176\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.29849139765232197 \t\t Validation Loss: 1.2486442923545837\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.2250837124066075 \t\t Validation Loss: 0.9819538409893329\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.15487232602931358 \t\t Validation Loss: 0.7143000754026266\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08796737309721475 \t\t Validation Loss: 0.5030602617905691\n",
            "\t\t Validation Loss Decreased(0.655333--->0.503060) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03596574087891526 \t\t Validation Loss: 0.3651468719427402\n",
            "\t\t Validation Loss Decreased(0.503060--->0.365147) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.009202743918207046 \t\t Validation Loss: 0.29739951628905076\n",
            "\t\t Validation Loss Decreased(0.365147--->0.297400) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005054135622796477 \t\t Validation Loss: 0.2834750875257529\n",
            "\t\t Validation Loss Decreased(0.297400--->0.283475) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0052950331749319025 \t\t Validation Loss: 0.2741130633422962\n",
            "\t\t Validation Loss Decreased(0.283475--->0.274113) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004365120597523512 \t\t Validation Loss: 0.2596601780790549\n",
            "\t\t Validation Loss Decreased(0.274113--->0.259660) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003949306253973092 \t\t Validation Loss: 0.24610529180902702\n",
            "\t\t Validation Loss Decreased(0.259660--->0.246105) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003853621848582013 \t\t Validation Loss: 0.23551931547430846\n",
            "\t\t Validation Loss Decreased(0.246105--->0.235519) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004008013253238656 \t\t Validation Loss: 0.22610694771775833\n",
            "\t\t Validation Loss Decreased(0.235519--->0.226107) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0036948697414953967 \t\t Validation Loss: 0.21628961984354717\n",
            "\t\t Validation Loss Decreased(0.226107--->0.216290) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0038423126561856653 \t\t Validation Loss: 0.2061674537566992\n",
            "\t\t Validation Loss Decreased(0.216290--->0.206167) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035203554631349303 \t\t Validation Loss: 0.19854810934227246\n",
            "\t\t Validation Loss Decreased(0.206167--->0.198548) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032998508540913463 \t\t Validation Loss: 0.19043138866814283\n",
            "\t\t Validation Loss Decreased(0.198548--->0.190431) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033340364068469687 \t\t Validation Loss: 0.18298056807655555\n",
            "\t\t Validation Loss Decreased(0.190431--->0.182981) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033064324855552733 \t\t Validation Loss: 0.17624629088319266\n",
            "\t\t Validation Loss Decreased(0.182981--->0.176246) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003283379311560074 \t\t Validation Loss: 0.17099801938121134\n",
            "\t\t Validation Loss Decreased(0.176246--->0.170998) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031559428426663617 \t\t Validation Loss: 0.16589255086504495\n",
            "\t\t Validation Loss Decreased(0.170998--->0.165893) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032538139730112978 \t\t Validation Loss: 0.1613503102069864\n",
            "\t\t Validation Loss Decreased(0.165893--->0.161350) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032062718339541272 \t\t Validation Loss: 0.15558701696304175\n",
            "\t\t Validation Loss Decreased(0.161350--->0.155587) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002993295617351258 \t\t Validation Loss: 0.15130231042320913\n",
            "\t\t Validation Loss Decreased(0.155587--->0.151302) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003197346146948434 \t\t Validation Loss: 0.14660175553021523\n",
            "\t\t Validation Loss Decreased(0.151302--->0.146602) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031359698481832608 \t\t Validation Loss: 0.14328470489440057\n",
            "\t\t Validation Loss Decreased(0.146602--->0.143285) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00311976350526124 \t\t Validation Loss: 0.14008691928421074\n",
            "\t\t Validation Loss Decreased(0.143285--->0.140087) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030299231671174435 \t\t Validation Loss: 0.1360698269966703\n",
            "\t\t Validation Loss Decreased(0.140087--->0.136070) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030736391451461494 \t\t Validation Loss: 0.1344919137370128\n",
            "\t\t Validation Loss Decreased(0.136070--->0.134492) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030806723925461237 \t\t Validation Loss: 0.13087332979417765\n",
            "\t\t Validation Loss Decreased(0.134492--->0.130873) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003002636754457411 \t\t Validation Loss: 0.12841734648323977\n",
            "\t\t Validation Loss Decreased(0.130873--->0.128417) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003049404141681923 \t\t Validation Loss: 0.1256883771230395\n",
            "\t\t Validation Loss Decreased(0.128417--->0.125688) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029717104515765568 \t\t Validation Loss: 0.12282239076179954\n",
            "\t\t Validation Loss Decreased(0.125688--->0.122822) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003116870256505806 \t\t Validation Loss: 0.12110882850650412\n",
            "\t\t Validation Loss Decreased(0.122822--->0.121109) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032114127707491454 \t\t Validation Loss: 0.11796121575081578\n",
            "\t\t Validation Loss Decreased(0.121109--->0.117961) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030666599821022435 \t\t Validation Loss: 0.11629308595393713\n",
            "\t\t Validation Loss Decreased(0.117961--->0.116293) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029404886755966457 \t\t Validation Loss: 0.11486463990205756\n",
            "\t\t Validation Loss Decreased(0.116293--->0.114865) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028219116974395474 \t\t Validation Loss: 0.11283848355882443\n",
            "\t\t Validation Loss Decreased(0.114865--->0.112838) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029628560138941816 \t\t Validation Loss: 0.1113814815449027\n",
            "\t\t Validation Loss Decreased(0.112838--->0.111381) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002933438480328265 \t\t Validation Loss: 0.10905580502003431\n",
            "\t\t Validation Loss Decreased(0.111381--->0.109056) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002998200333299669 \t\t Validation Loss: 0.10825727967760311\n",
            "\t\t Validation Loss Decreased(0.109056--->0.108257) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002964004339348223 \t\t Validation Loss: 0.10889068138427459\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002833155603651461 \t\t Validation Loss: 0.1048951215074899\n",
            "\t\t Validation Loss Decreased(0.108257--->0.104895) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031234663053780693 \t\t Validation Loss: 0.10462160920724273\n",
            "\t\t Validation Loss Decreased(0.104895--->0.104622) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002779098868690985 \t\t Validation Loss: 0.10265789750533608\n",
            "\t\t Validation Loss Decreased(0.104622--->0.102658) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028758751530502296 \t\t Validation Loss: 0.10262098908424377\n",
            "\t\t Validation Loss Decreased(0.102658--->0.102621) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002872760038715913 \t\t Validation Loss: 0.10035823009764919\n",
            "\t\t Validation Loss Decreased(0.102621--->0.100358) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027798051517960187 \t\t Validation Loss: 0.09992247490355602\n",
            "\t\t Validation Loss Decreased(0.100358--->0.099922) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030582024624596373 \t\t Validation Loss: 0.0995285079921954\n",
            "\t\t Validation Loss Decreased(0.099922--->0.099529) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003116281207562802 \t\t Validation Loss: 0.09751374016587551\n",
            "\t\t Validation Loss Decreased(0.099529--->0.097514) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002986688552285872 \t\t Validation Loss: 0.09820090784118153\n",
            "Epoch 54 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002883946571838916 \t\t Validation Loss: 0.09656384118044606\n",
            "\t\t Validation Loss Decreased(0.097514--->0.096564) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029904247384291848 \t\t Validation Loss: 0.09508546914618748\n",
            "\t\t Validation Loss Decreased(0.096564--->0.095085) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00279342827097689 \t\t Validation Loss: 0.09653694037562953\n",
            "Epoch 57 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002763812318507179 \t\t Validation Loss: 0.09290467331615779\n",
            "\t\t Validation Loss Decreased(0.095085--->0.092905) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031758698798414016 \t\t Validation Loss: 0.09265594827369429\n",
            "\t\t Validation Loss Decreased(0.092905--->0.092656) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00289867098468381 \t\t Validation Loss: 0.09179897892933625\n",
            "\t\t Validation Loss Decreased(0.092656--->0.091799) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027335527796625487 \t\t Validation Loss: 0.09160676231392874\n",
            "\t\t Validation Loss Decreased(0.091799--->0.091607) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027539979924473003 \t\t Validation Loss: 0.09099509950297383\n",
            "\t\t Validation Loss Decreased(0.091607--->0.090995) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027509824124265564 \t\t Validation Loss: 0.09123148095722382\n",
            "Epoch 63 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002802823901465918 \t\t Validation Loss: 0.08952711234227397\n",
            "\t\t Validation Loss Decreased(0.090995--->0.089527) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028544025671844546 \t\t Validation Loss: 0.09227153882742502\n",
            "Epoch 65 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028246908631125414 \t\t Validation Loss: 0.08724808563979772\n",
            "\t\t Validation Loss Decreased(0.089527--->0.087248) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029499513927746463 \t\t Validation Loss: 0.09113123857129651\n",
            "Epoch 67 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027868533436196377 \t\t Validation Loss: 0.087335673519052\n",
            "Epoch 68 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002951542884614822 \t\t Validation Loss: 0.08839801433854379\n",
            "Epoch 69 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028560094258471115 \t\t Validation Loss: 0.08554278203966813\n",
            "\t\t Validation Loss Decreased(0.087248--->0.085543) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029301332526940954 \t\t Validation Loss: 0.08911889087623702\n",
            "Epoch 71 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028368648899895314 \t\t Validation Loss: 0.0850891841826244\n",
            "\t\t Validation Loss Decreased(0.085543--->0.085089) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003097028651658597 \t\t Validation Loss: 0.08544433687347919\n",
            "Epoch 73 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002857733530470648 \t\t Validation Loss: 0.08437132727927886\n",
            "\t\t Validation Loss Decreased(0.085089--->0.084371) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027349399862106184 \t\t Validation Loss: 0.08545884298375593\n",
            "Epoch 75 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002833914018009563 \t\t Validation Loss: 0.0857582905401404\n",
            "Epoch 76 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029237495666067743 \t\t Validation Loss: 0.08400776929472788\n",
            "\t\t Validation Loss Decreased(0.084371--->0.084008) \t Saving The Model\n",
            "Epoch 77 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003000320045504014 \t\t Validation Loss: 0.08485302866364901\n",
            "Epoch 78 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028339503104861424 \t\t Validation Loss: 0.08191238712662688\n",
            "\t\t Validation Loss Decreased(0.084008--->0.081912) \t Saving The Model\n",
            "Epoch 79 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003010405757703592 \t\t Validation Loss: 0.08483790153136048\n",
            "Epoch 80 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026319105936390523 \t\t Validation Loss: 0.08360984442361559\n",
            "Epoch 81 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031486723298282436 \t\t Validation Loss: 0.08315500214051169\n",
            "Epoch 82 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003034233009423809 \t\t Validation Loss: 0.08308959627846399\n",
            "Epoch 83 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003250994316830828 \t\t Validation Loss: 0.08352556812147108\n",
            "Epoch 84 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002918319040405992 \t\t Validation Loss: 0.0846104640334558\n",
            "Epoch 85 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002788774936610984 \t\t Validation Loss: 0.08259859383822633\n",
            "Epoch 86 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031277742040205143 \t\t Validation Loss: 0.08326189591585156\n",
            "Epoch 87 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003046247302680402 \t\t Validation Loss: 0.08573294717531937\n",
            "Epoch 88 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003048538985485966 \t\t Validation Loss: 0.07965886052876997\n",
            "\t\t Validation Loss Decreased(0.081912--->0.079659) \t Saving The Model\n",
            "Epoch 89 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003506267024833407 \t\t Validation Loss: 0.085370692370746\n",
            "Epoch 90 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030565337133216294 \t\t Validation Loss: 0.08736371682383694\n",
            "Epoch 91 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030588133015191634 \t\t Validation Loss: 0.08311141477539562\n",
            "Epoch 92 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003643529733479325 \t\t Validation Loss: 0.08245550446176472\n",
            "Epoch 93 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003497602430650511 \t\t Validation Loss: 0.0859343943078644\n",
            "Epoch 94 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028710904298350215 \t\t Validation Loss: 0.08354950729363526\n",
            "Epoch 95 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035132088547421467 \t\t Validation Loss: 0.0813778172282932\n",
            "Epoch 96 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030838548932602076 \t\t Validation Loss: 0.08842525722530599\n",
            "Epoch 97 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028568878415909066 \t\t Validation Loss: 0.08526770245785323\n",
            "Epoch 98 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003148529746229886 \t\t Validation Loss: 0.08404721061770733\n",
            "Epoch 99 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029412944316612303 \t\t Validation Loss: 0.08201091428502248\n",
            "Epoch 100 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032875179879468036 \t\t Validation Loss: 0.08552109271001357\n",
            "\n",
            "Score: 7583.88959390863\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "\n",
            "Complex_3_LSTM_36_VADER\n",
            "\n",
            "The model has 606,074 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 1.1133030695790374 \t\t Validation Loss: 0.684642486847364\n",
            "\t\t Validation Loss Decreased(inf--->0.684642) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.8517043687327689 \t\t Validation Loss: 1.3120845647958608\n",
            "Epoch 3 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.5543650410364609 \t\t Validation Loss: 1.8401199670938344\n",
            "Epoch 4 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.38989973219262586 \t\t Validation Loss: 1.697149891119737\n",
            "Epoch 5 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.3281308698681863 \t\t Validation Loss: 1.4970939617890577\n",
            "Epoch 6 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.27189724459124076 \t\t Validation Loss: 1.254546188391172\n",
            "Epoch 7 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.21349558357552095 \t\t Validation Loss: 0.9965587258338928\n",
            "Epoch 8 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.1484718992047616 \t\t Validation Loss: 0.7331047241504376\n",
            "Epoch 9 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.08594503232932373 \t\t Validation Loss: 0.5579795722778027\n",
            "\t\t Validation Loss Decreased(0.684642--->0.557980) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.03579380770050291 \t\t Validation Loss: 0.44808460084291607\n",
            "\t\t Validation Loss Decreased(0.557980--->0.448085) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.01041776904477612 \t\t Validation Loss: 0.3979570922943262\n",
            "\t\t Validation Loss Decreased(0.448085--->0.397957) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005377044017124619 \t\t Validation Loss: 0.38077167994700944\n",
            "\t\t Validation Loss Decreased(0.397957--->0.380772) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.005591322247857681 \t\t Validation Loss: 0.37242177300728285\n",
            "\t\t Validation Loss Decreased(0.380772--->0.372422) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004851610333709097 \t\t Validation Loss: 0.3612115583740748\n",
            "\t\t Validation Loss Decreased(0.372422--->0.361212) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00455030964882541 \t\t Validation Loss: 0.34909986360714984\n",
            "\t\t Validation Loss Decreased(0.361212--->0.349100) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004354710975422751 \t\t Validation Loss: 0.3383009313390805\n",
            "\t\t Validation Loss Decreased(0.349100--->0.338301) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004423145280295127 \t\t Validation Loss: 0.32843390565652114\n",
            "\t\t Validation Loss Decreased(0.338301--->0.328434) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.004050580427566594 \t\t Validation Loss: 0.3173121125079118\n",
            "\t\t Validation Loss Decreased(0.328434--->0.317312) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0038769426551093725 \t\t Validation Loss: 0.30691578353826815\n",
            "\t\t Validation Loss Decreased(0.317312--->0.306916) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0037938510712764756 \t\t Validation Loss: 0.29800732261859453\n",
            "\t\t Validation Loss Decreased(0.306916--->0.298007) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0036325617315801414 \t\t Validation Loss: 0.2886952425424869\n",
            "\t\t Validation Loss Decreased(0.298007--->0.288695) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0036577082764533524 \t\t Validation Loss: 0.2790380042905991\n",
            "\t\t Validation Loss Decreased(0.288695--->0.279038) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0036378618600350376 \t\t Validation Loss: 0.2709447251489529\n",
            "\t\t Validation Loss Decreased(0.279038--->0.270945) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003551828896003851 \t\t Validation Loss: 0.2637774935708596\n",
            "\t\t Validation Loss Decreased(0.270945--->0.263777) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003185396539708096 \t\t Validation Loss: 0.25668406314574754\n",
            "\t\t Validation Loss Decreased(0.263777--->0.256684) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033715224433123964 \t\t Validation Loss: 0.24913435687239355\n",
            "\t\t Validation Loss Decreased(0.256684--->0.249134) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003497964345434731 \t\t Validation Loss: 0.2424732750424972\n",
            "\t\t Validation Loss Decreased(0.249134--->0.242473) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003214429727302411 \t\t Validation Loss: 0.2361742639197753\n",
            "\t\t Validation Loss Decreased(0.242473--->0.236174) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003128465499439453 \t\t Validation Loss: 0.2303280053803554\n",
            "\t\t Validation Loss Decreased(0.236174--->0.230328) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031704490633077315 \t\t Validation Loss: 0.22392659577039573\n",
            "\t\t Validation Loss Decreased(0.230328--->0.223927) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003241120546590537 \t\t Validation Loss: 0.2194218155569755\n",
            "\t\t Validation Loss Decreased(0.223927--->0.219422) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030225722883777642 \t\t Validation Loss: 0.21525003488820332\n",
            "\t\t Validation Loss Decreased(0.219422--->0.215250) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003180142422835972 \t\t Validation Loss: 0.21063014635672936\n",
            "\t\t Validation Loss Decreased(0.215250--->0.210630) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030249957014126957 \t\t Validation Loss: 0.2057483128916759\n",
            "\t\t Validation Loss Decreased(0.210630--->0.205748) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031030796597258667 \t\t Validation Loss: 0.20183173001099092\n",
            "\t\t Validation Loss Decreased(0.205748--->0.201832) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003139762402153448 \t\t Validation Loss: 0.19867730950220272\n",
            "\t\t Validation Loss Decreased(0.201832--->0.198677) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003058275531952244 \t\t Validation Loss: 0.19545187922911003\n",
            "\t\t Validation Loss Decreased(0.198677--->0.195452) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030115916680016025 \t\t Validation Loss: 0.192081745284108\n",
            "\t\t Validation Loss Decreased(0.195452--->0.192082) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002903764140068528 \t\t Validation Loss: 0.1888095337467698\n",
            "\t\t Validation Loss Decreased(0.192082--->0.188810) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002842755039612687 \t\t Validation Loss: 0.1842876568866464\n",
            "\t\t Validation Loss Decreased(0.188810--->0.184288) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029956194433710865 \t\t Validation Loss: 0.1817319763537783\n",
            "\t\t Validation Loss Decreased(0.184288--->0.181732) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002862940536107163 \t\t Validation Loss: 0.1798650764931853\n",
            "\t\t Validation Loss Decreased(0.181732--->0.179865) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030286988208850694 \t\t Validation Loss: 0.17566021753904912\n",
            "\t\t Validation Loss Decreased(0.179865--->0.175660) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002926709718775709 \t\t Validation Loss: 0.17343109724326775\n",
            "\t\t Validation Loss Decreased(0.175660--->0.173431) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002792607052125842 \t\t Validation Loss: 0.1712526919033665\n",
            "\t\t Validation Loss Decreased(0.173431--->0.171253) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0032460445324210704 \t\t Validation Loss: 0.16960053876615488\n",
            "\t\t Validation Loss Decreased(0.171253--->0.169601) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00320513268485917 \t\t Validation Loss: 0.1684310226342999\n",
            "\t\t Validation Loss Decreased(0.169601--->0.168431) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002909855353897689 \t\t Validation Loss: 0.1646245735195967\n",
            "\t\t Validation Loss Decreased(0.168431--->0.164625) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002927939229758104 \t\t Validation Loss: 0.16264334564598706\n",
            "\t\t Validation Loss Decreased(0.164625--->0.162643) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028573867654100664 \t\t Validation Loss: 0.16169157864239353\n",
            "\t\t Validation Loss Decreased(0.162643--->0.161692) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002974446419299253 \t\t Validation Loss: 0.15842322611178344\n",
            "\t\t Validation Loss Decreased(0.161692--->0.158423) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031507708811880767 \t\t Validation Loss: 0.15713555204610413\n",
            "\t\t Validation Loss Decreased(0.158423--->0.157136) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028024113554557836 \t\t Validation Loss: 0.15562786953523755\n",
            "\t\t Validation Loss Decreased(0.157136--->0.155628) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026910125649206944 \t\t Validation Loss: 0.15444970897470528\n",
            "\t\t Validation Loss Decreased(0.155628--->0.154450) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030892653173666346 \t\t Validation Loss: 0.15354768896045592\n",
            "\t\t Validation Loss Decreased(0.154450--->0.153548) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003101855455079695 \t\t Validation Loss: 0.15094757330818817\n",
            "\t\t Validation Loss Decreased(0.153548--->0.150948) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002900804764263936 \t\t Validation Loss: 0.14840389082494837\n",
            "\t\t Validation Loss Decreased(0.150948--->0.148404) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002883209862645615 \t\t Validation Loss: 0.148988573704488\n",
            "Epoch 59 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003030190024071851 \t\t Validation Loss: 0.14746422234636086\n",
            "\t\t Validation Loss Decreased(0.148404--->0.147464) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029497427953960927 \t\t Validation Loss: 0.1451836105507727\n",
            "\t\t Validation Loss Decreased(0.147464--->0.145184) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002967409321541168 \t\t Validation Loss: 0.14541253006945437\n",
            "Epoch 62 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028330507732273356 \t\t Validation Loss: 0.1437800259042818\n",
            "\t\t Validation Loss Decreased(0.145184--->0.143780) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029006806608113285 \t\t Validation Loss: 0.14332196510468537\n",
            "\t\t Validation Loss Decreased(0.143780--->0.143322) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028821938715220706 \t\t Validation Loss: 0.14192738953547981\n",
            "\t\t Validation Loss Decreased(0.143322--->0.141927) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028514862847136887 \t\t Validation Loss: 0.1398791080663124\n",
            "\t\t Validation Loss Decreased(0.141927--->0.139879) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028241875241636423 \t\t Validation Loss: 0.1384064301561851\n",
            "\t\t Validation Loss Decreased(0.139879--->0.138406) \t Saving The Model\n",
            "Epoch 67 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0026799389892698242 \t\t Validation Loss: 0.13906026049517095\n",
            "Epoch 68 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002802078996004688 \t\t Validation Loss: 0.13849818500546882\n",
            "Epoch 69 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030005358150103005 \t\t Validation Loss: 0.1348157655626822\n",
            "\t\t Validation Loss Decreased(0.138406--->0.134816) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029183666605653393 \t\t Validation Loss: 0.13616112644712514\n",
            "Epoch 71 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029301685108048085 \t\t Validation Loss: 0.13459182369451111\n",
            "\t\t Validation Loss Decreased(0.134816--->0.134592) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031068640166415354 \t\t Validation Loss: 0.13394180216038457\n",
            "\t\t Validation Loss Decreased(0.134592--->0.133942) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003284560373317249 \t\t Validation Loss: 0.13233722576226753\n",
            "\t\t Validation Loss Decreased(0.133942--->0.132337) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.00290652658047809 \t\t Validation Loss: 0.13263884326443076\n",
            "Epoch 75 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002751555358298828 \t\t Validation Loss: 0.13434239059615022\n",
            "Epoch 76 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031338487232632534 \t\t Validation Loss: 0.13221825637782997\n",
            "\t\t Validation Loss Decreased(0.132337--->0.132218) \t Saving The Model\n",
            "Epoch 77 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003177559206174133 \t\t Validation Loss: 0.13004451637299588\n",
            "\t\t Validation Loss Decreased(0.132218--->0.130045) \t Saving The Model\n",
            "Epoch 78 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029702013904367246 \t\t Validation Loss: 0.13213077816180885\n",
            "Epoch 79 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029145262150622503 \t\t Validation Loss: 0.1301190858671012\n",
            "Epoch 80 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030332282972144516 \t\t Validation Loss: 0.12821505387098744\n",
            "\t\t Validation Loss Decreased(0.130045--->0.128215) \t Saving The Model\n",
            "Epoch 81 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028242906398215405 \t\t Validation Loss: 0.1282101587141649\n",
            "\t\t Validation Loss Decreased(0.128215--->0.128210) \t Saving The Model\n",
            "Epoch 82 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002862890120406912 \t\t Validation Loss: 0.129089585165135\n",
            "Epoch 83 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029306018290839887 \t\t Validation Loss: 0.12815163107230687\n",
            "\t\t Validation Loss Decreased(0.128210--->0.128152) \t Saving The Model\n",
            "Epoch 84 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002844415933862832 \t\t Validation Loss: 0.1287980371226485\n",
            "Epoch 85 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002961651291509436 \t\t Validation Loss: 0.12805393861176875\n",
            "\t\t Validation Loss Decreased(0.128152--->0.128054) \t Saving The Model\n",
            "Epoch 86 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002915247960167157 \t\t Validation Loss: 0.12766610327749872\n",
            "\t\t Validation Loss Decreased(0.128054--->0.127666) \t Saving The Model\n",
            "Epoch 87 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030394104927639804 \t\t Validation Loss: 0.12421788103305377\n",
            "\t\t Validation Loss Decreased(0.127666--->0.124218) \t Saving The Model\n",
            "Epoch 88 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0031717081547941307 \t\t Validation Loss: 0.12578275631396815\n",
            "Epoch 89 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0028179216047597897 \t\t Validation Loss: 0.12503915201299465\n",
            "Epoch 90 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0027322245689385847 \t\t Validation Loss: 0.12615969739496136\n",
            "Epoch 91 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002982492557084943 \t\t Validation Loss: 0.12580031894433957\n",
            "Epoch 92 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029929591330221375 \t\t Validation Loss: 0.1279890636125436\n",
            "Epoch 93 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003121295476621772 \t\t Validation Loss: 0.1261055670105494\n",
            "Epoch 94 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002934470302362398 \t\t Validation Loss: 0.12601389784294253\n",
            "Epoch 95 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0029329942142892933 \t\t Validation Loss: 0.12490938338809289\n",
            "Epoch 96 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0030445255594000824 \t\t Validation Loss: 0.12554716306308714\n",
            "Epoch 97 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.002845706529852406 \t\t Validation Loss: 0.12425920623354614\n",
            "Epoch 98 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0035236505256663704 \t\t Validation Loss: 0.1269741714394723\n",
            "Epoch 99 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.0033971881753500754 \t\t Validation Loss: 0.12600537061762923\n",
            "Epoch 100 \t\t Epoch time: 0m 0s\n",
            "\t\t Training Loss: 0.003082804216663479 \t\t Validation Loss: 0.12751750705333856\n",
            "\n",
            "Score: 11339.663705583756\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgha95t4xQ3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77f7931-c074-4a9c-d53f-3aa0ccd2b1a7"
      },
      "source": [
        "# train all complex with BERT -> need to rerun the preprocess -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"BERT_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = BERT_news()\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/71_model_BERT.pt'))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt'))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"\\n{model_name}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0]) # todo: need to rerun / continue from 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,481,627 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.5871091664366022 \t\t Validation Loss: 1.4165715345969567\n",
            "\t\t Validation Loss Decreased(inf--->1.416572) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.5281120884685299 \t\t Validation Loss: 1.4837551712989807\n",
            "Epoch 3 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.480121005855098 \t\t Validation Loss: 1.5314376812714796\n",
            "Epoch 4 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.4380995084065944 \t\t Validation Loss: 1.5600238304871779\n",
            "Epoch 5 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.4012207860706022 \t\t Validation Loss: 1.5709874538274913\n",
            "Epoch 6 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.3681046791827759 \t\t Validation Loss: 1.5644356195743268\n",
            "Epoch 7 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.33885162484806935 \t\t Validation Loss: 1.5418700850926912\n",
            "Epoch 8 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.3128637117837128 \t\t Validation Loss: 1.5050052083455598\n",
            "Epoch 9 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.2885794704717055 \t\t Validation Loss: 1.4548678260583143\n",
            "Epoch 10 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.2650660508488481 \t\t Validation Loss: 1.393220067024231\n",
            "\t\t Validation Loss Decreased(1.416572--->1.393220) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.24281594144633492 \t\t Validation Loss: 1.3218814501395593\n",
            "\t\t Validation Loss Decreased(1.393220--->1.321881) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.2218455121525236 \t\t Validation Loss: 1.2425759801497827\n",
            "\t\t Validation Loss Decreased(1.321881--->1.242576) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.20199064642341957 \t\t Validation Loss: 1.1570040537760808\n",
            "\t\t Validation Loss Decreased(1.242576--->1.157004) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.1829614885453437 \t\t Validation Loss: 1.0668556598516612\n",
            "\t\t Validation Loss Decreased(1.157004--->1.066856) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.16473728809128138 \t\t Validation Loss: 0.9740830017970159\n",
            "\t\t Validation Loss Decreased(1.066856--->0.974083) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.14711862991287097 \t\t Validation Loss: 0.8799705230272733\n",
            "\t\t Validation Loss Decreased(0.974083--->0.879971) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.1303568140515504 \t\t Validation Loss: 0.7863436203736526\n",
            "\t\t Validation Loss Decreased(0.879971--->0.786344) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.11487266766473751 \t\t Validation Loss: 0.6945891013512244\n",
            "\t\t Validation Loss Decreased(0.786344--->0.694589) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.10007115150801837 \t\t Validation Loss: 0.6059260139098535\n",
            "\t\t Validation Loss Decreased(0.694589--->0.605926) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0859898570169871 \t\t Validation Loss: 0.5221547048825484\n",
            "\t\t Validation Loss Decreased(0.605926--->0.522155) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.07388499421948516 \t\t Validation Loss: 0.4436674095117129\n",
            "\t\t Validation Loss Decreased(0.522155--->0.443667) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.061895362897524356 \t\t Validation Loss: 0.3713572839131722\n",
            "\t\t Validation Loss Decreased(0.443667--->0.371357) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.05188513214611826 \t\t Validation Loss: 0.3062761311347668\n",
            "\t\t Validation Loss Decreased(0.371357--->0.306276) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.04279282592186654 \t\t Validation Loss: 0.24855361764247602\n",
            "\t\t Validation Loss Decreased(0.306276--->0.248554) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.03477151410629016 \t\t Validation Loss: 0.19855551077769354\n",
            "\t\t Validation Loss Decreased(0.248554--->0.198556) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.027924047298477712 \t\t Validation Loss: 0.15606895547646743\n",
            "\t\t Validation Loss Decreased(0.198556--->0.156069) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.022228672921128973 \t\t Validation Loss: 0.12068747900999509\n",
            "\t\t Validation Loss Decreased(0.156069--->0.120687) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.017636036110186094 \t\t Validation Loss: 0.09182719313181363\n",
            "\t\t Validation Loss Decreased(0.120687--->0.091827) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.01399094209930784 \t\t Validation Loss: 0.06860080475990589\n",
            "\t\t Validation Loss Decreased(0.091827--->0.068601) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.011016038677504135 \t\t Validation Loss: 0.05056761749661886\n",
            "\t\t Validation Loss Decreased(0.068601--->0.050568) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.008836148099067647 \t\t Validation Loss: 0.036828321046554126\n",
            "\t\t Validation Loss Decreased(0.050568--->0.036828) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.006972687051480485 \t\t Validation Loss: 0.026529394233456023\n",
            "\t\t Validation Loss Decreased(0.036828--->0.026529) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.005785036929631354 \t\t Validation Loss: 0.01900869271216484\n",
            "\t\t Validation Loss Decreased(0.026529--->0.019009) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.004759441299766705 \t\t Validation Loss: 0.013600491381321963\n",
            "\t\t Validation Loss Decreased(0.019009--->0.013600) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.004142725119694463 \t\t Validation Loss: 0.00984368953280724\n",
            "\t\t Validation Loss Decreased(0.013600--->0.009844) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.003694241444857137 \t\t Validation Loss: 0.007193231758160086\n",
            "\t\t Validation Loss Decreased(0.009844--->0.007193) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0033807693477216605 \t\t Validation Loss: 0.005357083721229663\n",
            "\t\t Validation Loss Decreased(0.007193--->0.005357) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.003114502806203893 \t\t Validation Loss: 0.004132267499629121\n",
            "\t\t Validation Loss Decreased(0.005357--->0.004132) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002924401925033512 \t\t Validation Loss: 0.0033133599208667874\n",
            "\t\t Validation Loss Decreased(0.004132--->0.003313) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0028581505836415533 \t\t Validation Loss: 0.002770090645823914\n",
            "\t\t Validation Loss Decreased(0.003313--->0.002770) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002710913449551004 \t\t Validation Loss: 0.0023862819160478045\n",
            "\t\t Validation Loss Decreased(0.002770--->0.002386) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002646620854156444 \t\t Validation Loss: 0.0021345630476179603\n",
            "\t\t Validation Loss Decreased(0.002386--->0.002135) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0025971877023678374 \t\t Validation Loss: 0.0019527890477687693\n",
            "\t\t Validation Loss Decreased(0.002135--->0.001953) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0026184651446906296 \t\t Validation Loss: 0.0018134438298427714\n",
            "\t\t Validation Loss Decreased(0.001953--->0.001813) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0025154904327103615 \t\t Validation Loss: 0.001733205661786577\n",
            "\t\t Validation Loss Decreased(0.001813--->0.001733) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0025128773964211547 \t\t Validation Loss: 0.001659640323710986\n",
            "\t\t Validation Loss Decreased(0.001733--->0.001660) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002469578509319674 \t\t Validation Loss: 0.0016055030931826108\n",
            "\t\t Validation Loss Decreased(0.001660--->0.001606) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002460650519178425 \t\t Validation Loss: 0.0015685857461693769\n",
            "\t\t Validation Loss Decreased(0.001606--->0.001569) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002442257889433185 \t\t Validation Loss: 0.001538959043010926\n",
            "\t\t Validation Loss Decreased(0.001569--->0.001539) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0024172881369579685 \t\t Validation Loss: 0.001521866985757907\n",
            "\t\t Validation Loss Decreased(0.001539--->0.001522) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0024090672629205763 \t\t Validation Loss: 0.0015114252589857923\n",
            "\t\t Validation Loss Decreased(0.001522--->0.001511) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.002403132529375521 \t\t Validation Loss: 0.0015048230326591204\n",
            "\t\t Validation Loss Decreased(0.001511--->0.001505) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 98.04077768083756\n",
            "\n",
            "Accuracy: 50.0\n",
            "\n",
            "The model has 2,164,138 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 1.0482263359165676 \t\t Validation Loss: 0.3597847286325235\n",
            "\t\t Validation Loss Decreased(inf--->0.359785) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.9291044482206171 \t\t Validation Loss: 0.43257948183096373\n",
            "Epoch 3 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.8217645540229372 \t\t Validation Loss: 0.5070349310453122\n",
            "Epoch 4 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.7212136054059138 \t\t Validation Loss: 0.5805356468145664\n",
            "Epoch 5 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.6313416144961642 \t\t Validation Loss: 0.65388734294818\n",
            "Epoch 6 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.5477227360714931 \t\t Validation Loss: 0.7232765211508825\n",
            "Epoch 7 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.4739821672351477 \t\t Validation Loss: 0.7884207367897034\n",
            "Epoch 8 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.4093797479982714 \t\t Validation Loss: 0.847419009758876\n",
            "Epoch 9 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.35158333118149154 \t\t Validation Loss: 0.8942886797281412\n",
            "Epoch 10 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.3040497420053627 \t\t Validation Loss: 0.9294490860058711\n",
            "Epoch 11 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.26493141664871694 \t\t Validation Loss: 0.9512122731942397\n",
            "Epoch 12 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.23155238499512543 \t\t Validation Loss: 0.9567523782069867\n",
            "Epoch 13 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.20502192751076576 \t\t Validation Loss: 0.9473925003638635\n",
            "Epoch 14 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.18176477888246645 \t\t Validation Loss: 0.9233329066863427\n",
            "Epoch 15 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.1657461595243296 \t\t Validation Loss: 0.8910790681838989\n",
            "Epoch 16 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.14998781723851287 \t\t Validation Loss: 0.8494436144828796\n",
            "Epoch 17 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.1381788864429738 \t\t Validation Loss: 0.8028265283657954\n",
            "Epoch 18 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.1242816433864268 \t\t Validation Loss: 0.7492560469187223\n",
            "Epoch 19 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.11388562440972876 \t\t Validation Loss: 0.6945573137356684\n",
            "Epoch 20 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.10433209854857745 \t\t Validation Loss: 0.6382871659902426\n",
            "Epoch 21 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.09445229618231187 \t\t Validation Loss: 0.5832361808189979\n",
            "Epoch 22 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0873023792849602 \t\t Validation Loss: 0.529246442593061\n",
            "Epoch 23 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.07815190417238989 \t\t Validation Loss: 0.47768405309090245\n",
            "Epoch 24 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.07226476131158101 \t\t Validation Loss: 0.4286000728607178\n",
            "Epoch 25 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.06570425307428515 \t\t Validation Loss: 0.3839689653653365\n",
            "Epoch 26 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.060227474373941485 \t\t Validation Loss: 0.34295680775092197\n",
            "\t\t Validation Loss Decreased(0.359785--->0.342957) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.055358111128412386 \t\t Validation Loss: 0.3047444006571403\n",
            "\t\t Validation Loss Decreased(0.342957--->0.304744) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.05070871152487155 \t\t Validation Loss: 0.2703216534394484\n",
            "\t\t Validation Loss Decreased(0.304744--->0.270322) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.04779861922803763 \t\t Validation Loss: 0.23873096245985764\n",
            "\t\t Validation Loss Decreased(0.270322--->0.238731) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.04500405100249761 \t\t Validation Loss: 0.21149638753670913\n",
            "\t\t Validation Loss Decreased(0.238731--->0.211496) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.04091609979199397 \t\t Validation Loss: 0.18618416900818163\n",
            "\t\t Validation Loss Decreased(0.211496--->0.186184) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.038518629464748745 \t\t Validation Loss: 0.1646744253543707\n",
            "\t\t Validation Loss Decreased(0.186184--->0.164674) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.03376525703414872 \t\t Validation Loss: 0.14454392802256805\n",
            "\t\t Validation Loss Decreased(0.164674--->0.144544) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.03318413822735484 \t\t Validation Loss: 0.1277528451039241\n",
            "\t\t Validation Loss Decreased(0.144544--->0.127753) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.02910171137065501 \t\t Validation Loss: 0.11358533398463176\n",
            "\t\t Validation Loss Decreased(0.127753--->0.113585) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.028131313244434627 \t\t Validation Loss: 0.10037559385483082\n",
            "\t\t Validation Loss Decreased(0.113585--->0.100376) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.025947501731885446 \t\t Validation Loss: 0.08910567026871902\n",
            "\t\t Validation Loss Decreased(0.100376--->0.089106) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.025603893917758722 \t\t Validation Loss: 0.07922013218586262\n",
            "\t\t Validation Loss Decreased(0.089106--->0.079220) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.022997488318061508 \t\t Validation Loss: 0.07006318523333623\n",
            "\t\t Validation Loss Decreased(0.079220--->0.070063) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.021586760652025003 \t\t Validation Loss: 0.06197830719443468\n",
            "\t\t Validation Loss Decreased(0.070063--->0.061978) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.02022302754827448 \t\t Validation Loss: 0.05477036593052057\n",
            "\t\t Validation Loss Decreased(0.061978--->0.054770) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.018415536278405704 \t\t Validation Loss: 0.04865572572900699\n",
            "\t\t Validation Loss Decreased(0.054770--->0.048656) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.01862863078713417 \t\t Validation Loss: 0.04343487895452059\n",
            "\t\t Validation Loss Decreased(0.048656--->0.043435) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.015661685123435548 \t\t Validation Loss: 0.03866852991856062\n",
            "\t\t Validation Loss Decreased(0.043435--->0.038669) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.014994660593770645 \t\t Validation Loss: 0.03438467732988871\n",
            "\t\t Validation Loss Decreased(0.038669--->0.034385) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.014707260639280887 \t\t Validation Loss: 0.030108024581120565\n",
            "\t\t Validation Loss Decreased(0.034385--->0.030108) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.012987914608439078 \t\t Validation Loss: 0.026482771938809983\n",
            "\t\t Validation Loss Decreased(0.030108--->0.026483) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.01181633783051291 \t\t Validation Loss: 0.023659993464557025\n",
            "\t\t Validation Loss Decreased(0.026483--->0.023660) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0109944143961813 \t\t Validation Loss: 0.021305844331016906\n",
            "\t\t Validation Loss Decreased(0.023660--->0.021306) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.010256612774085355 \t\t Validation Loss: 0.018841717248925798\n",
            "\t\t Validation Loss Decreased(0.021306--->0.018842) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.010018001323112764 \t\t Validation Loss: 0.016648452299145553\n",
            "\t\t Validation Loss Decreased(0.018842--->0.016648) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.00910560879856348 \t\t Validation Loss: 0.014717016798945574\n",
            "\t\t Validation Loss Decreased(0.016648--->0.014717) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.008313718805643352 \t\t Validation Loss: 0.013096284121274948\n",
            "\t\t Validation Loss Decreased(0.014717--->0.013096) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.0076841213614554016 \t\t Validation Loss: 0.011494840137087382\n",
            "\t\t Validation Loss Decreased(0.013096--->0.011495) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 1m 48s\n",
            "\t\t Training Loss: 0.006998695093332915 \t\t Validation Loss: 0.01004492101044609\n",
            "\t\t Validation Loss Decreased(0.011495--->0.010045) \t Saving The Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C08j1zt__g3S",
        "outputId": "30ffd197-4c1d-4831-f9b5-b65b4397ebc3"
      },
      "source": [
        "# train all complex with BERT -> need to rerun the preprocess -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"BERT_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = BERT_news()\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/71_model_BERT.pt'))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt'))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(1, len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"{model_name[0]}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex_1_LSTM_92_BERT_news\n",
            "\n",
            "The model has 111,729,739 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.9408257897515353 \t\t Validation Loss: 0.8650538394084344\n",
            "\t\t Validation Loss Decreased(inf--->0.865054) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.809583116026998 \t\t Validation Loss: 0.9217331959651067\n",
            "Epoch 3 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.6935437537629057 \t\t Validation Loss: 0.9746408393749824\n",
            "Epoch 4 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.5884939457944317 \t\t Validation Loss: 1.0204669695634108\n",
            "Epoch 5 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.4958254685751288 \t\t Validation Loss: 1.0570586507137005\n",
            "Epoch 6 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.4157972204045871 \t\t Validation Loss: 1.0829536135380085\n",
            "Epoch 7 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.3501531849615276 \t\t Validation Loss: 1.0928952143742487\n",
            "Epoch 8 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.2963723861486525 \t\t Validation Loss: 1.0882888344617991\n",
            "Epoch 9 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.252075610531343 \t\t Validation Loss: 1.0686182838219862\n",
            "Epoch 10 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.21634157865333395 \t\t Validation Loss: 1.034888322536762\n",
            "Epoch 11 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.18927394986354015 \t\t Validation Loss: 0.9892861338762137\n",
            "Epoch 12 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1678120190831455 \t\t Validation Loss: 0.9341947436332703\n",
            "Epoch 13 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.15061301252225767 \t\t Validation Loss: 0.8729055111224835\n",
            "Epoch 14 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.13399312996330695 \t\t Validation Loss: 0.8084334089205816\n",
            "\t\t Validation Loss Decreased(0.865054--->0.808433) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.12048535658096945 \t\t Validation Loss: 0.7423246136078467\n",
            "\t\t Validation Loss Decreased(0.808433--->0.742325) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.10827113110949663 \t\t Validation Loss: 0.6761691478582529\n",
            "\t\t Validation Loss Decreased(0.742325--->0.676169) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0978998913731728 \t\t Validation Loss: 0.610836196404237\n",
            "\t\t Validation Loss Decreased(0.676169--->0.610836) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.08689984201604652 \t\t Validation Loss: 0.5476185908684363\n",
            "\t\t Validation Loss Decreased(0.610836--->0.547619) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0775415673600258 \t\t Validation Loss: 0.4879432481068831\n",
            "\t\t Validation Loss Decreased(0.547619--->0.487943) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.06862123524518432 \t\t Validation Loss: 0.43149563670158386\n",
            "\t\t Validation Loss Decreased(0.487943--->0.431496) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.06094144001552785 \t\t Validation Loss: 0.3786517289968637\n",
            "\t\t Validation Loss Decreased(0.431496--->0.378652) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.05334581614383874 \t\t Validation Loss: 0.3292601154400752\n",
            "\t\t Validation Loss Decreased(0.378652--->0.329260) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.046940832184879364 \t\t Validation Loss: 0.2840365033883315\n",
            "\t\t Validation Loss Decreased(0.329260--->0.284037) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.04128878545393613 \t\t Validation Loss: 0.24275121780542228\n",
            "\t\t Validation Loss Decreased(0.284037--->0.242751) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.03543693803854891 \t\t Validation Loss: 0.2051311731338501\n",
            "\t\t Validation Loss Decreased(0.242751--->0.205131) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.029956011032383587 \t\t Validation Loss: 0.1717011137650563\n",
            "\t\t Validation Loss Decreased(0.205131--->0.171701) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.025592141943618755 \t\t Validation Loss: 0.14229086901132876\n",
            "\t\t Validation Loss Decreased(0.171701--->0.142291) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.02175139026194056 \t\t Validation Loss: 0.1167632146523549\n",
            "\t\t Validation Loss Decreased(0.142291--->0.116763) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.01854881359503378 \t\t Validation Loss: 0.09477547900034831\n",
            "\t\t Validation Loss Decreased(0.116763--->0.094775) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.015815936648790295 \t\t Validation Loss: 0.07612817333294795\n",
            "\t\t Validation Loss Decreased(0.094775--->0.076128) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.013218460999416641 \t\t Validation Loss: 0.060423276745356046\n",
            "\t\t Validation Loss Decreased(0.076128--->0.060423) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.011163805501308997 \t\t Validation Loss: 0.04755356965156702\n",
            "\t\t Validation Loss Decreased(0.060423--->0.047554) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.009461736215303678 \t\t Validation Loss: 0.036902107584934965\n",
            "\t\t Validation Loss Decreased(0.047554--->0.036902) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.008238588500692434 \t\t Validation Loss: 0.02834916143463208\n",
            "\t\t Validation Loss Decreased(0.036902--->0.028349) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0070584452855778305 \t\t Validation Loss: 0.021670414278140433\n",
            "\t\t Validation Loss Decreased(0.028349--->0.021670) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.006151676391930999 \t\t Validation Loss: 0.016507619060575962\n",
            "\t\t Validation Loss Decreased(0.021670--->0.016508) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.005521286839015178 \t\t Validation Loss: 0.012479662608641844\n",
            "\t\t Validation Loss Decreased(0.016508--->0.012480) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.004808849727458044 \t\t Validation Loss: 0.009446249737475928\n",
            "\t\t Validation Loss Decreased(0.012480--->0.009446) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.004282072620667718 \t\t Validation Loss: 0.007210080010386614\n",
            "\t\t Validation Loss Decreased(0.009446--->0.007210) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.004223271244759294 \t\t Validation Loss: 0.005609302960622769\n",
            "\t\t Validation Loss Decreased(0.007210--->0.005609) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0038501910261205725 \t\t Validation Loss: 0.004455582704395056\n",
            "\t\t Validation Loss Decreased(0.005609--->0.004456) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0036289833416860246 \t\t Validation Loss: 0.0036645826811973867\n",
            "\t\t Validation Loss Decreased(0.004456--->0.003665) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0034943279036245235 \t\t Validation Loss: 0.0031077709819118562\n",
            "\t\t Validation Loss Decreased(0.003665--->0.003108) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0032610415883765026 \t\t Validation Loss: 0.0027357335023295423\n",
            "\t\t Validation Loss Decreased(0.003108--->0.002736) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 131.22935834390864\n",
            "\n",
            "Accuracy: 51.26903553299492\n",
            "\n",
            "Complex_1_LSTM_36_BERT_news\n",
            "\n",
            "The model has 545,658 trainable parameters\n",
            "The model has 857 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.4719796703901847 \t\t Validation Loss: 0.8924598418749295\n",
            "\t\t Validation Loss Decreased(inf--->0.892460) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.4049651659521702 \t\t Validation Loss: 0.9054265985122094\n",
            "Epoch 3 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.34770001849864385 \t\t Validation Loss: 0.9122044467009031\n",
            "Epoch 4 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.29449763637338133 \t\t Validation Loss: 0.911687786762531\n",
            "Epoch 5 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.25085361739872275 \t\t Validation Loss: 0.9034690077488239\n",
            "Epoch 6 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.21638567330366051 \t\t Validation Loss: 0.8870424674107478\n",
            "\t\t Validation Loss Decreased(0.892460--->0.887042) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1871228965024489 \t\t Validation Loss: 0.8623288090412433\n",
            "\t\t Validation Loss Decreased(0.887042--->0.862329) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.16492362758396445 \t\t Validation Loss: 0.8305818002957565\n",
            "\t\t Validation Loss Decreased(0.862329--->0.830582) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.14649621145548047 \t\t Validation Loss: 0.7923913850234106\n",
            "\t\t Validation Loss Decreased(0.830582--->0.792391) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.13154886046273484 \t\t Validation Loss: 0.7491189172634711\n",
            "\t\t Validation Loss Decreased(0.792391--->0.749119) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.12096490524709225 \t\t Validation Loss: 0.7017626922864181\n",
            "\t\t Validation Loss Decreased(0.749119--->0.701763) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.10910817156419964 \t\t Validation Loss: 0.6525023373273703\n",
            "\t\t Validation Loss Decreased(0.701763--->0.652502) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.09998909333670461 \t\t Validation Loss: 0.6024280121693244\n",
            "\t\t Validation Loss Decreased(0.652502--->0.602428) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.08998156074635885 \t\t Validation Loss: 0.5525699876821958\n",
            "\t\t Validation Loss Decreased(0.602428--->0.552570) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 47s\n",
            "\t\t Training Loss: 0.08433828585957354 \t\t Validation Loss: 0.5031110736039969\n",
            "\t\t Validation Loss Decreased(0.552570--->0.503111) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.07680615060333465 \t\t Validation Loss: 0.4546841107881986\n",
            "\t\t Validation Loss Decreased(0.503111--->0.454684) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.06992712468412277 \t\t Validation Loss: 0.4081437518963447\n",
            "\t\t Validation Loss Decreased(0.454684--->0.408144) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.06471145497886716 \t\t Validation Loss: 0.36338451504707336\n",
            "\t\t Validation Loss Decreased(0.408144--->0.363385) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.059295293824696865 \t\t Validation Loss: 0.32022142983399904\n",
            "\t\t Validation Loss Decreased(0.363385--->0.320221) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.05184923839830869 \t\t Validation Loss: 0.2797842174768448\n",
            "\t\t Validation Loss Decreased(0.320221--->0.279784) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.045487287982895565 \t\t Validation Loss: 0.24248031698740447\n",
            "\t\t Validation Loss Decreased(0.279784--->0.242480) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.04085218732723513 \t\t Validation Loss: 0.20815109060360834\n",
            "\t\t Validation Loss Decreased(0.242480--->0.208151) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.03677974331721261 \t\t Validation Loss: 0.1770084133514991\n",
            "\t\t Validation Loss Decreased(0.208151--->0.177008) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.03311296595210159 \t\t Validation Loss: 0.1487239094880911\n",
            "\t\t Validation Loss Decreased(0.177008--->0.148724) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.029129249471667652 \t\t Validation Loss: 0.12333771471793835\n",
            "\t\t Validation Loss Decreased(0.148724--->0.123338) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.025334756745881325 \t\t Validation Loss: 0.10143831372261047\n",
            "\t\t Validation Loss Decreased(0.123338--->0.101438) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.022208294478824008 \t\t Validation Loss: 0.08258983836724208\n",
            "\t\t Validation Loss Decreased(0.101438--->0.082590) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.019698064157588256 \t\t Validation Loss: 0.06596465953267537\n",
            "\t\t Validation Loss Decreased(0.082590--->0.065965) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.017018204844380554 \t\t Validation Loss: 0.05244244219591984\n",
            "\t\t Validation Loss Decreased(0.065965--->0.052442) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.014628482177048116 \t\t Validation Loss: 0.04120738216890739\n",
            "\t\t Validation Loss Decreased(0.052442--->0.041207) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.013218598402533177 \t\t Validation Loss: 0.03240599332807156\n",
            "\t\t Validation Loss Decreased(0.041207--->0.032406) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.012043863308389444 \t\t Validation Loss: 0.025146314587730628\n",
            "\t\t Validation Loss Decreased(0.032406--->0.025146) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.010540680557086662 \t\t Validation Loss: 0.019564058273457564\n",
            "\t\t Validation Loss Decreased(0.025146--->0.019564) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.009742161850570826 \t\t Validation Loss: 0.015304004916777978\n",
            "\t\t Validation Loss Decreased(0.019564--->0.015304) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.008967203226544568 \t\t Validation Loss: 0.012028992247696105\n",
            "\t\t Validation Loss Decreased(0.015304--->0.012029) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.00807659953480234 \t\t Validation Loss: 0.00959860200348955\n",
            "\t\t Validation Loss Decreased(0.012029--->0.009599) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.007511940570799886 \t\t Validation Loss: 0.00786455892599546\n",
            "\t\t Validation Loss Decreased(0.009599--->0.007865) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.00711510908110319 \t\t Validation Loss: 0.006586583223767006\n",
            "\t\t Validation Loss Decreased(0.007865--->0.006587) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.006321760630738494 \t\t Validation Loss: 0.0057150016252238015\n",
            "\t\t Validation Loss Decreased(0.006587--->0.005715) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0063383997027837745 \t\t Validation Loss: 0.0050845795239393525\n",
            "\t\t Validation Loss Decreased(0.005715--->0.005085) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.005960409114187634 \t\t Validation Loss: 0.004612774480707371\n",
            "\t\t Validation Loss Decreased(0.005085--->0.004613) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 321.33552109771574\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "Complex_2_Linear_108_BERT_news\n",
            "\n",
            "The model has 1,401,290 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.5095280183831582 \t\t Validation Loss: 1.3566376658586354\n",
            "\t\t Validation Loss Decreased(inf--->1.356638) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.44066465747970585 \t\t Validation Loss: 1.355705426289485\n",
            "\t\t Validation Loss Decreased(1.356638--->1.355705) \t Saving The Model\n",
            "Epoch 3 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.38172435213031397 \t\t Validation Loss: 1.3211540717345018\n",
            "\t\t Validation Loss Decreased(1.355705--->1.321154) \t Saving The Model\n",
            "Epoch 4 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.32190006396204635 \t\t Validation Loss: 1.243695094035222\n",
            "\t\t Validation Loss Decreased(1.321154--->1.243695) \t Saving The Model\n",
            "Epoch 5 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.2615450698429266 \t\t Validation Loss: 1.1142283586355357\n",
            "\t\t Validation Loss Decreased(1.243695--->1.114228) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.20374582431002244 \t\t Validation Loss: 0.936581762937399\n",
            "\t\t Validation Loss Decreased(1.114228--->0.936582) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.15048807668122086 \t\t Validation Loss: 0.7262068459620843\n",
            "\t\t Validation Loss Decreased(0.936582--->0.726207) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1062394546193851 \t\t Validation Loss: 0.5147666724828573\n",
            "\t\t Validation Loss Decreased(0.726207--->0.514767) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.06881781979589849 \t\t Validation Loss: 0.3324836091353343\n",
            "\t\t Validation Loss Decreased(0.514767--->0.332484) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.04717801168963716 \t\t Validation Loss: 0.2005520577614124\n",
            "\t\t Validation Loss Decreased(0.332484--->0.200552) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.03476002983547546 \t\t Validation Loss: 0.11825930441801365\n",
            "\t\t Validation Loss Decreased(0.200552--->0.118259) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.026655721654360358 \t\t Validation Loss: 0.07207932208593075\n",
            "\t\t Validation Loss Decreased(0.118259--->0.072079) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.021258919676010672 \t\t Validation Loss: 0.04819694319023536\n",
            "\t\t Validation Loss Decreased(0.072079--->0.048197) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.017194057915460418 \t\t Validation Loss: 0.03432368200558882\n",
            "\t\t Validation Loss Decreased(0.048197--->0.034324) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.014559500775224454 \t\t Validation Loss: 0.026232511306611393\n",
            "\t\t Validation Loss Decreased(0.034324--->0.026233) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.01162415398033084 \t\t Validation Loss: 0.020496565060546763\n",
            "\t\t Validation Loss Decreased(0.026233--->0.020497) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.01036885565750905 \t\t Validation Loss: 0.01599277577434595\n",
            "\t\t Validation Loss Decreased(0.020497--->0.015993) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.00832590922662938 \t\t Validation Loss: 0.012655094946519686\n",
            "\t\t Validation Loss Decreased(0.015993--->0.012655) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.007184328977018595 \t\t Validation Loss: 0.010203902502186023\n",
            "\t\t Validation Loss Decreased(0.012655--->0.010204) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.005771087281204559 \t\t Validation Loss: 0.007867617353510398\n",
            "\t\t Validation Loss Decreased(0.010204--->0.007868) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.004790261538850295 \t\t Validation Loss: 0.006370456423610449\n",
            "\t\t Validation Loss Decreased(0.007868--->0.006370) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.003989096708922974 \t\t Validation Loss: 0.004993666249972124\n",
            "\t\t Validation Loss Decreased(0.006370--->0.004994) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.003644924211265469 \t\t Validation Loss: 0.004012796544254973\n",
            "\t\t Validation Loss Decreased(0.004994--->0.004013) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0032338267506880534 \t\t Validation Loss: 0.003274211731667702\n",
            "\t\t Validation Loss Decreased(0.004013--->0.003274) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0029508311292030724 \t\t Validation Loss: 0.0028420478159275194\n",
            "\t\t Validation Loss Decreased(0.003274--->0.002842) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.002734738337568234 \t\t Validation Loss: 0.0024064792714153346\n",
            "\t\t Validation Loss Decreased(0.002842--->0.002406) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.00254582248256869 \t\t Validation Loss: 0.002102318685501814\n",
            "\t\t Validation Loss Decreased(0.002406--->0.002102) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 121.3105567893401\n",
            "\n",
            "Accuracy: 49.23857868020304\n",
            "\n",
            "Complex_2_LSTM_92_BERT_news\n",
            "\n",
            "The model has 2,167,162 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.6377979062396932 \t\t Validation Loss: 1.0226459526098692\n",
            "\t\t Validation Loss Decreased(inf--->1.022646) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.49684905966215237 \t\t Validation Loss: 1.1713157342030451\n",
            "Epoch 3 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.3863229001998096 \t\t Validation Loss: 1.2988426914581885\n",
            "Epoch 4 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.3070313092069449 \t\t Validation Loss: 1.371185160600222\n",
            "Epoch 5 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.2565455462758404 \t\t Validation Loss: 1.3744901647934546\n",
            "Epoch 6 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.22340704685681173 \t\t Validation Loss: 1.3238412921245282\n",
            "Epoch 7 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.20379186524833376 \t\t Validation Loss: 1.2452786152179425\n",
            "Epoch 8 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1861336659765928 \t\t Validation Loss: 1.1521374170596783\n",
            "Epoch 9 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1717989465750351 \t\t Validation Loss: 1.055964667063493\n",
            "Epoch 10 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.15457873913534992 \t\t Validation Loss: 0.9561890271993784\n",
            "\t\t Validation Loss Decreased(1.022646--->0.956189) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.14316003460034327 \t\t Validation Loss: 0.8587206510397104\n",
            "\t\t Validation Loss Decreased(0.956189--->0.858721) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.12892778230377952 \t\t Validation Loss: 0.7607664649303143\n",
            "\t\t Validation Loss Decreased(0.858721--->0.760766) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.1144869367315157 \t\t Validation Loss: 0.6637639449192927\n",
            "\t\t Validation Loss Decreased(0.760766--->0.663764) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.10085813523698095 \t\t Validation Loss: 0.5692095917004806\n",
            "\t\t Validation Loss Decreased(0.663764--->0.569210) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.08952032171844228 \t\t Validation Loss: 0.47967585692038905\n",
            "\t\t Validation Loss Decreased(0.569210--->0.479676) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.07831834996672901 \t\t Validation Loss: 0.3948242377776366\n",
            "\t\t Validation Loss Decreased(0.479676--->0.394824) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.0643639883177506 \t\t Validation Loss: 0.31655994286903966\n",
            "\t\t Validation Loss Decreased(0.394824--->0.316560) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.05516099907155778 \t\t Validation Loss: 0.24570890573354867\n",
            "\t\t Validation Loss Decreased(0.316560--->0.245709) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.045117870854163494 \t\t Validation Loss: 0.18463059342824495\n",
            "\t\t Validation Loss Decreased(0.245709--->0.184631) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.03709663438132486 \t\t Validation Loss: 0.1341852729137127\n",
            "\t\t Validation Loss Decreased(0.184631--->0.134185) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.030239389907266642 \t\t Validation Loss: 0.09394951250690681\n",
            "\t\t Validation Loss Decreased(0.134185--->0.093950) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.02465684515600269 \t\t Validation Loss: 0.06359938704050504\n",
            "\t\t Validation Loss Decreased(0.093950--->0.063599) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 46s\n",
            "\t\t Training Loss: 0.021209555451531668 \t\t Validation Loss: 0.04201875182871635\n",
            "\t\t Validation Loss Decreased(0.063599--->0.042019) \t Saving The Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQbNjJM-pQXw",
        "outputId": "2a42698d-ca10-4c1e-ffb3-fa3c95856af7"
      },
      "source": [
        "# train all complex with BERT -> need to rerun the preprocess -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"BERT_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = BERT_news()\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/71_model_BERT.pt'))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt'))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(4, len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"{model_name[0]}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex_2_LSTM_92_BERT_news\n",
            "\n",
            "The model has 111,732,763 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 44s\n",
            "\t\t Training Loss: 0.6379210791122671 \t\t Validation Loss: 1.0227225973055913\n",
            "\t\t Validation Loss Decreased(inf--->1.022723) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.4970067121038163 \t\t Validation Loss: 1.1710784343572764\n",
            "Epoch 3 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.3861650169537579 \t\t Validation Loss: 1.2985263191736662\n",
            "Epoch 4 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.3071261369119826 \t\t Validation Loss: 1.370970904827118\n",
            "Epoch 5 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.25660994124120556 \t\t Validation Loss: 1.3741920773799603\n",
            "Epoch 6 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.22530225961393602 \t\t Validation Loss: 1.3236134006426885\n",
            "Epoch 7 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.20303016635463447 \t\t Validation Loss: 1.245961537727943\n",
            "Epoch 8 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.1872267334822666 \t\t Validation Loss: 1.154379734626183\n",
            "Epoch 9 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.17078538733610027 \t\t Validation Loss: 1.0574938288101783\n",
            "Epoch 10 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.1553061654967432 \t\t Validation Loss: 0.9578882134877719\n",
            "\t\t Validation Loss Decreased(1.022723--->0.957888) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.1416295663126417 \t\t Validation Loss: 0.8585787369654729\n",
            "\t\t Validation Loss Decreased(0.957888--->0.858579) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.12890898066295967 \t\t Validation Loss: 0.7595924414121188\n",
            "\t\t Validation Loss Decreased(0.858579--->0.759592) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.11631144081066186 \t\t Validation Loss: 0.6648262555782611\n",
            "\t\t Validation Loss Decreased(0.759592--->0.664826) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.1014936480520142 \t\t Validation Loss: 0.5711479439185216\n",
            "\t\t Validation Loss Decreased(0.664826--->0.571148) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.08900720379441171 \t\t Validation Loss: 0.48038168595387387\n",
            "\t\t Validation Loss Decreased(0.571148--->0.480382) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.07659061441852434 \t\t Validation Loss: 0.3954263742153461\n",
            "\t\t Validation Loss Decreased(0.480382--->0.395426) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.06626819081705164 \t\t Validation Loss: 0.31670520168084365\n",
            "\t\t Validation Loss Decreased(0.395426--->0.316705) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.05476655722973314 \t\t Validation Loss: 0.24617980191340813\n",
            "\t\t Validation Loss Decreased(0.316705--->0.246180) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.04528899825605992 \t\t Validation Loss: 0.1854972971173433\n",
            "\t\t Validation Loss Decreased(0.246180--->0.185497) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.03643510783896656 \t\t Validation Loss: 0.13460933302457517\n",
            "\t\t Validation Loss Decreased(0.185497--->0.134609) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.03008357293249385 \t\t Validation Loss: 0.09428077936172485\n",
            "\t\t Validation Loss Decreased(0.134609--->0.094281) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.02452295136361106 \t\t Validation Loss: 0.06383285929377262\n",
            "\t\t Validation Loss Decreased(0.094281--->0.063833) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.020718180700331122 \t\t Validation Loss: 0.04229159037081095\n",
            "\t\t Validation Loss Decreased(0.063833--->0.042292) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.017673525531348343 \t\t Validation Loss: 0.02762794752533619\n",
            "\t\t Validation Loss Decreased(0.042292--->0.027628) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.015344070635635304 \t\t Validation Loss: 0.018039086260474645\n",
            "\t\t Validation Loss Decreased(0.027628--->0.018039) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.013123932256791237 \t\t Validation Loss: 0.01259074818629485\n",
            "\t\t Validation Loss Decreased(0.018039--->0.012591) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.011830505386397645 \t\t Validation Loss: 0.009666736488445448\n",
            "\t\t Validation Loss Decreased(0.012591--->0.009667) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.010314392806911791 \t\t Validation Loss: 0.008170754362184268\n",
            "\t\t Validation Loss Decreased(0.009667--->0.008171) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.009358399497294749 \t\t Validation Loss: 0.007345968606666877\n",
            "\t\t Validation Loss Decreased(0.008171--->0.007346) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.008822219282690738 \t\t Validation Loss: 0.006841825106396125\n",
            "\t\t Validation Loss Decreased(0.007346--->0.006842) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.00798669142799603 \t\t Validation Loss: 0.006454021096802675\n",
            "\t\t Validation Loss Decreased(0.006842--->0.006454) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 237.1485168147208\n",
            "\n",
            "Accuracy: 47.20812182741117\n",
            "\n",
            "Complex_2_LSTM_36_BERT_news\n",
            "\n",
            "The model has 548,682 trainable parameters\n",
            "The model has 3,881 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.6997866310821091 \t\t Validation Loss: 1.0560245697314923\n",
            "\t\t Validation Loss Decreased(inf--->1.056025) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.5739148369537214 \t\t Validation Loss: 1.232736770923321\n",
            "Epoch 3 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.47578049921808213 \t\t Validation Loss: 1.3988153155033405\n",
            "Epoch 4 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.3965650986175279 \t\t Validation Loss: 1.5267507708989656\n",
            "Epoch 5 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.3373928905238171 \t\t Validation Loss: 1.5884536596444936\n",
            "Epoch 6 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.2966768147826597 \t\t Validation Loss: 1.5762104575450604\n",
            "Epoch 7 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.2651927287791025 \t\t Validation Loss: 1.511430547787593\n",
            "Epoch 8 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.23932588060789212 \t\t Validation Loss: 1.4113793418957636\n",
            "Epoch 9 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.2146248845698161 \t\t Validation Loss: 1.2893946308356066\n",
            "Epoch 10 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.19144861893476667 \t\t Validation Loss: 1.1552901726502638\n",
            "Epoch 11 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.16792094588241968 \t\t Validation Loss: 1.0091671485167284\n",
            "\t\t Validation Loss Decreased(1.056025--->1.009167) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.14515303400647198 \t\t Validation Loss: 0.8578457098740798\n",
            "\t\t Validation Loss Decreased(1.009167--->0.857846) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.12107185336975129 \t\t Validation Loss: 0.7038572682784154\n",
            "\t\t Validation Loss Decreased(0.857846--->0.703857) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0982390595600009 \t\t Validation Loss: 0.5551999050837296\n",
            "\t\t Validation Loss Decreased(0.703857--->0.555200) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.07698028863130792 \t\t Validation Loss: 0.4180958316876338\n",
            "\t\t Validation Loss Decreased(0.555200--->0.418096) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.058947136638232984 \t\t Validation Loss: 0.29875416595202225\n",
            "\t\t Validation Loss Decreased(0.418096--->0.298754) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.04299915057128748 \t\t Validation Loss: 0.20495940286379594\n",
            "\t\t Validation Loss Decreased(0.298754--->0.204959) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.03299510287674698 \t\t Validation Loss: 0.1376494622001281\n",
            "\t\t Validation Loss Decreased(0.204959--->0.137649) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.023730724269675242 \t\t Validation Loss: 0.08970902201074821\n",
            "\t\t Validation Loss Decreased(0.137649--->0.089709) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.01900016420797722 \t\t Validation Loss: 0.061420647570720084\n",
            "\t\t Validation Loss Decreased(0.089709--->0.061421) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.01675207237083767 \t\t Validation Loss: 0.0431998148560524\n",
            "\t\t Validation Loss Decreased(0.061421--->0.043200) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.014109854913643888 \t\t Validation Loss: 0.032134780660271645\n",
            "\t\t Validation Loss Decreased(0.043200--->0.032135) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.013295478514722875 \t\t Validation Loss: 0.026013252182075612\n",
            "\t\t Validation Loss Decreased(0.032135--->0.026013) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.011606487407776955 \t\t Validation Loss: 0.021846700173157912\n",
            "\t\t Validation Loss Decreased(0.026013--->0.021847) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.01054282475111855 \t\t Validation Loss: 0.01844185683876276\n",
            "\t\t Validation Loss Decreased(0.021847--->0.018442) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.010064993620019507 \t\t Validation Loss: 0.015993700470202245\n",
            "\t\t Validation Loss Decreased(0.018442--->0.015994) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.009251595323754323 \t\t Validation Loss: 0.013559589126648812\n",
            "\t\t Validation Loss Decreased(0.015994--->0.013560) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.008164249071096247 \t\t Validation Loss: 0.011652474864744224\n",
            "\t\t Validation Loss Decreased(0.013560--->0.011652) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.007427231232459481 \t\t Validation Loss: 0.01021344748397286\n",
            "\t\t Validation Loss Decreased(0.011652--->0.010213) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.006651950735800169 \t\t Validation Loss: 0.008641094327546082\n",
            "\t\t Validation Loss Decreased(0.010213--->0.008641) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.006316520991055547 \t\t Validation Loss: 0.007675341437928951\n",
            "\t\t Validation Loss Decreased(0.008641--->0.007675) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.005788345972227084 \t\t Validation Loss: 0.006967719178646803\n",
            "\t\t Validation Loss Decreased(0.007675--->0.006968) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.00505341257824487 \t\t Validation Loss: 0.006140760504282438\n",
            "\t\t Validation Loss Decreased(0.006968--->0.006141) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.005111856222454761 \t\t Validation Loss: 0.005319466187547033\n",
            "\t\t Validation Loss Decreased(0.006141--->0.005319) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0043914809388838506 \t\t Validation Loss: 0.00479528264930615\n",
            "\t\t Validation Loss Decreased(0.005319--->0.004795) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.004563291978745445 \t\t Validation Loss: 0.004203833944092576\n",
            "\t\t Validation Loss Decreased(0.004795--->0.004204) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.00394785963942775 \t\t Validation Loss: 0.0038105327671823595\n",
            "\t\t Validation Loss Decreased(0.004204--->0.003811) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 344.26943210659897\n",
            "\n",
            "Accuracy: 49.49238578680203\n",
            "\n",
            "Complex_3_Linear_108_BERT_news\n",
            "\n",
            "The model has 1,458,682 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 1.1662923191372003 \t\t Validation Loss: 0.5642174723056647\n",
            "\t\t Validation Loss Decreased(inf--->0.564217) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.8730172025719406 \t\t Validation Loss: 0.9399954539078933\n",
            "Epoch 3 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.4935138196860616 \t\t Validation Loss: 1.4351791326816266\n",
            "Epoch 4 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.289622677348529 \t\t Validation Loss: 1.3623418716283946\n",
            "Epoch 5 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.257357598074737 \t\t Validation Loss: 1.2702075930742116\n",
            "Epoch 6 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.22946040773437032 \t\t Validation Loss: 1.1635143298369188\n",
            "Epoch 7 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.20366467546193381 \t\t Validation Loss: 1.050402934734638\n",
            "Epoch 8 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.17697707080987055 \t\t Validation Loss: 0.9255170684594375\n",
            "Epoch 9 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.150381517772739 \t\t Validation Loss: 0.7983429294366103\n",
            "Epoch 10 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.12083955635820087 \t\t Validation Loss: 0.6647652754416833\n",
            "Epoch 11 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.08831781897114942 \t\t Validation Loss: 0.5456272489749469\n",
            "\t\t Validation Loss Decreased(0.564217--->0.545627) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.05632401182870003 \t\t Validation Loss: 0.4550477254849214\n",
            "\t\t Validation Loss Decreased(0.545627--->0.455048) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.030307942359532054 \t\t Validation Loss: 0.399434527525535\n",
            "\t\t Validation Loss Decreased(0.455048--->0.399435) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.01183416204514435 \t\t Validation Loss: 0.3615656283039313\n",
            "\t\t Validation Loss Decreased(0.399435--->0.361566) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.004935828257805189 \t\t Validation Loss: 0.34339232857410723\n",
            "\t\t Validation Loss Decreased(0.361566--->0.343392) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.004353803217511725 \t\t Validation Loss: 0.3369656248161426\n",
            "\t\t Validation Loss Decreased(0.343392--->0.336966) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.004317690286395216 \t\t Validation Loss: 0.3314912634400221\n",
            "\t\t Validation Loss Decreased(0.336966--->0.331491) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.003890007916787589 \t\t Validation Loss: 0.3240593605889724\n",
            "\t\t Validation Loss Decreased(0.331491--->0.324059) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0035947134318711185 \t\t Validation Loss: 0.3164447755194627\n",
            "\t\t Validation Loss Decreased(0.324059--->0.316445) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0034165570285876056 \t\t Validation Loss: 0.30900336572757137\n",
            "\t\t Validation Loss Decreased(0.316445--->0.309003) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0032980862212040134 \t\t Validation Loss: 0.3021412119269371\n",
            "\t\t Validation Loss Decreased(0.309003--->0.302141) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0032003706429669684 \t\t Validation Loss: 0.2954847594866386\n",
            "\t\t Validation Loss Decreased(0.302141--->0.295485) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0031179442633623004 \t\t Validation Loss: 0.2889659565228682\n",
            "\t\t Validation Loss Decreased(0.295485--->0.288966) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0030616836471332085 \t\t Validation Loss: 0.2828707250838096\n",
            "\t\t Validation Loss Decreased(0.288966--->0.282871) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0030009329359902926 \t\t Validation Loss: 0.2774199843406677\n",
            "\t\t Validation Loss Decreased(0.282871--->0.277420) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0029582765607817753 \t\t Validation Loss: 0.27193827669207865\n",
            "\t\t Validation Loss Decreased(0.277420--->0.271938) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002896385350755441 \t\t Validation Loss: 0.2663880709845286\n",
            "\t\t Validation Loss Decreased(0.271938--->0.266388) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0028380543108387675 \t\t Validation Loss: 0.2614972008248934\n",
            "\t\t Validation Loss Decreased(0.266388--->0.261497) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0027824079820798157 \t\t Validation Loss: 0.25653471940985095\n",
            "\t\t Validation Loss Decreased(0.261497--->0.256535) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002735893941339659 \t\t Validation Loss: 0.2518901930978665\n",
            "\t\t Validation Loss Decreased(0.256535--->0.251890) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002687911178155274 \t\t Validation Loss: 0.24718012646413767\n",
            "\t\t Validation Loss Decreased(0.251890--->0.247180) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0026633475830061108 \t\t Validation Loss: 0.24233489082409784\n",
            "\t\t Validation Loss Decreased(0.247180--->0.242335) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0026578673797130986 \t\t Validation Loss: 0.23778934289629644\n",
            "\t\t Validation Loss Decreased(0.242335--->0.237789) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0026007392577436826 \t\t Validation Loss: 0.23311391444160387\n",
            "\t\t Validation Loss Decreased(0.237789--->0.233114) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002559159489071651 \t\t Validation Loss: 0.22853039563275301\n",
            "\t\t Validation Loss Decreased(0.233114--->0.228530) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0025669549571387972 \t\t Validation Loss: 0.22427308545089686\n",
            "\t\t Validation Loss Decreased(0.228530--->0.224273) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0025491233493478314 \t\t Validation Loss: 0.2204901336763914\n",
            "\t\t Validation Loss Decreased(0.224273--->0.220490) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0025111547265378002 \t\t Validation Loss: 0.2167933263744299\n",
            "\t\t Validation Loss Decreased(0.220490--->0.216793) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0025189528635048584 \t\t Validation Loss: 0.21313744883697766\n",
            "\t\t Validation Loss Decreased(0.216793--->0.213137) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002488657652016578 \t\t Validation Loss: 0.20920852371133292\n",
            "\t\t Validation Loss Decreased(0.213137--->0.209209) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024585431171077732 \t\t Validation Loss: 0.20615826962658992\n",
            "\t\t Validation Loss Decreased(0.209209--->0.206158) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024689268777021082 \t\t Validation Loss: 0.20283322802816445\n",
            "\t\t Validation Loss Decreased(0.206158--->0.202833) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024583429307362217 \t\t Validation Loss: 0.19948255163259232\n",
            "\t\t Validation Loss Decreased(0.202833--->0.199483) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024291011323913226 \t\t Validation Loss: 0.19683157774404839\n",
            "\t\t Validation Loss Decreased(0.199483--->0.196832) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024354068925793006 \t\t Validation Loss: 0.19382141516185725\n",
            "\t\t Validation Loss Decreased(0.196832--->0.193821) \t Saving The Model\n",
            "Epoch 46 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024303861270734183 \t\t Validation Loss: 0.19078575841222817\n",
            "\t\t Validation Loss Decreased(0.193821--->0.190786) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024006112960066545 \t\t Validation Loss: 0.18791354046418116\n",
            "\t\t Validation Loss Decreased(0.190786--->0.187914) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024135660079647662 \t\t Validation Loss: 0.18592928314151672\n",
            "\t\t Validation Loss Decreased(0.187914--->0.185929) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0024361139465425468 \t\t Validation Loss: 0.18354968239481634\n",
            "\t\t Validation Loss Decreased(0.185929--->0.183550) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023968259629327804 \t\t Validation Loss: 0.18134829480774128\n",
            "\t\t Validation Loss Decreased(0.183550--->0.181348) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023975449887960144 \t\t Validation Loss: 0.1794124826645622\n",
            "\t\t Validation Loss Decreased(0.181348--->0.179412) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023971994421633615 \t\t Validation Loss: 0.17731696933221358\n",
            "\t\t Validation Loss Decreased(0.179412--->0.177317) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023955863946104874 \t\t Validation Loss: 0.17543579265475273\n",
            "\t\t Validation Loss Decreased(0.177317--->0.175436) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002367928875075351 \t\t Validation Loss: 0.17333367072905487\n",
            "\t\t Validation Loss Decreased(0.175436--->0.173334) \t Saving The Model\n",
            "Epoch 55 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002391776703203701 \t\t Validation Loss: 0.17127676895604685\n",
            "\t\t Validation Loss Decreased(0.173334--->0.171277) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023627441897170268 \t\t Validation Loss: 0.16957587318924758\n",
            "\t\t Validation Loss Decreased(0.171277--->0.169576) \t Saving The Model\n",
            "Epoch 57 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002403804135659861 \t\t Validation Loss: 0.16836766415060714\n",
            "\t\t Validation Loss Decreased(0.169576--->0.168368) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023835879401303828 \t\t Validation Loss: 0.16609397247576943\n",
            "\t\t Validation Loss Decreased(0.168368--->0.166094) \t Saving The Model\n",
            "Epoch 59 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023694235038616367 \t\t Validation Loss: 0.16423565263931567\n",
            "\t\t Validation Loss Decreased(0.166094--->0.164236) \t Saving The Model\n",
            "Epoch 60 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023721558052882856 \t\t Validation Loss: 0.16332063214996687\n",
            "\t\t Validation Loss Decreased(0.164236--->0.163321) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002380735457188928 \t\t Validation Loss: 0.16162327447762856\n",
            "\t\t Validation Loss Decreased(0.163321--->0.161623) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023862789919901943 \t\t Validation Loss: 0.15985161445748347\n",
            "\t\t Validation Loss Decreased(0.161623--->0.159852) \t Saving The Model\n",
            "Epoch 63 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023772262769902277 \t\t Validation Loss: 0.15818200629347792\n",
            "\t\t Validation Loss Decreased(0.159852--->0.158182) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023758312745485455 \t\t Validation Loss: 0.1570997517555952\n",
            "\t\t Validation Loss Decreased(0.158182--->0.157100) \t Saving The Model\n",
            "Epoch 65 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023856759425315605 \t\t Validation Loss: 0.15598305257467124\n",
            "\t\t Validation Loss Decreased(0.157100--->0.155983) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002387784023429393 \t\t Validation Loss: 0.15394013801303047\n",
            "\t\t Validation Loss Decreased(0.155983--->0.153940) \t Saving The Model\n",
            "Epoch 67 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023734009387349157 \t\t Validation Loss: 0.15276451512741354\n",
            "\t\t Validation Loss Decreased(0.153940--->0.152765) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002378670171507307 \t\t Validation Loss: 0.15211615038032716\n",
            "\t\t Validation Loss Decreased(0.152765--->0.152116) \t Saving The Model\n",
            "Epoch 69 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023775790358667036 \t\t Validation Loss: 0.15079727912178406\n",
            "\t\t Validation Loss Decreased(0.152116--->0.150797) \t Saving The Model\n",
            "Epoch 70 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002419476741861955 \t\t Validation Loss: 0.14966651116712734\n",
            "\t\t Validation Loss Decreased(0.150797--->0.149667) \t Saving The Model\n",
            "Epoch 71 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002371340054024414 \t\t Validation Loss: 0.1490332051180303\n",
            "\t\t Validation Loss Decreased(0.149667--->0.149033) \t Saving The Model\n",
            "Epoch 72 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002395273566189398 \t\t Validation Loss: 0.1478167439882572\n",
            "\t\t Validation Loss Decreased(0.149033--->0.147817) \t Saving The Model\n",
            "Epoch 73 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.002372722969723966 \t\t Validation Loss: 0.14686761757072347\n",
            "\t\t Validation Loss Decreased(0.147817--->0.146868) \t Saving The Model\n",
            "Epoch 74 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023877428281372667 \t\t Validation Loss: 0.14542032180067438\n",
            "\t\t Validation Loss Decreased(0.146868--->0.145420) \t Saving The Model\n",
            "Epoch 75 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.0023974315168940135 \t\t Validation Loss: 0.14467050754823363\n",
            "\t\t Validation Loss Decreased(0.145420--->0.144671) \t Saving The Model\n",
            "\n",
            "Score: 12491.200507614214\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "Complex_3_LSTM_92_BERT_news\n",
            "\n",
            "The model has 2,224,554 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.5876513719307007 \t\t Validation Loss: 1.2606711433484004\n",
            "\t\t Validation Loss Decreased(inf--->1.260671) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.520406881385055 \t\t Validation Loss: 1.3605696834050691\n",
            "Epoch 3 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.43295431072269946 \t\t Validation Loss: 1.4381842017173767\n",
            "Epoch 4 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.309500909322319 \t\t Validation Loss: 1.3557520417066722\n",
            "Epoch 5 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.18360109863853133 \t\t Validation Loss: 1.1363466336176946\n",
            "\t\t Validation Loss Decreased(1.260671--->1.136347) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.09731982582928361 \t\t Validation Loss: 0.9311015812250284\n",
            "\t\t Validation Loss Decreased(1.136347--->0.931102) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.038769850985625305 \t\t Validation Loss: 0.7247636753779191\n",
            "\t\t Validation Loss Decreased(0.931102--->0.724764) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.01693231799966983 \t\t Validation Loss: 0.6007093236996577\n",
            "\t\t Validation Loss Decreased(0.724764--->0.600709) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.012742232971799534 \t\t Validation Loss: 0.5416443049907684\n",
            "\t\t Validation Loss Decreased(0.600709--->0.541644) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.009359754275882969 \t\t Validation Loss: 0.47894589373698604\n",
            "\t\t Validation Loss Decreased(0.541644--->0.478946) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.00735786255461642 \t\t Validation Loss: 0.41576526027459365\n",
            "\t\t Validation Loss Decreased(0.478946--->0.415765) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 45s\n",
            "\t\t Training Loss: 0.005734163257796821 \t\t Validation Loss: 0.36160787080342954\n",
            "\t\t Validation Loss Decreased(0.415765--->0.361608) \t Saving The Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNtrmivdprK4",
        "outputId": "db213cce-c301-4ce3-a1fd-1b6db4019f36"
      },
      "source": [
        "# train all complex with BERT -> need to rerun the preprocess -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"BERT_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = BERT_news()\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/71_model_BERT.pt'))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt'))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt'))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in range(7, len(model_name_tmp)):\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"{model_name[0]}\\n\")\n",
        "\n",
        "    # freeze the numerical and news weights\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    for p in model.model_num.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.model_news.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), 0.0001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    e_res = []\n",
        "    acc_res = []\n",
        "    batch_s_res = []\n",
        "    optimizer_start_res = []\n",
        "    optimizer_end_res = []\n",
        "    dropout_res = []\n",
        "    layer_dep_res = []\n",
        "    act_res = []\n",
        "    score_res = []\n",
        "    plt_teach_res = []\n",
        "    plt_pred_res = []\n",
        "\n",
        "    epoch_out, score_out, acc_out, plt_teach_out, plt_best_out = run_model_complex(batch_size_in=32,model_in=model,\n",
        "              optimizer_in=optimizer,criterion_in=criterion,model_name_in=model_name[0])\n",
        "\n",
        "    e_res.append(epoch_out)\n",
        "    acc_res.append(acc_out)\n",
        "    batch_s_res.append(\"32\")\n",
        "    optimizer_start_res.append(\"0.0001\")\n",
        "    optimizer_end_res.append(\"0.0001\")\n",
        "    dropout_res.append(\"TBD\")\n",
        "    layer_dep_res.append(\"TBD\")\n",
        "    act_res.append(\"TBD\")\n",
        "    score_res.append(score_out)\n",
        "    plt_teach_res.append(plt_teach_out)\n",
        "    plt_pred_res.append(plt_best_out)\n",
        "\n",
        "    save_the_log(path=\"drive/MyDrive/complex/complex_models\",name=model_name[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex_3_LSTM_92_BERT_news\n",
            "\n",
            "The model has 111,790,155 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.5876555943836432 \t\t Validation Loss: 1.260966149660257\n",
            "\t\t Validation Loss Decreased(inf--->1.260966) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.5203756654408533 \t\t Validation Loss: 1.3612547287574182\n",
            "Epoch 3 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.43242618547928696 \t\t Validation Loss: 1.4385899213644175\n",
            "Epoch 4 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.3103264354647616 \t\t Validation Loss: 1.3582571561519916\n",
            "Epoch 5 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.1851999313394363 \t\t Validation Loss: 1.1415072954618013\n",
            "\t\t Validation Loss Decreased(1.260966--->1.141507) \t Saving The Model\n",
            "Epoch 6 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.09761459409334772 \t\t Validation Loss: 0.938871661057839\n",
            "\t\t Validation Loss Decreased(1.141507--->0.938872) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.040108631113292395 \t\t Validation Loss: 0.7347625012581165\n",
            "\t\t Validation Loss Decreased(0.938872--->0.734763) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0171537681331707 \t\t Validation Loss: 0.6075090284530933\n",
            "\t\t Validation Loss Decreased(0.734763--->0.607509) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.012178405269829405 \t\t Validation Loss: 0.5419392597216827\n",
            "\t\t Validation Loss Decreased(0.607509--->0.541939) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.009666022070608026 \t\t Validation Loss: 0.48322024597571445\n",
            "\t\t Validation Loss Decreased(0.541939--->0.483220) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.00705532800104167 \t\t Validation Loss: 0.4193902680507073\n",
            "\t\t Validation Loss Decreased(0.483220--->0.419390) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.005804874037543463 \t\t Validation Loss: 0.3634470752798594\n",
            "\t\t Validation Loss Decreased(0.419390--->0.363447) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.004686679157768251 \t\t Validation Loss: 0.31481455037227046\n",
            "\t\t Validation Loss Decreased(0.363447--->0.314815) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.004097913945647511 \t\t Validation Loss: 0.27339736142983806\n",
            "\t\t Validation Loss Decreased(0.314815--->0.273397) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0036680982259379045 \t\t Validation Loss: 0.23871890713389105\n",
            "\t\t Validation Loss Decreased(0.273397--->0.238719) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.003237096844492732 \t\t Validation Loss: 0.2098715643470104\n",
            "\t\t Validation Loss Decreased(0.238719--->0.209872) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0029640952990440702 \t\t Validation Loss: 0.1865824174422484\n",
            "\t\t Validation Loss Decreased(0.209872--->0.186582) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0028520865905463594 \t\t Validation Loss: 0.16673145529169303\n",
            "\t\t Validation Loss Decreased(0.186582--->0.166731) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002775118563550751 \t\t Validation Loss: 0.15102729430565467\n",
            "\t\t Validation Loss Decreased(0.166731--->0.151027) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0026491717285967455 \t\t Validation Loss: 0.1371789934256902\n",
            "\t\t Validation Loss Decreased(0.151027--->0.137179) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002634286801834163 \t\t Validation Loss: 0.12654930654053503\n",
            "\t\t Validation Loss Decreased(0.137179--->0.126549) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002429899517320902 \t\t Validation Loss: 0.11675777995529082\n",
            "\t\t Validation Loss Decreased(0.126549--->0.116758) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002456327890064819 \t\t Validation Loss: 0.10965893883258104\n",
            "\t\t Validation Loss Decreased(0.116758--->0.109659) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002505937896392031 \t\t Validation Loss: 0.10179303708271338\n",
            "\t\t Validation Loss Decreased(0.109659--->0.101793) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0024767799438229084 \t\t Validation Loss: 0.0975886955140875\n",
            "\t\t Validation Loss Decreased(0.101793--->0.097589) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0024103443405741978 \t\t Validation Loss: 0.09216476894485262\n",
            "\t\t Validation Loss Decreased(0.097589--->0.092165) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002432559959030383 \t\t Validation Loss: 0.08984136391574374\n",
            "\t\t Validation Loss Decreased(0.092165--->0.089841) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002494307461421232 \t\t Validation Loss: 0.0857120520220353\n",
            "\t\t Validation Loss Decreased(0.089841--->0.085712) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002476084414189933 \t\t Validation Loss: 0.08187930282348624\n",
            "\t\t Validation Loss Decreased(0.085712--->0.081879) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002708156261202955 \t\t Validation Loss: 0.07969959065891229\n",
            "\t\t Validation Loss Decreased(0.081879--->0.079700) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002530874506683668 \t\t Validation Loss: 0.0758988566410083\n",
            "\t\t Validation Loss Decreased(0.079700--->0.075899) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0027529532011170443 \t\t Validation Loss: 0.07340304388736303\n",
            "\t\t Validation Loss Decreased(0.075899--->0.073403) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0024908459383516093 \t\t Validation Loss: 0.0722297217983466\n",
            "\t\t Validation Loss Decreased(0.073403--->0.072230) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002598493064225123 \t\t Validation Loss: 0.06849858017924887\n",
            "\t\t Validation Loss Decreased(0.072230--->0.068499) \t Saving The Model\n",
            "Epoch 35 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0027592614532221814 \t\t Validation Loss: 0.06898759692334212\n",
            "Epoch 36 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0026429894968640765 \t\t Validation Loss: 0.0676045598139843\n",
            "\t\t Validation Loss Decreased(0.068499--->0.067605) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0027196181763429195 \t\t Validation Loss: 0.06441480451478408\n",
            "\t\t Validation Loss Decreased(0.067605--->0.064415) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002624624962540897 \t\t Validation Loss: 0.06307040008071524\n",
            "\t\t Validation Loss Decreased(0.064415--->0.063070) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0026128358487669074 \t\t Validation Loss: 0.060591392075786225\n",
            "\t\t Validation Loss Decreased(0.063070--->0.060591) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002890164331482673 \t\t Validation Loss: 0.06202664469870237\n",
            "Epoch 41 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0025872087878540057 \t\t Validation Loss: 0.06070405449001835\n",
            "Epoch 42 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0025478671603785777 \t\t Validation Loss: 0.05922255441188239\n",
            "\t\t Validation Loss Decreased(0.060591--->0.059223) \t Saving The Model\n",
            "Epoch 43 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0026167936330484982 \t\t Validation Loss: 0.057859385798040494\n",
            "\t\t Validation Loss Decreased(0.059223--->0.057859) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002743165704069307 \t\t Validation Loss: 0.05540363581922765\n",
            "\t\t Validation Loss Decreased(0.057859--->0.055404) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0027396396633136916 \t\t Validation Loss: 0.05860832530575303\n",
            "Epoch 46 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0024930867657531053 \t\t Validation Loss: 0.05420207058509382\n",
            "\t\t Validation Loss Decreased(0.055404--->0.054202) \t Saving The Model\n",
            "Epoch 47 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002793216217321822 \t\t Validation Loss: 0.056990896279995255\n",
            "Epoch 48 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0026190176014662596 \t\t Validation Loss: 0.05562876524905173\n",
            "Epoch 49 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002425607227770662 \t\t Validation Loss: 0.05370858025092345\n",
            "\t\t Validation Loss Decreased(0.054202--->0.053709) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002760209605676701 \t\t Validation Loss: 0.05363194257594072\n",
            "\t\t Validation Loss Decreased(0.053709--->0.053632) \t Saving The Model\n",
            "Epoch 51 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002616861026507576 \t\t Validation Loss: 0.052610410878864616\n",
            "\t\t Validation Loss Decreased(0.053632--->0.052610) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0027227671706822475 \t\t Validation Loss: 0.05355678989480321\n",
            "Epoch 53 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002562249431738982 \t\t Validation Loss: 0.050019221810194164\n",
            "\t\t Validation Loss Decreased(0.052610--->0.050019) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002669016270571061 \t\t Validation Loss: 0.05060397133302803\n",
            "Epoch 55 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002658091676641709 \t\t Validation Loss: 0.04799105261022655\n",
            "\t\t Validation Loss Decreased(0.050019--->0.047991) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.00286610154403575 \t\t Validation Loss: 0.052092014865663186\n",
            "Epoch 57 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.00256078815279604 \t\t Validation Loss: 0.04791650043513913\n",
            "\t\t Validation Loss Decreased(0.047991--->0.047917) \t Saving The Model\n",
            "\tEarly stop of the training\n",
            "\n",
            "Score: 4334.5390228426395\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n",
            "Complex_3_LSTM_36_BERT_news\n",
            "\n",
            "The model has 606,074 trainable parameters\n",
            "The model has 61,273 trainable parameters\n",
            "Epoch 1 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.8847317335674086 \t\t Validation Loss: 0.8344104725580949\n",
            "\t\t Validation Loss Decreased(inf--->0.834410) \t Saving The Model\n",
            "Epoch 2 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.6434456244916529 \t\t Validation Loss: 1.1820596823325524\n",
            "Epoch 3 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.38372777696900273 \t\t Validation Loss: 1.3181966313948998\n",
            "Epoch 4 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.24175395888301568 \t\t Validation Loss: 1.11391242650839\n",
            "Epoch 5 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.18303552486405178 \t\t Validation Loss: 0.8823893757966849\n",
            "Epoch 6 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.12644150991596886 \t\t Validation Loss: 0.6787373882073623\n",
            "\t\t Validation Loss Decreased(0.834410--->0.678737) \t Saving The Model\n",
            "Epoch 7 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.06811839369805278 \t\t Validation Loss: 0.5089356922186338\n",
            "\t\t Validation Loss Decreased(0.678737--->0.508936) \t Saving The Model\n",
            "Epoch 8 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.024326263879727875 \t\t Validation Loss: 0.3823602319909976\n",
            "\t\t Validation Loss Decreased(0.508936--->0.382360) \t Saving The Model\n",
            "Epoch 9 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0061122861539793984 \t\t Validation Loss: 0.3312539251951071\n",
            "\t\t Validation Loss Decreased(0.382360--->0.331254) \t Saving The Model\n",
            "Epoch 10 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.005145583971957299 \t\t Validation Loss: 0.3224041731311725\n",
            "\t\t Validation Loss Decreased(0.331254--->0.322404) \t Saving The Model\n",
            "Epoch 11 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.004875683843951068 \t\t Validation Loss: 0.31089498217289263\n",
            "\t\t Validation Loss Decreased(0.322404--->0.310895) \t Saving The Model\n",
            "Epoch 12 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.004087943034687055 \t\t Validation Loss: 0.2939222856209828\n",
            "\t\t Validation Loss Decreased(0.310895--->0.293922) \t Saving The Model\n",
            "Epoch 13 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0038622465359072224 \t\t Validation Loss: 0.2790015179377336\n",
            "\t\t Validation Loss Decreased(0.293922--->0.279002) \t Saving The Model\n",
            "Epoch 14 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0034854307729826383 \t\t Validation Loss: 0.2641866353268807\n",
            "\t\t Validation Loss Decreased(0.279002--->0.264187) \t Saving The Model\n",
            "Epoch 15 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.003624412725125817 \t\t Validation Loss: 0.2522949410172609\n",
            "\t\t Validation Loss Decreased(0.264187--->0.252295) \t Saving The Model\n",
            "Epoch 16 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.003346371841994492 \t\t Validation Loss: 0.2399711087346077\n",
            "\t\t Validation Loss Decreased(0.252295--->0.239971) \t Saving The Model\n",
            "Epoch 17 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0031262125538322268 \t\t Validation Loss: 0.22790367270891482\n",
            "\t\t Validation Loss Decreased(0.239971--->0.227904) \t Saving The Model\n",
            "Epoch 18 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.003108960672319439 \t\t Validation Loss: 0.21790898290391153\n",
            "\t\t Validation Loss Decreased(0.227904--->0.217909) \t Saving The Model\n",
            "Epoch 19 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002986937023485331 \t\t Validation Loss: 0.20916542697411317\n",
            "\t\t Validation Loss Decreased(0.217909--->0.209165) \t Saving The Model\n",
            "Epoch 20 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0029709183349827858 \t\t Validation Loss: 0.1990521735010239\n",
            "\t\t Validation Loss Decreased(0.209165--->0.199052) \t Saving The Model\n",
            "Epoch 21 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0030626943846220602 \t\t Validation Loss: 0.19061731862334105\n",
            "\t\t Validation Loss Decreased(0.199052--->0.190617) \t Saving The Model\n",
            "Epoch 22 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002965867382512 \t\t Validation Loss: 0.1825850626023916\n",
            "\t\t Validation Loss Decreased(0.190617--->0.182585) \t Saving The Model\n",
            "Epoch 23 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0028382653081034485 \t\t Validation Loss: 0.17394748616677064\n",
            "\t\t Validation Loss Decreased(0.182585--->0.173947) \t Saving The Model\n",
            "Epoch 24 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0027796914494231445 \t\t Validation Loss: 0.1660461503152664\n",
            "\t\t Validation Loss Decreased(0.173947--->0.166046) \t Saving The Model\n",
            "Epoch 25 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0028448074228583356 \t\t Validation Loss: 0.15871092025190592\n",
            "\t\t Validation Loss Decreased(0.166046--->0.158711) \t Saving The Model\n",
            "Epoch 26 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002686326450202614 \t\t Validation Loss: 0.15545652333933574\n",
            "\t\t Validation Loss Decreased(0.158711--->0.155457) \t Saving The Model\n",
            "Epoch 27 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002949047040798374 \t\t Validation Loss: 0.1497674251978214\n",
            "\t\t Validation Loss Decreased(0.155457--->0.149767) \t Saving The Model\n",
            "Epoch 28 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0028636687013597504 \t\t Validation Loss: 0.14629063103348017\n",
            "\t\t Validation Loss Decreased(0.149767--->0.146291) \t Saving The Model\n",
            "Epoch 29 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002725811826216208 \t\t Validation Loss: 0.14144179282280114\n",
            "\t\t Validation Loss Decreased(0.146291--->0.141442) \t Saving The Model\n",
            "Epoch 30 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002602673373248025 \t\t Validation Loss: 0.13905809432841265\n",
            "\t\t Validation Loss Decreased(0.141442--->0.139058) \t Saving The Model\n",
            "Epoch 31 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002660682942479144 \t\t Validation Loss: 0.13506787485228136\n",
            "\t\t Validation Loss Decreased(0.139058--->0.135068) \t Saving The Model\n",
            "Epoch 32 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0027863341949980807 \t\t Validation Loss: 0.13127300136077863\n",
            "\t\t Validation Loss Decreased(0.135068--->0.131273) \t Saving The Model\n",
            "Epoch 33 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002635064602489709 \t\t Validation Loss: 0.1285677321899969\n",
            "\t\t Validation Loss Decreased(0.131273--->0.128568) \t Saving The Model\n",
            "Epoch 34 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.00280749929653531 \t\t Validation Loss: 0.12911470886319876\n",
            "Epoch 35 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0026183447020279396 \t\t Validation Loss: 0.12620561011135578\n",
            "\t\t Validation Loss Decreased(0.128568--->0.126206) \t Saving The Model\n",
            "Epoch 36 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002710920480092534 \t\t Validation Loss: 0.1232738050703819\n",
            "\t\t Validation Loss Decreased(0.126206--->0.123274) \t Saving The Model\n",
            "Epoch 37 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0026998998348701847 \t\t Validation Loss: 0.12263475279681958\n",
            "\t\t Validation Loss Decreased(0.123274--->0.122635) \t Saving The Model\n",
            "Epoch 38 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0027961275663000306 \t\t Validation Loss: 0.11992001024862894\n",
            "\t\t Validation Loss Decreased(0.122635--->0.119920) \t Saving The Model\n",
            "Epoch 39 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.00266784499242046 \t\t Validation Loss: 0.11915259331894609\n",
            "\t\t Validation Loss Decreased(0.119920--->0.119153) \t Saving The Model\n",
            "Epoch 40 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0025641875203094773 \t\t Validation Loss: 0.11513981412952909\n",
            "\t\t Validation Loss Decreased(0.119153--->0.115140) \t Saving The Model\n",
            "Epoch 41 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0024718536941863195 \t\t Validation Loss: 0.11368531626291\n",
            "\t\t Validation Loss Decreased(0.115140--->0.113685) \t Saving The Model\n",
            "Epoch 42 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002650151351731069 \t\t Validation Loss: 0.11562303410699734\n",
            "Epoch 43 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002575347405336042 \t\t Validation Loss: 0.1120252785129616\n",
            "\t\t Validation Loss Decreased(0.113685--->0.112025) \t Saving The Model\n",
            "Epoch 44 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002790270681568497 \t\t Validation Loss: 0.1081387152035649\n",
            "\t\t Validation Loss Decreased(0.112025--->0.108139) \t Saving The Model\n",
            "Epoch 45 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002549901102807977 \t\t Validation Loss: 0.11013799192957006\n",
            "Epoch 46 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002507277109937088 \t\t Validation Loss: 0.10860187653452158\n",
            "Epoch 47 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002662386790050093 \t\t Validation Loss: 0.10785101243079855\n",
            "\t\t Validation Loss Decreased(0.108139--->0.107851) \t Saving The Model\n",
            "Epoch 48 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.0025956904298525202 \t\t Validation Loss: 0.10755226209473151\n",
            "\t\t Validation Loss Decreased(0.107851--->0.107552) \t Saving The Model\n",
            "Epoch 49 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002562565030530095 \t\t Validation Loss: 0.1038532519641404\n",
            "\t\t Validation Loss Decreased(0.107552--->0.103853) \t Saving The Model\n",
            "Epoch 50 \t\t Epoch time: 1m 59s\n",
            "\t\t Training Loss: 0.002742698895933761 \t\t Validation Loss: 0.10855949598436172\n",
            "Epoch 51 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.00253513253079979 \t\t Validation Loss: 0.10351429252813642\n",
            "\t\t Validation Loss Decreased(0.103853--->0.103514) \t Saving The Model\n",
            "Epoch 52 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0026300757890567183 \t\t Validation Loss: 0.10338922002567695\n",
            "\t\t Validation Loss Decreased(0.103514--->0.103389) \t Saving The Model\n",
            "Epoch 53 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0027831824232094193 \t\t Validation Loss: 0.10068699903786182\n",
            "\t\t Validation Loss Decreased(0.103389--->0.100687) \t Saving The Model\n",
            "Epoch 54 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002570378325086691 \t\t Validation Loss: 0.10207703849300742\n",
            "Epoch 55 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0025872296820445037 \t\t Validation Loss: 0.09950833740787438\n",
            "\t\t Validation Loss Decreased(0.100687--->0.099508) \t Saving The Model\n",
            "Epoch 56 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002618936133552091 \t\t Validation Loss: 0.10059375785147914\n",
            "Epoch 57 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002714744697457079 \t\t Validation Loss: 0.09621822907446095\n",
            "\t\t Validation Loss Decreased(0.099508--->0.096218) \t Saving The Model\n",
            "Epoch 58 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002687785461406551 \t\t Validation Loss: 0.09986895494736157\n",
            "Epoch 59 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002684294354686564 \t\t Validation Loss: 0.09834751841397239\n",
            "Epoch 60 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002805297786835581 \t\t Validation Loss: 0.09539222993099919\n",
            "\t\t Validation Loss Decreased(0.096218--->0.095392) \t Saving The Model\n",
            "Epoch 61 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0025852042080431775 \t\t Validation Loss: 0.09428924297054227\n",
            "\t\t Validation Loss Decreased(0.095392--->0.094289) \t Saving The Model\n",
            "Epoch 62 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0027190646411523827 \t\t Validation Loss: 0.09573414832210311\n",
            "Epoch 63 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002597241462648821 \t\t Validation Loss: 0.09179747949999112\n",
            "\t\t Validation Loss Decreased(0.094289--->0.091797) \t Saving The Model\n",
            "Epoch 64 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002690526165974301 \t\t Validation Loss: 0.09694298403337598\n",
            "Epoch 65 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0025293935433256666 \t\t Validation Loss: 0.09088511584111704\n",
            "\t\t Validation Loss Decreased(0.091797--->0.090885) \t Saving The Model\n",
            "Epoch 66 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002933263692161622 \t\t Validation Loss: 0.096849946394706\n",
            "Epoch 67 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002760654515932893 \t\t Validation Loss: 0.08681914704636885\n",
            "\t\t Validation Loss Decreased(0.090885--->0.086819) \t Saving The Model\n",
            "Epoch 68 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.0026428239118005776 \t\t Validation Loss: 0.09618737484113528\n",
            "Epoch 69 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002953935041709023 \t\t Validation Loss: 0.08690709690563381\n",
            "Epoch 70 \t\t Epoch time: 1m 58s\n",
            "\t\t Training Loss: 0.002756716565518464 \t\t Validation Loss: 0.09169380033675295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8URIhcMvng1M",
        "outputId": "7591c42b-299f-4ad5-875b-80b189971a37"
      },
      "source": [
        "# train all complex with BERT -> need to rerun the preprocess -> there the check may be not epoch to epoch\n",
        "complex_name_tmp = [\"Complex_1\", \"Complex_2\", \"Complex_3\"]\n",
        "nums_name_tmp = [\"Linear_108\", \"LSTM_92\", \"LSTM_36\"]\n",
        "news_name_tmp = [\"BERT_news\"]\n",
        "\n",
        "model_name_tmp = []\n",
        "model_num = []\n",
        "model_complex = []\n",
        "\n",
        "model_news = BERT_news()\n",
        "model_news.load_state_dict(torch.load(f'drive/MyDrive/complex/news_models/71_model_BERT.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "for complex_name_i in complex_name_tmp:\n",
        "    for nums_name_i in nums_name_tmp:\n",
        "        model_name_tmp.append(f\"{complex_name_i}_{nums_name_i}_{news_name_tmp[0]}\")\n",
        "\n",
        "        if nums_name_i == \"Linear_108\":\n",
        "          model_tmp = Linear_108_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_Linear_108.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_92\":\n",
        "          model_tmp = LSTM_92_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_92_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)\n",
        "        elif nums_name_i == \"LSTM_36\":\n",
        "          model_tmp = LSTM_36_num()\n",
        "          model_tmp.load_state_dict(torch.load('drive/MyDrive/complex/numerical_models/best_model_LSTM_36_num.pt',map_location=torch.device('cpu')))\n",
        "          model_num.append(model_tmp)        \n",
        "        else:\n",
        "          pass\n",
        "\n",
        "        if complex_name_i == \"Complex_1\":\n",
        "          model_tmp_complex = Complex_1(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        elif complex_name_i == \"Complex_2\":\n",
        "          model_tmp_complex = Complex_2(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)          \n",
        "        elif complex_name_i == \"Complex_3\":\n",
        "          model_tmp_complex = Complex_3(model_tmp, model_news)\n",
        "          model_complex.append(model_tmp_complex)\n",
        "        else:\n",
        "          pass   \n",
        "\n",
        "\n",
        "\n",
        "for i in [8]:\n",
        "    model_name = []\n",
        "    model_name.append(model_name_tmp[i])\n",
        "\n",
        "    model = model_complex[i]\n",
        "\n",
        "    print(f\"{model_name[0]}\\n\")\n",
        "\n",
        "    model.load_state_dict(torch.load('drive/MyDrive/complex/complex_models/best_model_' + str(model_name[0]) + '.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "    predict_test_array = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(test_X_num)):\n",
        "            model.eval()\n",
        "            predict = model(test_X_num[i].reshape(1,-1).to(device), test_X_news[i].reshape(1,-1).to(device))        \n",
        "            predict_test_array.append(predict.item()*output_df.std()+output_df.mean()) \n",
        "\n",
        "    real_test_array = []\n",
        "    real_trend_array = test[\"Trend target\"]  \n",
        "    for y in test_Y_num:\n",
        "      real_test_array.append(y*output_df.std()+output_df.mean())\n",
        "\n",
        "\n",
        "    # calculate score\n",
        "    criterion = nn.MSELoss()\n",
        "    score = calc_score(predict_test_array,real_test_array)    \n",
        "    print(f\"\\nScore: {score}\\n\")      \n",
        "\n",
        "    #check the trend -> create an array with the real and with the predicted trend\n",
        "    #if the current value is bigger than before -> 1\n",
        "    #otherwise (same or smaller) -> 0\n",
        "    predicted_trend_array = []\n",
        "    for element in range(len(predict_test_array)):\n",
        "        real_today_close = test[\"Close\"][element]*input_df[\"Close\"].std()+input_df[\"Close\"].mean()\n",
        "        if real_today_close > predict_test_array[element].values:\n",
        "            predicted_trend_array.append(0)\n",
        "        else:\n",
        "            predicted_trend_array.append(1)\n",
        "\n",
        "    #check the number of differences\n",
        "    trend_diff_array = []\n",
        "    for element in range(len(real_trend_array)):\n",
        "        if real_trend_array[element] != predicted_trend_array[element]:\n",
        "          trend_diff_array.append(element)\n",
        "\n",
        "    #percentage of good predict\n",
        "    acc = (len(real_trend_array)-len(trend_diff_array))/len(real_trend_array)*100\n",
        "    print(f\"Accuracy: {acc}\\n\")  \n",
        "    #visualize it\n",
        "    f2 = plt.figure(figsize=(24,12))\n",
        "    plt.title(\"Predicted and real close prices\", fontsize = 18)\n",
        "    plt.plot(real_test_array, color = \"blue\", linewidth = 4,\n",
        "            label = \"Real\")\n",
        "    plt.plot(predict_test_array, color = \"red\", linewidth = 2,\n",
        "            label = \"Predicted\")\n",
        "    plt.xlabel(\"Date\",fontsize = 18)\n",
        "    plt.legend(fontsize = 18)\n",
        "    f2.set_size_inches(12,6)\n",
        "    plt_best = f2\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex_3_LSTM_36_BERT_news\n",
            "\n",
            "\n",
            "Score: 8443.171319796955\n",
            "\n",
            "Accuracy: 48.984771573604064\n",
            "\n"
          ]
        }
      ]
    }
  ]
}