{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Filter_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVt5PVrBAhko"
      },
      "source": [
        "# **Stock market news feed semantic analysis** *(Baseline Filter)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjmNuE1ABo2W"
      },
      "source": [
        "Ebben a modellben a log reg modellhez tartozó korrelációs tényezők alapján próbálom filterezni az adathalmazom.\r\n",
        "Nyolcasával egybe fűzöm a reddit-es híreket, majd megvizsgálom a korrelációs tényezőket 2,5 ngram modellel. \r\n",
        "Ezek után egyesével megvizsgálom a híreket és ha a korrelációs tényezők alapján a végeredmény semleges (tűréssel együtt), akkor azt az elemet kiveszem.\r\n",
        "Szűrés után megvizsgálom majd a halmaz számosságát és a modell pontosságát újra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgZr85OICUI8"
      },
      "source": [
        "## **A projekt előkészítése**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J14w6gnLCV4x"
      },
      "source": [
        "A Drive csatlakoztatása a szükséges fájlok későbbi betöltésére. A betöltés közvetlen a használat előtt fogom megtenni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yivbQmqCXFg",
        "outputId": "6d54f9e6-3840-4032-cf9c-3b3cd428947d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br3us9qsCYzQ"
      },
      "source": [
        "A szükséges könyvtárak betöltése a projekthez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLOPK2YFCadT",
        "outputId": "05a484e6-52de-41f0-a5ce-d5ac42b72aa4"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pandas_datareader as web\r\n",
        "from numpy.random import MT19937\r\n",
        "from numpy.random import RandomState, SeedSequence\r\n",
        "import string\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.tokenize import word_tokenize  \r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import accuracy_score \r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2Jn07zCfeu"
      },
      "source": [
        "A reprodukálhatóság miatt definiálok egy seed-et a véletlen szám generátorhoz, amit a továbbiakban használni fogok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NThrDNbsCgkg"
      },
      "source": [
        "# Random seed\r\n",
        "RANDOM_SEED = 1234\r\n",
        "\r\n",
        "# Numpy random seed\r\n",
        "NP_SEED = 1234\r\n",
        "\r\n",
        "# Max iteration for training\r\n",
        "MAX_ITER = 100000\r\n",
        "\r\n",
        "# Train size\r\n",
        "TRAIN_SPLIT = 0.85\r\n",
        "\r\n",
        "# Test size\r\n",
        "TEST_SPLIT = 0.15\r\n",
        "\r\n",
        "# Shuffle cycle number for the dataframe\r\n",
        "SHUFFLE_CYCLE = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRT9SvoEChqv"
      },
      "source": [
        "np.random.seed(NP_SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1DLONKC8BW"
      },
      "source": [
        "## **Az adathalmaz elkőkészítése**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeeKBze70j74"
      },
      "source": [
        "Nyolcasával csoportosítás és az eddigi összes preprocess algoritmus használata.\r\n",
        "Felbontás train és teszt adathalmazra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab1K7smXC_eI",
        "outputId": "7ed2a008-3fc3-453e-e420-b53a4fc30077"
      },
      "source": [
        "print(\"Start of the preprocess\\n\")\r\n",
        "\r\n",
        "# Copy the dataset to the local environment\r\n",
        "!cp \"/content/drive/MyDrive/Combined_News_DJIA.csv\" \"Combined_News_DJIA.csv\"\r\n",
        "\r\n",
        "# Load the dataset \r\n",
        "df_combined = pd.read_csv('Combined_News_DJIA.csv', index_col = \"Date\")\r\n",
        "\r\n",
        "# Load the stock data\r\n",
        "df_stock = web.DataReader(\"DJIA\", data_source=\"yahoo\", start=\"2008-08-08\", \r\n",
        "                          end=\"2016-07-01\")\r\n",
        "\r\n",
        "temp_day = []\r\n",
        "\r\n",
        "for day in range(len(df_stock)):\r\n",
        "    temp_day.append(df_stock.index[day].date())\r\n",
        "\r\n",
        "df_stock.index = temp_day\r\n",
        "\r\n",
        "difference = []\r\n",
        "\r\n",
        "for day in range(max(len(df_combined), len(df_stock))):\r\n",
        "    if str(df_combined.index[day]) != str(df_stock.index[day]):\r\n",
        "        difference.append(day)\r\n",
        "\r\n",
        "if len(difference) is 0:\r\n",
        "    print(\"The dates matched!\\n\")\r\n",
        "\r\n",
        "difference = []\r\n",
        "\r\n",
        "for day in range(len(df_stock)):\r\n",
        "    # label should be 1 -> rise\r\n",
        "    if int(df_stock[\"Adj Close\"][day]) >= int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "        if df_combined[\"Label\"][day] != 1:\r\n",
        "            difference.append(str(df_stock.index[day]))\r\n",
        "            print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "            print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\")\r\n",
        "\r\n",
        "    # label should be 0 -> fall\r\n",
        "    if int(df_stock[\"Adj Close\"][day]) < int(df_stock[\"Adj Close\"][day - 1]):\r\n",
        "        if df_combined[\"Label\"][day] != 0:\r\n",
        "            difference.append(str(df_stock.index[day]))\r\n",
        "            print(\"Problem at day \" + str(df_stock.index[day]))\r\n",
        "            print(\"Today: \" + str(df_stock[\"Adj Close\"][day]) +\"\\t\\tYesterday: \" + str(df_stock[\"Adj Close\"][day - 1]) + \"\\t\\tLabel: \" + str(df_combined[\"Label\"][day]) + \"\\n\") \r\n",
        "\r\n",
        "# correct the wrong labels\r\n",
        "for row in difference:\r\n",
        "    if df_combined.loc[row, \"Label\"] == 0:\r\n",
        "        df_combined.loc[row, \"Label\"] = 1\r\n",
        "    else:\r\n",
        "        df_combined.loc[row, \"Label\"] = 0\r\n",
        "\r\n",
        "print(\"All differences: \" + str(len(difference)) + \"\\nFixed!\\n\") \r\n",
        "\r\n",
        "# Find the cells with NaN and after the rows for them\r\n",
        "is_NaN = df_combined.isnull()\r\n",
        "row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "# Replace them\r\n",
        "df_combined = df_combined.replace(np.nan, \" \")\r\n",
        "\r\n",
        "# Check the process\r\n",
        "is_NaN = df_combined.isnull()\r\n",
        "row_has_NaN = is_NaN.any(axis = 1)\r\n",
        "rows_with_NaN = df_combined[row_has_NaN]\r\n",
        "\r\n",
        "assert len(rows_with_NaN) is 0\r\n",
        "\r\n",
        "# The label column \r\n",
        "LABEL_COLUMN = 0\r\n",
        "\r\n",
        "news_sum = []\r\n",
        "label_sum = []\r\n",
        "\r\n",
        "# Get the column names\r\n",
        "combined_column_names = []\r\n",
        "for column in df_combined.columns:\r\n",
        "  combined_column_names.append(column)\r\n",
        "\r\n",
        "# Connect the news with the labels\r\n",
        "for column in range(len(df_combined)):\r\n",
        "  for row in range(len(combined_column_names) - 1):\r\n",
        "    news = df_combined[combined_column_names[row + 1]][column]\r\n",
        "    # Remove the b character at the begining of the string\r\n",
        "    if news[0] is \"b\":\r\n",
        "        news = \" \" + news[1:]\r\n",
        "    news_sum.append(news)\r\n",
        "    label_sum.append(df_combined[combined_column_names[LABEL_COLUMN]][column])\r\n",
        "\r\n",
        "# Create the new DataFrame\r\n",
        "df_sum_news_labels = pd.DataFrame(data = label_sum, index = None, columns = [\"Label\"])\r\n",
        "df_sum_news_labels[\"News\"] = news_sum\r\n",
        "\r\n",
        "# Removing punctuations\r\n",
        "temp_news = []\r\n",
        "for line in news_sum:\r\n",
        "  temp_attach = \"\"\r\n",
        "  for word in line:\r\n",
        "    temp = \" \"\r\n",
        "    if word not in string.punctuation:\r\n",
        "      temp = word\r\n",
        "    temp_attach = temp_attach + \"\".join(temp)\r\n",
        "  temp_news.append(temp_attach)\r\n",
        "\r\n",
        "news_sum = temp_news\r\n",
        "temp_news = []\r\n",
        "\r\n",
        "# Remove numbers\r\n",
        "for line in news_sum:\r\n",
        "  temp_attach = \"\"\r\n",
        "  for word in line:\r\n",
        "    temp = \" \"\r\n",
        "    if not word.isdigit():\r\n",
        "      temp = word\r\n",
        "    temp_attach = temp_attach + \"\".join(temp)\r\n",
        "  temp_news.append(temp_attach)\r\n",
        "\r\n",
        "# Remove space\r\n",
        "for line in range(len(temp_news)):    \r\n",
        "  temp_news[line] = \" \".join(temp_news[line].split())\r\n",
        "\r\n",
        "# Converting headlines to lower case\r\n",
        "for line in range(len(temp_news)): \r\n",
        "    temp_news[line] = temp_news[line].lower()\r\n",
        "\r\n",
        "# Update the data frame\r\n",
        "df_sum_news_labels[\"News\"] = temp_news\r\n",
        "\r\n",
        "# Load the stop words\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "filtered_sentence = []\r\n",
        "news_sum = df_sum_news_labels[\"News\"]\r\n",
        "\r\n",
        "# Remove stop words\r\n",
        "for line in news_sum:\r\n",
        "  word_tokens = word_tokenize(line)\r\n",
        "  temp_attach = \"\"\r\n",
        "  for word in word_tokens:\r\n",
        "    temp = \" \"\r\n",
        "    if not word in stop_words:\r\n",
        "      temp = temp + word\r\n",
        "    temp_attach = temp_attach + \"\".join(temp)\r\n",
        "  filtered_sentence.append(temp_attach)\r\n",
        "\r\n",
        "# Remove space\r\n",
        "for line in range(len(filtered_sentence)):    \r\n",
        "  filtered_sentence[line] = \" \".join(filtered_sentence[line].split())\r\n",
        "\r\n",
        "# Update the data frame\r\n",
        "df_sum_news_labels[\"News\"] = filtered_sentence\r\n",
        "\r\n",
        "news_sum = df_sum_news_labels[\"News\"]\r\n",
        "null_indexes = []\r\n",
        "index = 0\r\n",
        "\r\n",
        "for line in news_sum:\r\n",
        "  if line is \"\":\r\n",
        "    null_indexes.append(index)\r\n",
        "  index = index + 1\r\n",
        "\r\n",
        "print(\"\\nNull indexes: \" + str(null_indexes) + \"\\n\")\r\n",
        "\r\n",
        "for row in null_indexes:\r\n",
        "  df_sum_news_labels = df_sum_news_labels.drop(row)\r\n",
        "\r\n",
        "news_sum = df_sum_news_labels[\"News\"]\r\n",
        "null_indexes = []\r\n",
        "index = 0\r\n",
        "\r\n",
        "for line in news_sum:\r\n",
        "  if line is \"\":\r\n",
        "    null_indexes.append(index)\r\n",
        "  index = index + 1\r\n",
        "  \r\n",
        "assert len(null_indexes) is 0\r\n",
        "\r\n",
        "df_sum_news_labels.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "# Show the data frame\r\n",
        "print(df_sum_news_labels.head())\r\n",
        "print()\r\n",
        "print(df_stock.head())\r\n",
        "\r\n",
        "INPUT_SIZE = len(df_sum_news_labels)\r\n",
        "TRAIN_SIZE = int(TRAIN_SPLIT * INPUT_SIZE) \r\n",
        "TEST_SIZE = int(TEST_SPLIT * INPUT_SIZE)\r\n",
        "\r\n",
        "# Split the dataset\r\n",
        "train_before = df_sum_news_labels[:TRAIN_SIZE] \r\n",
        "test_before = df_sum_news_labels[TRAIN_SIZE:]\r\n",
        "test_before.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "# Print out the length\r\n",
        "print(\"\\nTrain data set length: \" + str(len(train_before)))\r\n",
        "print(\"Test data set length: \" + str(len(test_before)))\r\n",
        "print(\"Split summa: \" + str(len(train_before) + len(test_before)))\r\n",
        "print(\"Dataset summa before split: \" + str(len(df_sum_news_labels)) + \"\\n\")\r\n",
        "\r\n",
        "# check\r\n",
        "split_sum = len(train_before) + len(test_before)\r\n",
        "sum = len(df_sum_news_labels)\r\n",
        "assert split_sum == sum\r\n",
        "\r\n",
        "print(\"Train last:\\n\" + str(train_before.tail(1)) + \"\\n\")\r\n",
        "\r\n",
        "print(\"Test first:\\n\" + str(test_before.head(1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of the preprocess\n",
            "\n",
            "The dates matched!\n",
            "\n",
            "Problem at day 2010-10-14\n",
            "Today: 11096.919921875\t\tYesterday: 11096.080078125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-12\n",
            "Today: 12815.080078125\t\tYesterday: 12815.3896484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2012-11-15\n",
            "Today: 12570.9501953125\t\tYesterday: 12570.9501953125\t\tLabel: 0\n",
            "\n",
            "Problem at day 2013-04-12\n",
            "Today: 14865.0595703125\t\tYesterday: 14865.1396484375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2014-04-24\n",
            "Today: 16501.650390625\t\tYesterday: 16501.650390625\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-08-12\n",
            "Today: 17402.509765625\t\tYesterday: 17402.83984375\t\tLabel: 0\n",
            "\n",
            "Problem at day 2015-11-27\n",
            "Today: 17813.390625\t\tYesterday: 17813.390625\t\tLabel: 0\n",
            "\n",
            "All differences: 7\n",
            "Fixed!\n",
            "\n",
            "\n",
            "Null indexes: [6947, 6948, 6949, 8723, 8724, 13134, 17048, 17049]\n",
            "\n",
            "   Label                                               News\n",
            "0      0  georgia downs two russian warplanes countries ...\n",
            "1      0                       breaking musharraf impeached\n",
            "2      0  russia today columns troops roll south ossetia...\n",
            "3      0  russian tanks moving towards capital south oss...\n",
            "4      0  afghan children raped impunity u n official sa...\n",
            "\n",
            "                    High           Low  ...      Volume     Adj Close\n",
            "2008-08-08  11808.490234  11344.230469  ...  4966810000  11734.320312\n",
            "2008-08-11  11933.549805  11580.190430  ...  5067310000  11782.349609\n",
            "2008-08-12  11830.389648  11541.429688  ...  4711290000  11642.469727\n",
            "2008-08-13  11689.049805  11377.370117  ...  4787600000  11532.959961\n",
            "2008-08-14  11744.330078  11399.839844  ...  4064000000  11615.929688\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "\n",
            "Train data set length: 42259\n",
            "Test data set length: 7458\n",
            "Split summa: 49717\n",
            "Dataset summa before split: 49717\n",
            "\n",
            "Train last:\n",
            "       Label                                               News\n",
            "42258      1  humpback whale population grows animals propos...\n",
            "\n",
            "Test first:\n",
            "   Label                                               News\n",
            "0      1  number women england wales becoming nuns hits ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhXK6B2BTZOE",
        "outputId": "02889f93-9945-44fb-9238-5dc4f5894439"
      },
      "source": [
        "# Train merge\r\n",
        "merged_news = []\r\n",
        "merged_labels = []\r\n",
        "temp_news = \"\"\r\n",
        "in_rows_counter = 0\r\n",
        "merged_counter = 0\r\n",
        "for row in range(len(train_before)):\r\n",
        "    if merged_counter == 3:\r\n",
        "        merged_counter = 0 \r\n",
        "        in_rows_counter = 0\r\n",
        "    elif in_rows_counter == 7:\r\n",
        "        temp_news = temp_news + \" \" + train_before[\"News\"][row]\r\n",
        "        merged_counter = merged_counter + 1\r\n",
        "        merged_news.append(temp_news)\r\n",
        "        merged_labels.append(train_before[\"Label\"][row])\r\n",
        "        temp_news = \"\"\r\n",
        "        in_rows_counter = 0\r\n",
        "    else:\r\n",
        "        if in_rows_counter == 0:\r\n",
        "            temp_news = temp_news + train_before[\"News\"][row]\r\n",
        "        else:\r\n",
        "            temp_news = temp_news + \" \" + train_before[\"News\"][row]\r\n",
        "        in_rows_counter = in_rows_counter + 1\r\n",
        "\r\n",
        "train_merged = pd.DataFrame()\r\n",
        "train_merged[\"News\"] = merged_news\r\n",
        "train_merged[\"Label\"] = merged_labels\r\n",
        "\r\n",
        "# Test merge\r\n",
        "merged_news = []\r\n",
        "merged_labels = []\r\n",
        "temp_news = \"\"\r\n",
        "in_rows_counter = 0\r\n",
        "merged_counter = 0\r\n",
        "for row in range(len(test_before)):\r\n",
        "    if merged_counter == 3:\r\n",
        "        merged_counter = 0 \r\n",
        "        in_rows_counter = 0\r\n",
        "    elif in_rows_counter == 7:\r\n",
        "        temp_news = temp_news + \" \" + test_before[\"News\"][row]\r\n",
        "        merged_counter = merged_counter + 1\r\n",
        "        merged_news.append(temp_news)\r\n",
        "        merged_labels.append(test_before[\"Label\"][row])\r\n",
        "        temp_news = \"\"\r\n",
        "        in_rows_counter = 0\r\n",
        "    else:\r\n",
        "        if in_rows_counter == 0:\r\n",
        "            temp_news = temp_news + test_before[\"News\"][row]\r\n",
        "        else:\r\n",
        "            temp_news = temp_news + \" \" + test_before[\"News\"][row]\r\n",
        "        in_rows_counter = in_rows_counter + 1\r\n",
        "\r\n",
        "test_merged = pd.DataFrame()\r\n",
        "test_merged[\"News\"] = merged_news\r\n",
        "test_merged[\"Label\"] = merged_labels\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Print out the length\r\n",
        "print(\"\\nTrain merged data set length: \" + str(len(train_merged)))\r\n",
        "print(\"Test merged data set length: \" + str(len(test_merged)))\r\n",
        "print(\"Split summa: \" + str(len(train_merged) + len(test_merged)))\r\n",
        "print(\"Dataset summa before merge and split: \" + str(len(df_sum_news_labels)) + \"\\n\")\r\n",
        "\r\n",
        "print(\"Train merged last:\\n\" + str(train_merged.tail(1)) + \"\\n\")\r\n",
        "\r\n",
        "print(\"Test merged first:\\n\" + str(test_merged.head(1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train merged data set length: 5071\n",
            "Test merged data set length: 895\n",
            "Split summa: 5966\n",
            "Dataset summa before merge and split: 49717\n",
            "\n",
            "Train merged last:\n",
            "                                                   News  Label\n",
            "5070  salman rushdie chastises authors protesting ch...      1\n",
            "\n",
            "Test merged first:\n",
            "                                                News  Label\n",
            "0  number women england wales becoming nuns hits ...      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a9MYTrYFfNv"
      },
      "source": [
        "## **Log Reg modell**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phT9cqv6FgRV"
      },
      "source": [
        "2,4 n gram modell létrehozása és a korrelációs tényezők szemlélése."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWeDHQDC0wih",
        "outputId": "57f66d0b-f7e4-451f-c39d-8d4275a547fe"
      },
      "source": [
        "MODEL_TYPE = str(\"2,4\")\r\n",
        "\r\n",
        "train_headlines = []\r\n",
        "test_headlines = []\r\n",
        "\r\n",
        "for row in range(0, len(train_merged.index)):\r\n",
        "    train_headlines.append(train_merged.iloc[row, 0])\r\n",
        "\r\n",
        "for row in range(0,len(test_merged.index)):\r\n",
        "    test_headlines.append(test_merged.iloc[row, 0])\r\n",
        "\r\n",
        "# show the first\r\n",
        "print(train_headlines[0])\r\n",
        "\r\n",
        "_gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "_train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "_gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "_gram_model_ = _gram_model_.fit(_train_vectorizer_, train_merged[\"Label\"])\r\n",
        "\r\n",
        "_gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "_gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "print (accuracy_score(test_merged[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "georgia downs two russian warplanes countries move brink war breaking musharraf impeached russia today columns troops roll south ossetia footage fighting youtube russian tanks moving towards capital south ossetia reportedly completely destroyed georgian artillery fire afghan children raped impunity u n official says sick three year old raped nothing russian tanks entered south ossetia whilst georgia shoots two russian jets breaking georgia invades south ossetia russia warned would intervene side enemy combatent trials nothing sham salim haman sentenced years kept longer anyway feel like\n",
            "The shape is: (5071, 1254113)\n",
            "\n",
            "0.5307262569832403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeWgV3Ue09EQ",
        "outputId": "3d4efdf9-4a25-4f7b-9786-3f66f0116959"
      },
      "source": [
        "_gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "_gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                        'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     Word  Coefficient\n",
            "400721         first time     0.332402\n",
            "736017        new zealand     0.256574\n",
            "1093056          tear gas     0.248045\n",
            "249435        court rules     0.227543\n",
            "967623        says russia     0.225581\n",
            "980140   security council     0.208010\n",
            "1233710     world largest     0.202004\n",
            "1040064        sri lankan     0.192259\n",
            "1021365      social media     0.190208\n",
            "1110500       three years     0.187716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nd0gvH11Bqv",
        "outputId": "19f7f715-42fc-406c-bb5b-e50993e75494"
      },
      "source": [
        "print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  Word  Coefficient\n",
            "1028309   south africa    -0.193925\n",
            "812998   phone hacking    -0.206697\n",
            "900658       red cross    -0.209729\n",
            "1167029        us army    -0.212450\n",
            "503744       hong kong    -0.213815\n",
            "995753    sexual abuse    -0.221582\n",
            "116917       bin laden    -0.222097\n",
            "735898        new york    -0.226199\n",
            "1029275   south korean    -0.264553\n",
            "64515     around world    -0.321893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAgmb8Lp4po_"
      },
      "source": [
        "## **Hírek vizsgálata**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FElGnxvu4tR1"
      },
      "source": [
        "Ebben a fejezetben megvizsgálom a hírekben szereplő elemek korrelációs tényezőit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfBjTzcPqNva",
        "outputId": "a5f8b48c-683c-47ba-cb1f-6fe6a651e4f0"
      },
      "source": [
        "_gram_test_ = _gram_vectorizer_.transform(train_before[\"News\"])\r\n",
        "_gram_predictions_ = _gram_model_.predict_proba(_gram_test_)\r\n",
        "\r\n",
        "neutraliy_index = []\r\n",
        "for i in range(len(_gram_predictions_)):\r\n",
        "    if _gram_predictions_[i][0] < 0.47:\r\n",
        "        pass\r\n",
        "    elif _gram_predictions_[i][0] > 0.53:\r\n",
        "        pass        \r\n",
        "    else:\r\n",
        "        neutraliy_index.append(i)\r\n",
        "\r\n",
        "print(len(neutraliy_index))\r\n",
        "print(len(train_before))\r\n",
        "print(len(neutraliy_index) / len(train_before))\r\n",
        "print(_gram_predictions_[neutraliy_index[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5796\n",
            "42259\n",
            "0.13715421567003477\n",
            "[0.49568784 0.50431216]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcp-fzTrgsl",
        "outputId": "424b73cc-27ba-4dd1-cd9a-32f4962f156e"
      },
      "source": [
        "_gram_test_ = _gram_vectorizer_.transform(test_before[\"News\"])\r\n",
        "_gram_predictions_ = _gram_model_.predict_proba(_gram_test_)\r\n",
        "\r\n",
        "neutraliy_index = []\r\n",
        "for i in range(len(_gram_predictions_)):\r\n",
        "    if _gram_predictions_[i][0] < 0.45:\r\n",
        "        pass\r\n",
        "    elif _gram_predictions_[i][0] > 0.55:\r\n",
        "        pass        \r\n",
        "    else:\r\n",
        "        neutraliy_index.append(i)\r\n",
        "\r\n",
        "print(len(neutraliy_index))\r\n",
        "print(len(test_before))\r\n",
        "print(len(neutraliy_index) / len(test_before))\r\n",
        "print(_gram_predictions_[neutraliy_index[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "212\n",
            "7458\n",
            "0.028425851434700992\n",
            "[0.46518896 0.53481104]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58SkF1dmsLdY",
        "outputId": "ebfc968a-a450-4b16-a831-8027d4741495"
      },
      "source": [
        "_gram_test_ = _gram_vectorizer_.transform(train_before[\"News\"])\r\n",
        "_gram_predictions_ = _gram_model_.predict_proba(_gram_test_)\r\n",
        "\r\n",
        "neutraliy_index = []\r\n",
        "for i in range(len(_gram_predictions_)):\r\n",
        "    if _gram_predictions_[i][0] < 0.47:\r\n",
        "        pass\r\n",
        "    elif _gram_predictions_[i][0] > 0.53:\r\n",
        "        pass        \r\n",
        "    else:\r\n",
        "        neutraliy_index.append(i)\r\n",
        "\r\n",
        "print(len(neutraliy_index))\r\n",
        "print(len(train_before))\r\n",
        "print(len(neutraliy_index) / len(train_before))\r\n",
        "print(_gram_predictions_[neutraliy_index[0]])\r\n",
        "\r\n",
        "train_before_dropped = train_before.drop(neutraliy_index)\r\n",
        "print(len(train_before_dropped))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5796\n",
            "42259\n",
            "0.13715421567003477\n",
            "[0.49568784 0.50431216]\n",
            "36463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOsjX1mLVW6e",
        "outputId": "d69be41c-3f1e-4cb2-88ee-73ee5c8beb27"
      },
      "source": [
        "pos_label_news = []\r\n",
        "neg_label_news = []\r\n",
        "\r\n",
        "train_before_dropped.reset_index(inplace=True)\r\n",
        "\r\n",
        "for row in range(len(train_before_dropped)):\r\n",
        "    if str(train_before_dropped[\"Label\"][row]) == \"0\":\r\n",
        "        neg_label_news.append(str(train_before_dropped[\"News\"][row]))\r\n",
        "    elif str(train_before_dropped[\"Label\"][row]) == \"1\":\r\n",
        "        pos_label_news.append(str(train_before_dropped[\"News\"][row]))\r\n",
        "    else:\r\n",
        "        pass\r\n",
        "\r\n",
        "print(len(pos_label_news))\r\n",
        "print(len(neg_label_news))\r\n",
        "\r\n",
        "pos_train = pd.DataFrame()\r\n",
        "neg_train = pd.DataFrame()\r\n",
        "\r\n",
        "pos_labels = []\r\n",
        "for row in range(len(pos_label_news)):\r\n",
        "    pos_labels.append(\"1\")\r\n",
        "\r\n",
        "pos_train[\"News\"] = pos_label_news \r\n",
        "pos_train[\"Label\"] = pos_labels \r\n",
        "\r\n",
        "\r\n",
        "neg_labels = []\r\n",
        "for row in range(len(neg_label_news)):\r\n",
        "    neg_labels.append(\"0\")\r\n",
        "\r\n",
        "neg_train[\"News\"] = neg_label_news \r\n",
        "neg_train[\"Label\"] = neg_labels "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22728\n",
            "13735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MbcQz6jXXRYL",
        "outputId": "e6dc2b8e-5b58-4209-bbb8-3d963606449d"
      },
      "source": [
        "pos_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wont america nato help us wont help us help iraq</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bush puts foot georgian conflict</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jewish georgian minister thanks israeli traini...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>georgian army flees disarray russians advance ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>olympic opening ceremony fireworks faked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22723</th>\n",
              "      <td>australian reporter fired tweeting soldiers ra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22724</th>\n",
              "      <td>netherlands legalized sex marriage divorce rat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22725</th>\n",
              "      <td>earthquake slid india feet northwards matter s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22726</th>\n",
              "      <td>nine await mass execution indonesia security p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22727</th>\n",
              "      <td>humpback whale population grows animals propos...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22728 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    News Label\n",
              "0       wont america nato help us wont help us help iraq     1\n",
              "1                       bush puts foot georgian conflict     1\n",
              "2      jewish georgian minister thanks israeli traini...     1\n",
              "3      georgian army flees disarray russians advance ...     1\n",
              "4               olympic opening ceremony fireworks faked     1\n",
              "...                                                  ...   ...\n",
              "22723  australian reporter fired tweeting soldiers ra...     1\n",
              "22724  netherlands legalized sex marriage divorce rat...     1\n",
              "22725  earthquake slid india feet northwards matter s...     1\n",
              "22726  nine await mass execution indonesia security p...     1\n",
              "22727  humpback whale population grows animals propos...     1\n",
              "\n",
              "[22728 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "M2Jd0_44XTDP",
        "outputId": "ca9daec2-7bc1-4390-c20d-538d1f7c1870"
      },
      "source": [
        "neg_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>breaking musharraf impeached</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>russian tanks moving towards capital south oss...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>afghan children raped impunity u n official sa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>breaking georgia invades south ossetia russia ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>enemy combatent trials nothing sham salim hama...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13730</th>\n",
              "      <td>mass poisoning egypt sends hospital</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13731</th>\n",
              "      <td>israel airlift babies born surrogates nepal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13732</th>\n",
              "      <td>five billion people access safe surgery</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13733</th>\n",
              "      <td>erdoan engages war words new turkish cypriot l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13734</th>\n",
              "      <td>guantanamo ex inmates demand housing payment us</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13735 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    News Label\n",
              "0                           breaking musharraf impeached     0\n",
              "1      russian tanks moving towards capital south oss...     0\n",
              "2      afghan children raped impunity u n official sa...     0\n",
              "3      breaking georgia invades south ossetia russia ...     0\n",
              "4      enemy combatent trials nothing sham salim hama...     0\n",
              "...                                                  ...   ...\n",
              "13730                mass poisoning egypt sends hospital     0\n",
              "13731        israel airlift babies born surrogates nepal     0\n",
              "13732            five billion people access safe surgery     0\n",
              "13733  erdoan engages war words new turkish cypriot l...     0\n",
              "13734    guantanamo ex inmates demand housing payment us     0\n",
              "\n",
              "[13735 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfifNRkYsLxr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "dfbadf80-35d4-4f53-8e98-f7c97ee7c8e2"
      },
      "source": [
        "# Train merge\r\n",
        "# Neg merge\r\n",
        "neg_merged_news = []\r\n",
        "neg_merged_labels = []\r\n",
        "in_rows_counter = 0\r\n",
        "merged_counter = 0\r\n",
        "\r\n",
        "for i in range(int(len(neg_train) / 8)):\r\n",
        "    temp_news = \"\"\r\n",
        "    for j in range(8): #0,1...7\r\n",
        "        temp_news = temp_news + \" \" + neg_train[\"News\"][i* 8 + j]\r\n",
        "    neg_merged_news.append(temp_news)\r\n",
        "    neg_merged_labels.append(0)\r\n",
        "\r\n",
        "neg_merged_df = pd.DataFrame()\r\n",
        "neg_merged_df[\"News\"] = neg_merged_news\r\n",
        "neg_merged_df[\"Label\"] = neg_merged_labels\r\n",
        "\r\n",
        "# Pos merge\r\n",
        "pos_merged_news = []\r\n",
        "pos_merged_labels = []\r\n",
        "in_rows_counter = 0\r\n",
        "merged_counter = 0\r\n",
        "\r\n",
        "for i in range(int(len(pos_train) / 8)):\r\n",
        "    temp_news = \"\"\r\n",
        "    for j in range(8): #0,1...7\r\n",
        "        temp_news = temp_news + \" \" + pos_train[\"News\"][i* 8 + j]\r\n",
        "    pos_merged_news.append(temp_news)\r\n",
        "    pos_merged_labels.append(1)\r\n",
        "\r\n",
        "pos_merged_df = pd.DataFrame()\r\n",
        "pos_merged_df[\"News\"] = pos_merged_news\r\n",
        "pos_merged_df[\"Label\"] = pos_merged_labels\r\n",
        "\r\n",
        "# All merge\r\n",
        "filtered_merged_df = pd.concat([pos_merged_df, neg_merged_df])\r\n",
        "\r\n",
        "# Do the shuffle\r\n",
        "for i in range(SHUFFLE_CYCLE):\r\n",
        "  filtered_merged_df = shuffle(filtered_merged_df, random_state = RANDOM_SEED)\r\n",
        "\r\n",
        "# Reset the index\r\n",
        "filtered_merged_df.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "# Show the data frame\r\n",
        "filtered_merged_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aust government force citizens hand internet ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>north korea declares year unification boosts ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>crackdown fish poaching wales nets arrests st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>billionaire gives million bonus workers mos d...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>danish drugmaker seeks prevent use drug makes...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4552</th>\n",
              "      <td>philippines china increasing ships disputed s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4553</th>\n",
              "      <td>men find king thutmosis iii yr old temple hou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4554</th>\n",
              "      <td>new york cuba first direct charter flight tak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4555</th>\n",
              "      <td>ac dc drummer phil rudd marijuana conviction ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4556</th>\n",
              "      <td>mexico overtakes us percentage population ove...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4557 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   News  Label\n",
              "0      aust government force citizens hand internet ...      1\n",
              "1      north korea declares year unification boosts ...      0\n",
              "2      crackdown fish poaching wales nets arrests st...      0\n",
              "3      billionaire gives million bonus workers mos d...      1\n",
              "4      danish drugmaker seeks prevent use drug makes...      1\n",
              "...                                                 ...    ...\n",
              "4552   philippines china increasing ships disputed s...      0\n",
              "4553   men find king thutmosis iii yr old temple hou...      1\n",
              "4554   new york cuba first direct charter flight tak...      0\n",
              "4555   ac dc drummer phil rudd marijuana conviction ...      1\n",
              "4556   mexico overtakes us percentage population ove...      1\n",
              "\n",
              "[4557 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UATxwjtM4x",
        "outputId": "0b191b1a-34dd-4027-ddbf-9b9ff0c273ba"
      },
      "source": [
        "MODEL_TYPE = str(\"2,4\")\r\n",
        "\r\n",
        "train_headlines = []\r\n",
        "test_headlines = []\r\n",
        "\r\n",
        "for row in range(0, len(filtered_merged_df.index)):\r\n",
        "    train_headlines.append(filtered_merged_df.iloc[row, 0])\r\n",
        "\r\n",
        "for row in range(0,len(test_merged.index)):\r\n",
        "    test_headlines.append(test_merged.iloc[row, 0])\r\n",
        "\r\n",
        "# show the first\r\n",
        "print(train_headlines[0])\r\n",
        "\r\n",
        "_gram_vectorizer_ = CountVectorizer(ngram_range=(int(MODEL_TYPE[0]),int(MODEL_TYPE[2])))\r\n",
        "_train_vectorizer_ = _gram_vectorizer_.fit_transform(train_headlines)\r\n",
        "\r\n",
        "print(\"The shape is: \" + str(_train_vectorizer_.shape) + \"\\n\")\r\n",
        "\r\n",
        "_gram_model_ = LogisticRegression(random_state=RANDOM_SEED, max_iter=MAX_ITER)\r\n",
        "_gram_model_ = _gram_model_.fit(_train_vectorizer_, filtered_merged_df[\"Label\"])\r\n",
        "\r\n",
        "_gram_test_ = _gram_vectorizer_.transform(test_headlines)\r\n",
        "_gram_predictions_ = _gram_model_.predict(_gram_test_)\r\n",
        "\r\n",
        "print (accuracy_score(test_merged[\"Label\"], _gram_predictions_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " aust government force citizens hand internet passwords sending jail refuse top muslim cleric russia tatarstan province shot dead thursday another wounded car bomb attacks province leader local religious authorities said probably related priests criticism radical islamists beyond foxconn dirt factories making iphone troubling new findings cast doubt apple highly publicized promise improve conditions overseas factories using surveys onsite visits undercover investigations amp face face interviews factories evaluated french rightwing lawmakers raised eyebrows hooted minister territories housing ccile duflot took podium wearing floral dress uk border agency staff go strike one day olympics russia china veto western backed syria resolution un security council russia moderate muslim leaders attacked tatarstan ian tomlinson death pc guilty\n",
            "The shape is: (4557, 1162633)\n",
            "\n",
            "0.5150837988826815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEV_D2XPtaJu",
        "outputId": "1f657702-a787-420b-c5c3-bf090302784a"
      },
      "source": [
        "_gram_words_best_ = _gram_vectorizer_.get_feature_names()\r\n",
        "_gram_coeffs_best_ = _gram_model_.coef_.tolist()[0]\r\n",
        "\r\n",
        "coeffdf = pd.DataFrame({'Word' : _gram_words_best_, \r\n",
        "                        'Coefficient' : _gram_coeffs_best_})\r\n",
        "\r\n",
        "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\r\n",
        "\r\n",
        "print(coeffdf.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                     Word  Coefficient\n",
            "94584            bbc news     0.404294\n",
            "682451        new zealand     0.233953\n",
            "1013166          tear gas     0.217490\n",
            "1124647         west bank     0.206855\n",
            "896924        says russia     0.201999\n",
            "795496        pro russian     0.194115\n",
            "169507   chemical weapons     0.175787\n",
            "48445            anti gay     0.167968\n",
            "1143531     world largest     0.166182\n",
            "567331      latin america     0.162423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhGE8Tk_tbxC",
        "outputId": "fdbb4ebe-0870-4e98-d11d-7c597844d86a"
      },
      "source": [
        "print(coeffdf.tail(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    Word  Coefficient\n",
            "1154495        years ago    -0.204712\n",
            "916246   sentenced years    -0.208253\n",
            "953870      south korean    -0.220199\n",
            "510470      iran nuclear    -0.240260\n",
            "682335          new york    -0.242104\n",
            "696288   nuclear weapons    -0.247959\n",
            "467571         hong kong    -0.249882\n",
            "108731         bin laden    -0.280259\n",
            "923012      sexual abuse    -0.321462\n",
            "60362       around world    -0.462350\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}